---
title: >
    BREAKING DOWN BARRIERS --- PART 1: WHAT'S A BARRIER?
---
# BREAKING DOWN BARRIERS --- PART 1: WHAT'S A BARRIER?

D3D12かVulkanのプログラミングを少しでもかじったことがある人ならば誰しも、バリアの対処に長々と時間を費やしたことだろう。これらは正しく行おうとするとかなり扱いづらくなることがある。つまり、レンダリングコードを変更するたびに、または、Windowsアップデートを介して新しいバージョンの検証レイヤが導入されるときに、検証レイヤが常に新しい問題を指摘しているように見えてしまう。そして、それに加えて、IHVは、D3D11に匹敵するかそれを超えるGPUパフォーマンスを望む場合には、本当に気を付ける必要があるということ、そして、どのようにバリアを使うかということを伝え続けている。これはただアーティファクトフリーな結果を達成する上での追加の課題である。

では、何がどうなっているのだろうか？そもそも一体全体なぜバリアとやらが必要なのか、そして、これらを誤って使ってしまうとなぜそこまで上手くいかないのだろうか？貴方がかなりのコンソールプログラミングを行ったことのある人やモダンなGPUの低レベルの詳細にすでに親しみのある人であるならば、これらの疑問の答えを恐らく知っているだろう。その場合、この記事はまったくもって貴方用ではない。しかし、貴方がそんな経験の恩恵を持たない人であるならば、私は、貴方がバリアを発行するときのシーンの背後で起こっていることをより良く理解できるよう最善を尽くします。

## A HIGH BARRIER TO ENTRY

プログラミングやコンピュータにおける他の例に漏れず、"バリア"という用語はすでにいくらか多重定義されている。ある文脈では、"バリア"は大量のスレッドすべてが動作中のコードの特定のポイントに達するたびに停止しなければならない同期ポイントである。この場合、バリアを動かない壁として考えることができる。つまり、スレッドはすべて動作しているが、バリアに"ヒット"するときに急停止する。


```
void ThreadFunction() {
    DoStuff();

    // すべてのスレッドがバリアにヒットするまで待つ
    barrier.Wait();

    // すべてのスレッドがDoStuff()を呼び出したことが分かっている
}
```

この種のものは大量のスレッドがすべてタスクを実行し終えたときを知りたいとき(fork-joinモデルにおける"join")、または、他の結果を読む必要があるスレッドがあるときに役立つ。プログラマとしては、アトミック演算によって更新される変数での"spinning"(条件を満たすまでループすること)、または、待っている間にスレッドをスリープさせたいときにはセマフォと条件変数を用いることでスレッドバリアを実装できる。

他の文脈では、"バリア"という用語は、特にロックフリープログラミングの世界にいくらか入り込んだ場合には、"メモリバリア"を指すだろう("フェンス"としても知られる)。これらの場合では、コンパイラやプロセッサ自体によって行われるメモリ操作の並べ替えを通常扱っている。これは共有メモリを介して通信する複数のプロセッサがあるときに本当に滅茶苦茶にしてしまうことがある。メモリバリアはメモリ操作がバリアの前か後のいずれかに完了することを強制させることで支援し、実質的にフェンスの一方の"側"にこれらを留める。C++では、Windows APIにおけるメモリバリアのようなプラットフォーム固有のマクロやクロスプラットフォームの`std::atomic_thread_fence`を用いてコードにこれらを挿入することができる。一般的なユースケースはこのように見えるかもしれないだろう。

```
// DataIsReadyとDataは異なるスレッドで書き込まれる
if (DataIsReady) {
    // DataIsReadyの読み出しの*後*にDataの読み出しが起こることを確実にする
    MemoryBarrier();

    DoSomething(Data);
}
```

これら２つの"バリア"という用語の意味する所は異なるが、これらに共通するものもある。これらは片方が結果を生み出し、もう片方がその結果を読み出す必要があるときに大抵使われる。別の言い方をすると、あるタスクが別のタスクへの**依存性**を持つということである。依存性はコードを書くときにいつでも発生する。つまり、オフセットを計算するために2つの値を加算するコードの1行があれば、すぐ次の行が配列から読み出すためにそのオフセットを用いるだろう。しかしながら、コンパイラがこれらの依存性を**追跡**し、正しい結果をもたらすコードを生成することを確かめるので、貴方はこれに気付く必要はそれほどない。バリアの手動挿入は、コンパイラがコンパイル時にどのようにデータが書き出され、読み出されているかを理解できないような方法で物事を行うまで通常現れない。これは同じデータにアクセスする複数のスレッドによって一般的に発生するが、(他のハードウェアの一部がメモリに書き込むときのような)他の奇妙なケースでも発生し得る。いずれにしても、適切なバリアを用いることで、誤ったデータの読み出しとならないように保証され、結果がdependent stepsから**可視**であろうことを確実にするだろう。

コンパイラはマルチスレッドのCPUプログラミングを行うときに自動的に依存性を扱うことができないので、マルチスレッドのタスク間の依存性を表現し解決する方法を示すのにしばしば多くの時間を費やす。これらの状況では、どのタスクが他のタスクの結果に依存するかを示す依存性グラフを構築することが一般的である。このグラフはどんな順番でタスクを実行するか、そして、先行するタスクが次のタスクの実行開始前に完了するように、どのタイミングで2つのタスク(またはタスクのグループ)の間に同期ポイント(バリア)を置くべきかを決めるのに役立たせることができる。IntelのTBBの文書にあるこの理解しやすい例にあるように、木のような図として描かれるこれらのグラフをしばしば見ることだろう。

![](images/tbb_dependency_graph.jpg)

タスク指向のマルチスレッドプログラミングを一切行わなかったとしても、この図は依存性の概念をかなり明確にしてくれる。つまり、まずパンがなければパンにピーナッツバターは塗れない！大まかに、これはタスクの順番を決定するが(ピーナッツバターの前にパン)、現実にこれを行っていたならば明らかであったであろうことを若干暗示してもいる。つまり、棚からパンを一枚取ってくるまでピーナッツバターを塗り始めることはできない。貴方自身が現実でこれを行っていた場合、これについて考えてすらいなかっただろう。貴方ひとりだけなら、一度に1つの工程を行うだけであろう。しかし、我々は元々マルチスレッディングの文脈で議論していた。これは並列に異なるコアで異なるタスクを実行しようと試みることについて話していることを意味する。適切に待機しなければ、パンの工程と同時にピーナッツバターのタスクを実行することになってしまい、これは明らかに良くない！

![](images/overlapped_tasks.png)

これらの種類の問題を回避するため、TBBのようなタスクスケジューラは先行するタスク(または、タスクのグループ)が完全に実行し終えるまで待機するようタスク(または、タスクのグループ)を強制するメカニズムをもたらす。以前に言及したように、このメカニズムをバリア、または、同期ポイントと呼ぶことができるだろう。

![](images/overlapped_tasks_fixed.png)

この種のことは、意のままに操れる多大な柔軟性も強力なツールもあるので、モダンなPCのCPUでは実装するのがかなり容易である。つまり、アトミック演算、同期プリミティブ、OS提供の条件変数、などなどである。

## BACK TO GPU LAND

さて、バリアが何たるかの基礎を取り扱ったが、GPUとの対話のために設計されたAPIにこれらがある理由を未だ説明してこなかった。結局の所、ドローおよびディスパッチコールの発行は実際には別個のコアで実行するために大量の並列タスクをスケジュールするのと同じではない。いいね？というか、D3D11のAPI呼び出しのプログラムシーケンスを見ると、クッソ直列に見える。

![](images/renderdoc_d3d11.png)

このようなAPIを通してGPUを扱うのに慣れていたならば、GPUが一度にひとつずつのコマンドをサブミットされた順序で実行すると考えても仕方がないだろう。そして、これは長らく真実であったかもしれないが、モダンなGPUでは実際にかなりずっと複雑であるというのが現実である。私が行っていることを示すため、私のDeferred TexturingサンプルがAMDの素晴らしいプロファイリングツールであるRadeon GPU Profilerでキャプチャを取るとどう見えるかを見てみよう。

![](images/rgp_bindlessdeferred.png)

このスニペットはフレームの一部だけ、具体的にはシーンのジオメトリのすべてがGバッファにラスタライズされる部分を示している。左手側はドローコールを示し、右側の青線はドローコールが実際に実行を開始するときと終了するときを示す。そして、まさか、ここには全体にたくさんのオーバーラップがある！PIX for Windowsでやってもちょっと違うけど同じものを確認できる。

![](images/pix_timeline.png)

これはPIXのタイムラインビューのスニペットである。これはドローコールの同じシーケンスに対する実行時間を示してもいる(先のRGPキャプチャは大幅に劣るRX460で行われたが、今回はGTX1070でキャプチャした)。同じパターンを確認できる。つまり、描画は大まかにサブミッション順に実行し始めるが、至る所でオーバーラップする。いくつかの場合では、描画は先の描画が完了する前に終了する！

GPUについて少しでも知っているならば、これは*それほど*驚愕するべきものではない。結局の所、皆はGPUがほぼIHVが言う所の何千何万の"**シェーダコア**"から構成され、これらのシェーダコアのすべてが"あきれるほど並列"な問題を解くために一緒に動作することを知っている。近頃、描画を処理するために行われる処理のほとんど(とディスパッチを処理するために行われる処理のすべて)はこれらの輩で処理される。これはHLSL/GLSL/MetalSLコードからコンパイルされるシェーダプログラムを実行する。確かに、並列に単一のドローコールから数千の頂点を処理するシェーダコアを持つこととトライアングルのライスタライズからもたらされる何千何万のピクセルで同じことを行うことは理に適っている。しかし、実際の高レベルのコマンドも並列に実行しているように複数のドローコールやディスパッチを互いににじみこませることは本当に合理的なのだろうか？

<略>
