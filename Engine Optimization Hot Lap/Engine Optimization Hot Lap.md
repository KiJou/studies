---
title: Engine Optimization Hot Lap [@Lottes2018]
---
# Engine Optimization Hot Lap

# このトークの目標

- ハードウェアレベルにより近い所でのGPUの動作の仕方について更に教示する
- GPUワークロードやエンジン設計についての理由付けを行うのを助けるために
- 様々な最適化の結果を提示する
- <font color="red">そして、究極的にはGPUがスケールし続けるたびにビジュアルおよびパフォーマンス目標をさらに良く理解するのを助ける</font>

# GCN再掲

- SIMD(VALU) = 16幅ベクトルALUは4クロックに渡って64幅のwaveを実行する
    - 最大占有率 8 waves/SIMD (Polaris)、または、一般には10 waves/SIMD (Polaris以外)
- CU = Compute Unitは4SIMDを含む(1wave/clockのピークVALU実行スループット)
- SALU = Scalar ALU
- LDS = Local Data Store
- V\$ = ベクトルL1キャッシュ (VMEM)
- I\$ = 命令L1キャッシュ
- K\$ = スカラL1キャッシュ (SMEM)

# 描画の外側を考える

- しばしば最適化の努力は最も高価なシェーダを最適化することに注力[narrowly focused]される
- このトークは全体論的な視点を取る: **すべての描画/ディスパッチの内外のフレーム全体の実行時間に注目する**

# AMD内で使われる解析用ツールのイントロダクション

- 外部ツール
    - Radeon GPU Profiler (RGP)
- AMD内部ツール
    - VK_GPA_interface
    - ドライバ修正
    - 命令トレースリポート
        - → wave実行の詳細
        - → waveの寿命
        - → CU命令のまとめ
            - 緑 = SIMDあたりのVALUの発行
            - 橙 = VMEMの発行

# 核となる原則

# AMDAHLの法則

- 並列処理の観点でマシンがスケールするごとに可能なパフォーマンス改善は並列処理の量に比例する
- 例えば、GHzだと[at the GHz wall]、仕事の75%を並列に走らせただけで、ベストケースの実行時間は並列でないマシンの25%にまで下がるだろう
- Takeaway: **パイプライン化された並列な仕事でGPUを満杯にし続ける**

# GCNのベクトルメモリアクセスの理解

- 平均レイテンシーはGPU/ワークロード/他によって変化し得る
- 人工のシェーダレイテンシーのテスト結果からパフォーマンスの急上昇[cliff]を推定できる
    - L1ヒットで約114クロック(peak 16 clock/VMEM return rate for image ops) (1 clock/VALU)
    - L2ヒットで約190クロック(L1ヒットと比べて平均で約76クロックの追加レイテンシー ... 76/16 = 4.75のイメージL1\$ヒット)
    - L2ミスで約350クロック(L1ヒットと比べて平均で約236クロックの追加レイテンシー ... 236/16 = 14.75のイメージL1\$ヒット)
- メモリスループットを減らすもの
    - リクエストキューがdrainingしない程度に高速なVMEM操作を維持しないこと
        - **メモリリクエストの通常フローを維持してみる(wave占有率の不規則さを探す、など)**
    - L1キャッシュミスやL2キャッシュミスによって引き起こされるメモリバブル
        - **空間的および時間的にデータ局所性を改善して、L1ヒット率を上げてみる**

# Idle Bound

# Idle Boundの例 --- スライド1

GPU Draining
アイドル込みのバリア
直列の依存性＋小さなジョブ

# Idle Boundの例 --- スライド2

単純なジオメトリではGPUは飽和しない
GPU Draining
SIMULTANEOUS_USE_BITのコマンドバッファは並列性を妨げる

# 1/4解像度パスのDrainとFillに関する数

最初のPSのwaveへのフロントエンドのタイムスタンプでは2.2us(ここにはVSからPSへの遅延が含まれる)
PSのfill時間では1.2us(50%以下がアクティブ)
PSのdrain時間は5.9us(約50%がアクティブ)
バリアに2.6us(キャッシュのフラッシュを含む)
計59.9usのうち51.5usがアクティブ
↑このポストパスではGPUの最低でも14%のVALUがアイドル

独立した第2、第3のPS描画を伴う並列でのPSクリア(48us中35usのVALU処理)
100%として数える(実際にはこうはならないだろう)
vkCmdClearAttachmentsはVS+PSクリアを生成、VSからPSへの遅延に注意
最小のオーバーラップ、帯域幅boundのPSクリア(VALUなし)が第2PSの起動を遅らせる
↑このポストパス＋クリアではGPUの最低でも26%のVALUがアイドル

# GPUを埋める --- コールドキャッシュの話

- 64のCU(Vega) = 2560のwavesを起動できる(ピーク占有時なら約160Kiのwork items)
- "First-Fill Waves" = コールドキャッシュ条件で始まるwaves
    - 2560x1440のフルスクリーンパスの最大4.4%がfirst-fill wavesである
    - 1/4領域のポストパスの最大17%がfirst-fill wavesである(DOF/MB/他で一般的)
    - 1/16領域のポストパスの71%はfirst-fill wavesである(リダクションパスで一般的)
- GPU時間の大半がコールドキャッシュに費やされる可能性がある
    - CSのディスパッチ: wave起動率、Iキャッシュミス、Kキャッシュミス、従属的フェッチ[dependent fetches]
        - = **アイドル状態のマシンにおける隠蔽できない長いレイテンシー連鎖**

# RGPに戻って、それに関するコールドキャッシュの話

- AMD内部の命令トレース
    - Iキャッシュミス
    - ほぼアイドル
    - GPU満杯
    - GPUが満杯になって少し経ってから効率的な実行に達する
- RGPからの見え方
    - バリアのアイドル時間
    - GPU満杯
    - GPUが満杯になって少し経ってから効率的な実行に達する

# アイドルbound --- Takeaway

- 実践でのAmdahlの法則
- 描画外の時間
- 描画のシリアライズ = GPUサイズでスケールを止めるパフォーマンス
- パイプラインへの時間

# DCC & CS VS PS

# 低速クリアを回避する

- CSパスで書かれるイメージへのクリアを避ける
- フルスクリーントライアングルされる(イメージの大多数を上書きする)イメージへのクリアを避ける
- 圧縮済みサーフェスに対する、すべてを0.0か1.0でクリアすることによるアドバンテージ

VS処理はクリアと並列に動作できるが、PSはクリアの終わりまでブロックされる

低速クリア
圧縮されたクリア
低速クリア
VALUはほぼアイドル状態
クリアの終わり

# GCNにおけるPSとCS

- PS --- 一般的に次のVキャッシュに移動する前にVキャッシュあたり1から4の間のwavesを起動する --- Ordered export
    - DCC|Z圧縮出力をサポートする(圧縮はロードのレイテンシーを増加させる可能性があることに注意)
    - キャッシュは前のパスで書かれたレンダターゲットを読むためにフラッシュされる
    - VSからPSへの遅延はVSが長いレイテンシー連鎖やコールドキャッシュを持つときに実体化する可能性がある

CPマーカー(描画開始)と第1PS waveの起動の間はほぼ4000クロック

キャッシュミスが少ない = VSからPSへの遅延が小さい

- CS --- 次のVキャッシュに移動する前にVキャッシュあたりひとつのworkgroupを起動する --- exportなし(順不同)
    - PSパスで書かれた圧縮イメージを読むことができるが、DCC|Z圧縮イメージを格納できない
    - シェーダ読み込みからシェーダ書き込みへの遷移におけるキャッシュのフラッシュを必要としない
    - グラフィクスキューでパイプライン化させて、または、CSキューで非同期に実行し易い

# Delta Color Compression (DCC)のON/OFF

- PCのGPUでのDCCは利点にも欠点にもなり得る
    - レイテンシーの増加に対して、帯域幅が減少する可能性(メタデータをフェッチする必要がある)
- **DCCを使う時と使わない時を選ぶためにプロファイルするのに最適なこと**
    - STORAGEかUAVのusageフラグを用いることでレンダターゲットでのDCCを無効化できる
    - ONかOFFの二者択一ではなく個々のレンダターゲットで試すのがより良い
- あるゲームパスに対してDCCを有効化した場合のパフォーマンス差の例(Vegaの数値)
    - ディファードシェーディングで-5.0% (より遅い)
    - Screen Space Reflectionで-3.1%
    - Screen Space Occlusionで1.8%
    - 合成パスで2.2%
    - ポストプロセッシングで3.8 (より速い)

# イベント付きパイプライン化

# 理想のトライアングルベースエンジンの著者の見解

- ジオメトリをレンダリングするときはVS+PSのみ、その他ではCS
- **CSは容易にパイプライン化する能力を最大化する**

# パイプライン化

- 前の仕事のdrainを穴埋めしながらコールドキャッシュのレイテンシーを隠蔽する
- まずパイプラインのAPIイリーガルなテクニックを探す: バリアを外して、データ競合をテストする
    - ハードウェアを理解するのに有用
    - APIリーガルなソリューションに対してテストするためにパフォーマンス上限を推定するのに役立つ
    - ある種の実行依存性が必須であることを証明するにの役立つ
- APIリーガルなソリューションは後述する

# 単純なデータ競合テスター

# 単純なデータ競合テスター: データ競合は起こり得るという結果に

- RX 580で様々なイメージサイズを動作させた結果
    - 競合なし: 256x256以上
    - 競合あり: 128x128以下 --- ある形式の実行依存性が実際には必要であるという評価結果
- あるケースでは要素の組み合わせがデータ競合を不確実にする
    - コヒーレントでない(GLC=0)ストアは依然としてコヒーレントなL2キャッシュにwrite-throughする
    - Step#2のwavesはStep#3のwavesの前に起動する必要がある(APIのサブミッション順序で起動、順不同に完了)
    - GPUのlooseなworkgroupの起動はディスパッチ順(大まかに右から左、その後、上から下)

ロードの開始前にストアが見える

この挙動に依存できない
GPUサイズやワークロードがデータ競合の発生確率を調整する可能性がある

# GLSLにおけるcoherentメモリ修飾子の活用

- coherentはGCNにおけるGLC=1のストアに翻訳される。これは、L1キャッシュを迂回し、コヒーレントなL2キャッシュに格納される
    - 例: `layout(set=0, binding=5, rgba8) uniform coherent image2D imgA;`
- ARB_shader_image_load_store --- *"[coherent]変数を通してアクセスされるバッファオブジェクトまたはテクスチャイメージメモリは、他のいかなるシェーダinvocationによって発行されるストアによってキャッシュが自動的に更新される場合に限り、キャッシュされてもよい。"
    - コヒーレントなストアはコヒーレントでないキャッシュを迂回する必要があると明言されている
- **重大な観察結果** : ロードはcoherent修飾子を使う必要がなく、以下の使い方のモデルの下でキャッシュさせることができる
    - 新鮮なキャッシュを保証するためのフレームの開始/終了のキャッシュフラッシュ
    - いずれかのリードの前のコヒーレントなストアで一度だけキャッシュラインに書き込む(実行依存性を必要とするのみ、いわゆるVkEvent)
    - 何回もコヒーレントでないキャッシュ(いわゆる最高のパフォーマンス)を介してキャッシュラインを読み出す

The Perfect Tool For Pipelining In Vulkan(R)

# イベントによるパイプライン化の機構

- 手動でひとつのディスパッチを2つに分ける
    - 分割は最大フィルタカーネル窓に基づいて変えなければならないだろう

# いずれかの読み込みの前の一度だけコヒーレントな書き込み、そして、キャッシュ済み読み込み

- APIはGPUのキャッシュラインの大きさを開示していおらず、ハードウェアはキャッシュラインの大きさを自由に変えられる
- 2018年3月現在、"安全な"分割ライン粒度[split line grahularity]を報告するAPIインターフェイスは存在しない
    - 分割がキャッシュラインの境界をまたがないことを確実にするため
- 現在のワークアラウンドは必要なアラインメントを大きく過剰に推定すること
    - 分割オフセットは大きな2の累乗の値にすべき
    - 64バイトキャッシュラインでのひとつのハードウェアswizzleパターンで混ぜられる32ビット/テクセルは4テクセルのアラインメントを必要とするかもしれない
    - 128バイトキャッシュラインを持つマシンでの16ビット/テクセルのイメージは8テクセルのアラインメントを必要とするかもしれない
    - **おそらく64テクセルのアラインメントにパッドするのが良い --- キャッシュラインサイズとswizzleパターンにおける変更に対する柔軟性を確保する**

# 単純なデータ競合テスター: Vegaにおける1152x1152の評価結果

- 1.00 time: データ競合テスター(APIリーガルではない)
    - 手動パイプライン化の上限の限界の可能性
- 1.31 time: バリアの再導入
    - APIリーガルの一般的なプラクティス(timeはバリアを含む)
    - 過度に保守的なドライバはL2キャッシュをフラッシュさせる
        - または、L2キャッシュのフラッシュを必要とする使い方を持ったバリアかも
- **1.03 time: イベントによる手動分割パイプライン**
    - イメージバリアではなく"coherent"なGLC=1のストアを使う
    - オススメの"安全"でAPIリーガルなソリューション

# CSとGFXキューの間の違い

- グラフィクスキューはフロントエンドの操作をパイプライン化するのに優れている(しばしば並列に40超の描画が確認される)
    - しばしばグラフィクスキューで小さなジョブをパイプライン化するのに優れる
- コンピュートキューはフロントエンドのオーバーヘッドを隠すためにより長く動作するwavesを必要とする

限られたwavesの起動
GPUの非パイプライン化

シグナル化したVkEventの待機はメモリのフェッチを必要とする(隠すにはより長いwavesが必要)

長く動作するwavesが良い(待機のフェッチを上手く隠す)

# パイプライン化された世界で隔離したタイミング計測

- 古いパイプライン化されていない世界は最初のwaveの起動から最後のwaveの終わりまでの時間を最小化することに注目する
    - これは、fillおよびdrain時間を最小化することを暗に示し、(長いdrainに由来する)長く動作するwavesを避けることを暗に示す！
- 新しいパイプライン化された世界は平均"wave実行時間/wave占有率"を最小化することに注目する
    - 最適化すべきことを変更する(drain時間の最小化には興味がない)
    - 孤立して時間を計測したシェーダに対して計測すべきことを変更する(drainとfillを計算に入れる)
- どのシェーダが速い？(それは、ディスパッチがパイプライン化されているかどうかに依存する)

# RGPにおけるパイプライン化されたタイミイングの推定 --- 大雑把だけど有用

0.5 * (青で)選択した満杯の領域の経過時間 + 0.5 * 完全な描画の経過時間

- 使用率の傾斜に起因するdrainとfillの50%使用率推定の動機は命令トレースで確認できる

# イベントによるパイプライン化 --- Takeaway

- Vulkan
    - **パイプライン化されたCSはVkEventと"coherent"キーワードによりとても良く動作する**
    - キャッシュのフラッシュを回避することで得をする可能性
    - GPUがdrainするのを回避することで得をする可能性
    - このトークの時点では(2018年3月)
        - バリア: 過度に保守的なキャッシュのフラッシュが起こり可能性があり、我々は改善しようとしている
- Direct3D 12
    - バリアを回避するために独立した仕事を組とし、グラフィクスdrainを満たすため非同期コンピュートを活用する
    - HLSLでの"coherent"は"globallycoherent"にあたる
    - このトークの時点では(2018年3月)
        - 分割バリア: 過度に保守的である可能性があり、我々は改善するための選択肢に注目している

# GPU使用率

# 様々なワークロードによる命令トレース集

<font color="red">➡</font>ダウンサンプリング    ←4つのほぼ空の緑色のSIMDの行　**VALUアイドルに到達**
モーションブラー
GPUの描画
ライト遮蔽
<font color="red">➡</font>水平ブラー    赤い矢印　**完全にメモリ限界[bound]**
<font color="red">➡</font>SSR
Gバッファ埋め
<font color="red">➡</font>SSAO
ディファードシェーディング
TAA
トーンマッピング＆諸々    4つのほぼいっばいの緑色のSIMDの行　**VALU限界に到達**

# "ドロー/ディスパッチ内部"に起因する利用率の傾向

- GPUがしばしばVALUアイドルとなる = 仕事を行うべき大量の計算能力が使用されていない！
    - 固定機能の制限によるボトルネックを回避するためにエンジニアリングする
    - メモリ限界になるのを回避するためにパスをマージする(例えば、ダウンサンプリングを1パスに)
    - 非同期コンピュートを活用する
    - ...
- いくつかのパスはVMEM要求キューを完全に飽和させている
    - 最適化できる余地はある
    - キャッシュの局所性を改善することに注目する
    - Vキャッシュのマッピング処理を改善することで
    - 処理のスケジューリングを改善することで
    - アクセスパターンを改善することで
    - ...

# wave起動率

# GCNのwave起動率はGPU間で変化する

- SE = Shader Engines --- 各SEは4クロックごとに1つのwaveを起動できる
- CU = Compute Units --- CU数はGPUの大きさを決める
- SE:CU比はそのマシンが自身をどれだけ速くfillできるかを決定する
    - 1:16 --- Fury X、Nano、Vega 64    ← GPUが大きくなればfillにより時間がかかるので、より長く実行するwavesの方が良い
    - 1:14 --- Fury、Vega 56
    - 1:11 --- R9 390X
    - 1:10 --- XBox 1X
    - 1:9 --- PS4、PS4 Pro、RX 480、RX 580
    - 1:8 --- R9 380X、RX 470、RX 560、RX570
    - 1:7 --- RX 460
    - 1:6 --- XBox 1    ← GPUが小さくなればfillが速くなる

# 短く実行するwavesは大きなGPUほどボトルネックになり得る

# 短く動作するPSが非同期コンピュートをストールさせるもうひとつの例

# Workgroupの最適化

# PSからCSへの変換の一般的なプラクティス

- PSをCSでの{8, 8, 1}のworkgroupに移す
    - 局所性において、Vキャッシュあたり1wave(PSより悪くなり得る)
    - GCNはCUあたり1つのworkgroupを起動する(CUあたり1つのVキャッシュ)
        - 次のCUに移る前に
- レーンから8x8のタイルへのマッピングは"線形ブロック"である
    - texture fetch quadに対して貧弱な{4x1}パターンとなる可能性
    - Vキャッシュのバンク衝突の可能性
    - GCNのサンプリングは4レーンのグループで動作する

# "最適化"(注)された構成の例

- {32, 16, 1}に構成される{512, 1, 1}のworkgroup
    - Vキャッシュあたり8waveの局所性
    - 各waveは8x8のタイルとなる
    - 8wavesは8x8タイルの4x2の集まりに組織化される
- レーンから8x8タイルへのマッピングは"swizzleされたブロック線形"
    - texture fetch quadに対して良好な{2x2}パターン
- (注) 特定のシェーダに対して最適化される
    - これは、キャッシュヒットに対する2D局所性に強く依存する
    - そして、wave起動の問題にあまり依存しない

# 前のスライドの{512, 1, 1}から{32, 16, 1}への再構成コード

```glsl
// GCNでは追加のVALUオーバーヘッドとなる5つの命令
uvec2 Remap(uint a) {
    uint y = bitfieldExtract(a, 3, 4);  // v_bfe_u32 ---> {...0, y3, y2, y1, x2}
    y = bitfieldInsert(y, a, 0, 1);     // v_bfi_b32 ---> {...0, y3, y2, y1, y0}
    uint x = bitfieldExtract(a, 1, 3);  // v_bfe_u32 ---> {...0, x2, x1, x0}
    a = bitfieldExtract(a, 4, 5);       // v_bfe_u32 ---> {...0, x4, x3, y3, y2, y1}
    x = bitfieldInsert(a, x, 0, 3);     // v_bfe_u32 ---> {...0, x4, x3, x2, x1, x0}
    return uvec2(x, y);
}

// シェーダでの使い方
uvec2 xy = Remap(gl_LocalInvocationID.x);

// Vegaでは2命令
xy.x += gl_WorkGroupID.x << 5;  // v_lshl_add_u32
xy.y += gl_WorkGroupID.y << 4;  // v_lshl_add_u32
```

# 異なる構成でパフォーマンス計測されたCSのSSAO

- 実行時間で減少が43%に最適化される(黄色の場合)
    - {64, 1, 1}を{8, 8, 1}にリマップ: 1.93 ms (ベースとなるが、ブロックはswizzleされる)
    - {128, 1, 1}を{8, 16, 1}にリマップ : 1.69 ms
    - {8, 32, 1}リマップなし : 1.57 ms (線形ブロック)
    - {256, 1, 1}を{8, 32, 1}にリマップ : 1.36 ms
    - {512, 1, 1}を{16, 32, 1}にリマップ : 1.24 ms
    - {1024, 1, 1}を{8, 128, 1}にリマップ : 1.22 ms (non-pinnedなワークロードに対してworkgroupが大きすぎる)
    - {512, 1, 1}を{8, 64, 1}にリマップ : 1.15 ms (VALU変換がより簡単だが、L1キャッシュのパフォーマンスが悪い)
    - {512, 1, 1}を{32, 16, 1}にリマップ : 1.09 ms (前のスライドの変換を使用)
- ただし、大きなworkgroupsが常にこの種のゲインを生み出すわけではない
    - 特に非同期コンピュートでは
    - 次のスライドではこの挑戦の更なる深淵に潜っていく…

# ダイナミクスGCN Workgroup起動

- Vキャッシュあたり1つ(CUあたり1つ)が起動されるworkgroupsは仕事をディスパッチできるCUsに渡ってラウンドロビンする
    - つまり、Vキャッシュのキャッシュ局所性には主な2つの選択肢がある: <font color="red">workgroupsを多くする、</font>または、**ひとつのwaveで多く仕事をする**
- 各SEでは、waveひとつは4クロックごとに起動できるが、wavesはそれより速く終了する可能性がある
    - 高速にマシンを再fillできるとは限らない
- workgroupsはLDSの連続ブロックを必要とし、wavesは起動するのにVGPRs/SGPRsの連続ブロックを必要とする
    - <font color="orange">小さなworkgroups = 起動しやすい</font>、<font color="red">{サイズ、wave実行時間、など}における不規則性が実行時間の空白を生み出す可能性がある</font>

**ライトプロブを適用する** --- workgroupあたり1waveがより高速

**ライトプロブを適用する** --- workgroupあたり8waveだが、SIMDあたり6wavesのみが一致

# 半永続的waves

# 半永続的wavesの目標

- しばしば、大きなCSのworkgroupsを使うことは結果として起動問題となる(特に非同期コンピュートで)
- **メモリ局所性の利益を得、単一waveのworkgroupで維持する他の方法が好ましい**
    - 前のセクションでの良好な2x2クアッドに対するwave-sizedな{64, 1, 1}から8x8へのリマップを用いたい
- 現在のPCのグラフィクスAPIはマシンを一度だけfillするためにディスパッチする機能が欠けている
- なので、完全に永続的なworkgroupsはまだ実践的ではない
- しかしながら、半永続的なwavesは興味深い妥協案[middle ground]となり得る

# 半安定状態[semi-steady-state]の実行時における短いシェーダの解剖

- waveの再起動
    - 仕事を分解[factor]する能力を破壊する(例えば、定数をリロードする代わりに再使用したい)
    - シェーダの終わりでのレイテンシーを隠蔽する能力を破壊する
    - 再起動はwaveリソースが使われない、多大な影響を及ぼす時間となる可能性がある
- 半永続的なwave --- 代わりに、ひとつのwaveでN個のシェーダのインスタンスのいたるところでループできる
    - waveの開始/終了/再起動のオーバーヘッドを償却できる

# データ局所性と起動粒度を成り行きに任せない

- {8, 8, 1}のworkgroup、水平ののち垂直カーネル、実行の後にヒットが起こる
- {64, 1, 1}から8x8へのリマップ、同じカーネル、同じ占有率、若干異なる実行、より良いキャッシュレート

# 半永続的なwaveの起動

- 多waveのworkgroupを単wave-sizedのworkgroupにループのアンロールを介して変換する
    - groupの次元はデータ局所性を最大化するために設定する(すなわち、垂直ブラーには垂直グルーピングを用いる)
    - ディスパッチサイズを減らす
- 水平ブラーシェーダの例

# waveの実行とスケジューリングを理解する

- 一番古いwaveが最初にスケジューリングされる優先度の効果を示す
- wave起動順とテクスチャフェッチの大半が発生するときを強調する
- 一番古いwaveが最初にスケジューリングされる優先度はあるwaveでのキャッシュが再利用される機会を最大化しようとする
    - **半永続的なworkgroupに対するアンロールとの良いペアリング**

# 単純な水平ブラーの例

- 8x8タイルが単純な9タップのボックスブラーと1回のストアを行う
- 半永続的なwavesのunrollingはフィルタタップを共有する --- コンパイラは再フェッチを回避するようにこれを最適化する

unroll間の8テクセル間隔

- VMEM命令の数によって初期のスケーリングを推定できる(キャッシュヒット率が変化する可能性を無視する)
    - 1.00 ops : 8x8 unroll 8 times = 10 VMEM ops + 9 VMEM ops * 7 times = 73 VMEM ops / 8 tiles =  9.125 ops/tile
    - 1.01 ops : 8x8 unroll 4 times = 10 VMEM ops + 9 VMEM ops * 3 times = 37 VMEM ops / 4 tiles =  9.250 ops/tile
    - 1.04 ops : 8x8 unroll 2 times = 10 VMEM ops + 9 VMEM ops * 1 times = 19 VMEM ops / 2 tiles =  9.500 ops/tile
    - 1.10 ops : 8x8 no unroll . . . . = 10 VMEM ops . . . . . . . . . . . . . . . . . . . . . = 10 VMEM ops / 1 tiles = 10.000 ops/tile

# 単純な水平ブラーの例 --- 実行時間の計測

- **半永続的はVMEM ops数のみに基づく期待より良く動作している**
    - キャッシュヒット率の上昇に成功したことを示唆している
- 1.00 time : 1.00 ops : Semi-persistent 8x8 running 64x8 (aka unroll 8x)
- 1.04 time : 1.01 ops : Semi-persistent 8x8 running 32x8 (aka unroll 4x)
- 1.10 time : 1.04 ops : Semi-persistent 8x8 running 16x8 (aka unroll 2x)
- 1.22 time : 1.10 ops : Classic 8x8 linear

大幅なdrain効果 --- パイプライン化することを確実にする

# 単純な水平ブラーの例 --- RX580のwave制限時の結果

- 半永続的なwaveの例
    - いずれの非パイプライン化よりも高速
    - または、最適なパイプライン化された実行よりも

# workgroup最適化＋半永続的なwaves --- Takeaway

- 最適化のための2つの有用なツール
- **完全にメモリ限界の状況でもゲインを提供できる**
- 半永続的なwavesを介するレーンマッピングにwork-itemをun-pinningすることはwhat we have time to dive into today以上の利点がある…

# おわり

# XOR RAX, RAX; XOR [RAX], RAX;

- トーク後のフォローアップ: timothy.lottes@amd.com
- ...

# 免責＆帰属

# AMD
