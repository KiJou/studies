---
title: CUDA Performance [@Cozzi2017d]
numberSections: false
---
# 並列リダクション[Parallel Reduction]

<!-- p.9 -->

```cuda
__shared__ float partialSum[];
// ... 共有メモリに読み込む
unsigned int t = threadIdx.x;
for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {
    __syncthreads();
    if (t % (2 * stride) == 0)
        partialSum[t] += partialSum[t + stride];
}
```

<!-- p.10 -->

1-2行目: 共有メモリでの要素の総和の計算

<!-- p.11 -->

4行目: stride = 1, 2, 4, ...

<!-- p.12 -->

5行目: なぜ？

<!-- p.13 -->

- 6-7行目:
    - 同じ共有メモリで総和を計算する
    - `stride`が増えると、スレッドはどうなる？

<!-- p.15 -->

- 第1パス: スレッド1、3、5、7は何もしない
    - 本当は$n$個の要素に対して$n/2$個のスレッドのみが必要である

<!-- p.16 -->

- 第2パス: スレッド2、6は何もしない

<!-- p.17 -->

- 第3パス: スレッド4は何もしない

<!-- p.18 -->

- 一般に、必要なスレッド数は各パス後に半分になる

<!-- p.19 -->

- 実装を*調整*したらどうだろう？

<!-- p.24 -->

```cuda
__shared__ float partialSum[];
// ... load into shared memory
unsigned int t = threadIdx.x;
for (unsigned int stride = blockDim.x / 2; stride > 0; stride /= 2) {
    __syncthreads();
    if (t < stride)
        partialSum[t] += partialSum[t + stride];
}
```

<!-- p.27 -->

- 第1パス: スレッド4、5、6、7は何もしない
    - 本当は$n$個の要素に対して$n/2$個のスレッドのみが必要である

<!-- p.28 -->

- 第2パス: スレッド2、3は何もしない

<!-- p.29 -->

- 第3パス: スレッド1は何もしない

<!-- p.30 -->

- 何が違う？

<!-- p.31 -->

- `stride = 1, 2, 4, ...`
    - `if (t % (2 * stride) == 0)`
- `stride = ..., 4, 2, 1`
    - `if (t < stride)`

# Warp分割[Warp Partitioning]

- *Warp分割*: ひとつのブロックからのスレッドをWarpに分ける方法
- Warp分割の情報は以下で使われる
    - divergent branchesを最小化する
    - Warpを早期にリタイアさせる

<!-- p.33 -->

- *連続的に増加する*`threadIdx`に基づいて分割する

<!-- p.34 -->

- 1Dブロック
    - `threadIdx.x`は0から1023まで(Fermi以降)
        - `threadIdx.x`は0から511まで(G80/GT200)
    - $n$番目のWarp
        - $32n$番目のスレッドから始まる
        - $32(n+1)-1$番目で終わる
    - ブロックサイズが32の倍数でないならば、最後のWarpはパディングされる

<!-- p.35 -->

- 2Dブロック
    - `threadIdx`の増加は以下を意味する
        - `threadIdx.x`は増加
        - 行は`threadIdx.y == 0`から開始
- 例えば、64x8ブロックでは

<!-- p.37 -->

- 3Dブロック
    - `threadIdx.z == 0`から始まる
    - 2Dブロックと同様に分割する
    - `threadIdx.z`を増加し、繰り返す

<!-- p.38 -->

*divergent branchesはWarp内部にある*

<!-- p.39 -->

- `warpSize == 32`では、以下のコードではいずれのWarpもdivergent branchを持つ

```cuda
if (threadIdx.x > 15) {
    // ...
}
```

<!-- p.40 -->

- `warpSize > 1`では、以下のコードではいずれのWarpもdivergent branchを持つ

```cuda
if (threadIdx.x > warpSize - 1) {
    // ...
}
```

<!-- p.41 -->

- Warp分割の情報があるとすると、どちらの並列リダクションがより良いか？
    - `stride = 1, 2, 4, ...`
        - `if (t % (2 * stride) == 0)`
    - `stride = ..., 4, 2, 1`
        - `if (t < stride)`

<!-- p.42 -->

- `warpSize == 2`としてみる

<!-- p.43 -->

- 第1パス
    - `stride = 1, 2, 4, ...`
        - 4つのdivergent branches
    - `stride = ..., 4, 2, 1`
        - divergent branchesなし

<!-- p.44 -->

- 第2パス
    - `stride = 1, 2, 4, ...`
        - 2つのdivergent branches
    - `stride = ..., 4, 2, 1`
        - divergent branchesなし

<!-- p.45 -->

- 第3パス
    - `stride = 1, 2, 4, ...`
        - 1つのdivergent branch
    - `stride = ..., 4, 2, 1`
        - 1つのdivergent branch

<!-- p.47 -->

- 良い分割はWarpが早期にリタイアできるようにする
    - より良いハードウェアの利用

<!-- p.48 -->

- 並列リダクション

<!-- p.49 -->

- 第1パス
    - `stride = 1, 2, 4, ...`
        - Warpのリタイアなし
    - `stride = ..., 4, 2, 1`
        - 2つのWarpがリタイア

<!-- p.49 -->

- 第2パス
    - `stride = 1, 2, 4, ...`
        - 2つのWarpがリタイア
    - `stride = ..., 4, 2, 1`
        - 1つのWarpがリタイア

# Memory Coalescing

- *グローバルメモリ* に *row-major* で格納された行列があるとすると、*スレッド* の望ましいアクセスパターンとは？

<!-- p.55 -->

- a) column after column
    - *個別のスレッド* が増加する連続的なメモリアドレスを読む
- b) row after row
    - *隣接するスレッド* が増加する連続的なメモリアドレスを読む

<!-- p.58 -->

- グローバルメモリの帯域幅(DRAM)
    - G80    ---  86.4GB/s
    - GT200  --- 150  GB/s
    - Fermi  --- 192  GB/s
    - Kepler --- 240  GB/s
- DRAMから大きく連続的な位置を要求することでピーク帯域幅を達成する
    - 無作為な位置へのアクセスはさらなる帯域幅の低下となる

<!-- p.59 -->

- *Memory coalescing* --- アクセスパターンを組み替えてパフォーマンスを改善する
- こんにちでは役立つが、大きなオンチップキャッシュではあまり役に立たなくなるだろう

<!-- p.60 -->

- GPUは *full-warp* での連続的読み込みを単一読み込みにcoalesce[合体]する
    - G80/GT200での *half-warp* (コンピュート1.x)
- **戦略**: coalesce可能な方法で共有メモリにグローバルメモリを読み出す
    - 最大帯域幅で共有メモリに無作為にアクセスする
        - *bank conflicts* を無視すると…

# Bank Conflicts

- 共有メモリ
    - ときどき*並列データキャッシュ*と呼ばれる
        - 複数のスレッドが同時に共有メモリにアクセスできる
    - メモリは *バンク[bank]* に分けられる

<!-- p.62 -->

- バンク
    - 各バンクは2サイクルあたり1アドレスを提供できる
    - バンクごとの帯域幅: 2(シェーダクロック)サイクルあたり32ビット
    - 連続する32ビットワードは連続するバンクに割り当てられる

<!-- p.63 -->

- *バンクコンフリクト*: 同じバンクだが異なるアドレスへの2つの同時アクセス
    - シリアライズされる
- G80-GT200: 16バンク、8SPで、並列実行
- Fermi以降: 32バンク、16SPで、並列実行

<!-- p.66 -->

- ファストパス その1(Fermi以降)
    - Warp内のすべてのスレッドが異なるバンクにアクセスする

<!-- p.67 -->

- ファストパス その2(Fermi以降)
    - Warp内のすべてのスレッドが同じアドレスにアクセスする
    - "ブロードキャスト"

<!-- p.68 -->

- スローパス(Fermi以降)
    - half-warp内の複数スレッドが同じバンクにアクセスする
    - アクセスがシリアライズされる
    - コストはどのくらい？

<!-- p.69 -->

```cuda
shared__ float shared[256];
// ...
float f = shared[index + s * threadIdx.x];
```

- `s`の値がどれくらいであれば、コンフリクトフリーだろうか？

<!-- p.71 -->

- プロファイラを使わずに、どうやってバンクコンフリクトの除去で期待できるスピードアップの種類を伝えることができるか？
- Warpで1つ以上のスレッドが同じ共有メモリアドレスに書き込む場合に何が起こる？(非アトミック命令)

# SM Resource Partitioning

- SMは動的に分割することを思い出そう

<!-- p.74 -->

- 以下が持てる
    - 96スレッドを8ブロック
    - 192スレッドを4ブロック
    - ただし、192スレッドを8ブロックにはできない

<!-- p.75 -->

- 以下が持てる(256スレッドブロックを仮定)
    - 768スレッド(3ブロック)それぞれで10レジスタを使う
    - 512スレッド(2ブロック)それぞれで11レジスタを使う

<!-- p.76 -->

- これ以上のレジスタはスレッドレベルの並列性を損なう
    - パフォーマンスをさらに増加させることはできるか？

<!-- p.77 -->

- *パフォーマンスの崖[cliff]*: リソース使用率の増加は並列性における劇的な減少を引き起こす
    - 例えば、グローバルメモリアクセスのレイテンシーを隠蔽しない限り、レジスタ数が増加する

# Data Prefetching

- グローバルメモリの読み込みとその利用の間の独立した命令はメモリレイテンシを隠蔽できる

```cuda
float m = Md[i];
float f = a * b + c * d;
float f2 = m * f;
```

<!-- p.82 -->

- *Prefetching* グローバルメモリからのデータはグローバルメモリの読み込みと使用の間の独立した命令を事実上増加させる

<!-- p.83 -->

- タイル化された行列の乗算を思い出そう:

```cuda
for(...) {
    // 現在のタイルを共有メモリにロードする
    __syncthreads();
    // 内積を累積する
    __syncthreads();
}
```

- プリフェッチ付きタイル化された行列の乗算:

```cuda
// 最初のタイルをレジスタにロードする
for(...) {
    // レジスタを共有メモリに預ける[deposit]
    __syncthreads();
    // 次のタイルをレジスタにロードする
    // 内積を累積する
    __syncthreads();
}
```

# Instruction Mix

- Special Function Unit(SPU)
    - `__sinf()`、`__expf()`を計算するために使う
    - 4つしかないけど、クロックあたり1命令を実行できる

# Loop Unrolling

```cuda
for (int k = 0; k < BLOCK_SIZE; ++k) {
  Pvalue += Ms[ty][k] * Ns[k][tx];
}
```

- イテレーションあたりの命令
    - 浮動小数点乗算が1つ
    - 浮動小数点加算が1つ
    - ほかには？

<!-- p.92 -->

- 他のイテレーションあたりの命令
    - ループカウンタの更新
    - 分岐
    - アドレス演算

<!-- p.93 -->

- 命令ミックス
    - 2つの浮動小数点演算命令
    - 1つのループ分岐命令
    - 2つのアドレス演算命令
    - 1つのループカウンタ増加命令

<!-- p.94 -->

- 1/3のみが浮動小数点計算
    - でも、完全な理論上の1TFLOPS(Fermi)が欲しい
    - *loop unrolling* を考慮する

<!-- p.95 -->

- ループしない
    - ループカウントの更新なし
    - 分岐なし
    - 定数インデックス -- アドレス演算命令なし

<!-- p.96 -->

- 自動的に:
- unrollingの欠点は？

# Thread Granularity

- どれだけの処理をスレッドがすべきか？
    - 並列リダクション
        - 2要素を減らす？
    - 行列の乗算
        - Pdの1要素を計算する？

<!-- p.99 -->

- 行列の乗算
    - Pdの両要素は同じMdの行を必要とする

<!-- p.100 -->

- 行列の乗算
    - 同じスレッドで両方のPdの要素を計算する
        - グローバルメモリアクセスを1/4減らす
        - 独立した命令数を増加させる
            - その利点は？
        - 新しいカーネルはより多くのレジスタと共有メモリを使う
            - これはどうゆう意味？
