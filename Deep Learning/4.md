# 数値計算

機械学習アルゴリズムは通常、大量の数値計算を必要とする。これは一般に、正しい解のための記号的な式をもたらすために数式を解析的に導出するのではなく、反復的な過程を通して解の推定値を更新する手法によって数学的問題を解くアルゴリズムを参照する。一般的な操作には最適化（関数を最小化または最大化する引数の値を求めること）や一次方程式の系を解くことが含まれる。ディジタルなコンピュータ上で数学的な関数を計算することだけでも、その関数が実数値を伴うとき、有限のメモリ量を用いて正確に表現できないので、困難である可能性がある。

## オーバーフローとアンダーフロー

ディジタルなコンピュータ上で連続的な数学を処理することの根本的な困難さは、有限のビットパターンで無限に多い実数値を表現する必要があることにある。これは、ほぼすべての実数値に対して、コンピュータで値を表現するとき、いくらかの近似誤差が発生することを意味する。多くの場合、これは単なる丸め誤差である。丸め誤差は、特に多数の操作に渡って構成されるときに問題となり、丸め誤差の蓄積を最小化するよう設計されていない場合に理論ではうまく機能するアルゴリズムが実践ではうまく機能しなくなることが起こり得る。
特に壊滅的な丸め誤差の一種をアンダーフロー[underflow]という。アンダーフローはゼロに近い値がゼロに丸められるときに発生する。多くの関数は、その引数が小さな正の数ではなくゼロであるとき、性質上異なる振る舞いを見せる。例えば、我々は通常ではゼロによる除算（あるソフトウェア環境はこれが発生したときに例外を投げるだろうし、その他は代用として非数の値で結果を返すだろう）やゼロの対数を取ること（これは通常では$-\infty$として扱われ、さらなる算術演算で使われると非数になる）を避けたい。
もうひとつの非常に有害な数値誤差の一種はオーバーフロー[overflow]である。オーバーフローは大きな絶対値を持つ値が$\infty$または$-\infty$として近似されるときに発生する。さらなる算術は通常ではこれらの無限大の値を非数に変化させるだろう。
アンダーフローやオーバーフローに対して安定化させなければならない関数の一例としてsoftmax関数がある。softmax関数はしばしばmultinoulli分布に関連付けられる確率を予測するのに使われる。softmax関数は以下のように定義される。

$$
\text{softmax}(\boldsymbol{x})_i = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)}
$$

すべての$x_i$がある定数$c$に等しいとき何が起こるかを考えよう。解析的には、すべての出力が$\frac{1}{n}$に等しいはずであることが分かる。数値的には、これは$c$が大きな絶対値を持つときには起こらないかもしれない。$c$が非常に大きな負数である場合、$\exp(c)$はアンダーフローを起こすだろう。これはsoftmaxの分母が$0$になるであろうことを意味する。つまり、最終結果は未定義である。$c$が非常に大きな正数であるとき、$\exp(c)$はオーバーフローを起こし、同様に、式全体が未定義となるだろう。これら両方の困難さは$\boldsymbol{z} = \boldsymbol{x} - \max_i x_i$とする$\text{softmax}(\boldsymbol{z})$を代わりに計算することで解決することができる。単純な代数学は、softmax関数の値が入力ベクトルからスカラを足したり引いたりすることによって解析的に変化しないことを示す。$\max_i x_i$を引くことは$\exp$のlargest argumentを$0$にすることになる。これは、オーバーフローの可能性を排除する[rule out]。同様に、少なくとも1つの分母の項が$1$の値を持つ。これは、ゼロによる除算をもたらす分母におけるアンダーフローの可能性を排除する。
1つの小さな問題が依然として存在する。分子におけるアンダーフローは依然として式全体をゼロにしてしまう可能性がある。これは、はじめにsoftmaxのサブルーチンを実行し、その後に$\log$関数へその結果を渡すことで$\log \text{softmax}(\boldsymbol{x})$を実装すれば、誤って$-\infty$を得ることができてしまうであろうことを意味する。代わりに、我々は数値的に安定な方法で$\log \text{softmax}$を計算する別個の関数を実装しなければならない。$\log \text{softmax}$関数はsoftmax関数を安定化するのに使ったのと同じトリックを用いて安定化することができる。
ほとんどの部分で、我々は本書で述べられる様々なアルゴリズムを実装する際に伴う数値的な懸念事項のすべてを明示的に詳述したりはしない。低レベルライブラリの開発者は深層学習アルゴリズムを実装するときに数値的な問題を気に留めておくべきである。ほとんどの本書の読者は安定的な実装を提供する低レベルライブラリに単純に頼ることができる。ある場合には、新しいアルゴリズムを実装し、自動的に安定化された新しい実装を得ることができる。Theano[@Bergstra2010; @Bastien2012]は深層学習の文脈で発生する多数の一般的な数値的に不安定な式を自動的に検出して安定化するソフトウェアパッケージの一例である。

## 悪条件

条件[conditioning]は関数がその入力における小さな変化に関してどれだけ急激に変化するかを示す。入力がわずかに摂動するときに急激に変化する関数は、入力における丸め誤差が出力の大きな変化になる可能性があるので、科学計算にとって問題となり得る。
関数$f(\boldsymbol{x}) = \boldsymbol{A}^{-1} \boldsymbol{x}$を考えよう。$\boldsymbol{A} \in \mathbb{R}^{n \times n}$が固有値分解を持つとき、その条件数[condition number]は以下となる。

$$
\max_{i,j} \left| \frac{\lambda_i}{\lambda_j} \right|
$$

これは最大および最小の固有値の大きさの比である。この値が大きいとき、逆行列は入力における誤差に特に鋭敏である。
この感度は行列それ自体の生来の特性であり、逆行列を求める最中の丸め誤差の結果ではない。悪条件[poorly conditioned]の行列は、真の逆行列で乗算するとき、既存の誤差を増幅する。実際には、その誤差は逆行列を求めること自体の数値誤差によって更に悪化するだろう。

## 勾配ベースの最適化

ほとんどの深層学習アルゴリズムはある種の最適化を伴う。最適化とは、$\boldsymbol{x}$を代替することによってある関数$f(\boldsymbol{x})$を最小化または最大化のいずれかを行う作業を指す。我々は通常、$f(\boldsymbol{x})$の最小化に関するほとんどの最適化問題を表現するのに使う。最大化は$-f(\boldsymbol{x})$を最小化することで最小化アルゴリズムを介して達成できるだろう。
最小化または最大化したい関数は目的関数[objective function]とかcriterionと呼ばれる。これを最小化するとき、これをコスト関数[cost function]とか損失関数[loss function]とか誤差関数[error function]とも呼ぶかもしれない。本書では、ある機械学習の文献がこれらの用語のいくつかに特別な意味を割り当てているとしても、これらの用語を同義的に用いる。
我々はしばしば上付き文字$*$で関数を最小化または最大化する値を表記する。例えば、$\boldsymbol{x}^* = \argmin f(\boldsymbol{x})$と言えるだろう。
我々は読者がすでに微分積分学[calculus]に親しみがあると想定しているが、ここでは、微分積分学の概念がどのように最適化と関係しているかの簡潔な復習を提供する。
関数$y = f(x)$があるとする。ここで、$x$と$y$の両方は実数である。この関数の微分[derivative]は$f'(x)$とか$\frac{dy}{dx}$と表記される。微分$f'(x)$は点$x$における$f(x)$の傾斜をもたらす。言い換えれば、これは対応する出力の変化を得るために入力の小さな変化をスケールする方法を指定する。すなわち、$f(x + \epsilon) \approx f(x) + \epsilon f'(x)$である。
故に、微分は、$y$において小さな改良を加えるために$x$を変化させる方法を教えてくれるので、関数を最小化するのに有用である。例えば、我々は$f(x - \epsilon \text{sign}(f'(x)))$が十分に小さな$\epsilon$に対して$f(x)$より小さいことを知っている。従って、我々は微分の反対の符号を持つ小さな増分で$x$を動かすことで$f(x)$を減少させることができる。この技術は勾配降下法[gradient descent]と呼ばれる[@Cauchy1847]。この技術の一例は[@fig:4.1]を参照のこと。
$f'(x) = 0$であるとき、微分はどの方向に動くかといった情報をもたらさない。$f'(x) = 0$である点は臨界点[critical points]とか停留点[stationary points]として知られる。極小[local minimum]は$f(x)$がすべての近傍の点よりも小さい点である。つまり、微小増分を作ってもそれ以上に$f(x)$を減少させることができないということである。極大[local maximum]は$f(x)$がすべての近傍の点よりも大きい点である。つまり、微小増分を作ってもそれ以上に$f(x)$を増加させることができないということである。臨界点には最大でも最小でもない点が存在する。これらは鞍点[saddle points]として知られる。臨界点の各種の例は[@fig:4.2]を参照のこと。

![臨界点の種類。一次元における3種類の臨界点の例。臨界点はゼロの傾斜を持つ点である。そのような点は、近傍の点よりも小さい極小、近傍の点より大きい極大、その点自体よりも高い点と低い点の両方の近傍点を持つ鞍点、のいずれかとなり得る。](fig/4-2.png){#fig:4.2}

最も小さい$f(x)$の絶対値を得る点は最小[global minimum]という。関数には1つだけの最小、または、複数の最小が存在し得る。そこは大域最適[globally optimal]でない極小ともなり得る。深層学習の文脈において、我々は最適でない多数の極大や非常に平らな領域に囲まれた多数の鞍点を持ち得る関数を最適化する。このすべては、特に関数への入力が多次元であるとき、最適化を困難にする。故に、我々は通常、非常に小さければ正式に最小でなくても良い$f$の値を求めることで手打ちとする。一例は[@fig:4.3]を参照のこと。

![最小化の近似。最適化アルゴリズムは複数の極大または平坦域が存在するときに最大を求められないかもしれない。深層学習の文脈において、我々は一般に、真に最小でなかったとしても、それらがコスト関数の著しく低い値に対応している限り、そのような解を許容する。](fig/4-3.png){#fig:4.3}

我々はしばしば複数の入力を持つ関数を最小化する。すなわち、$f : \mathbb{R}^n \rightarrow \mathbb{R}$である。「最小化」の概念を理解するためには、依然としてたった1つ（スカラ）の出力である必要がある。
複数の入力を持つ関数では、偏微分[partial derivatives]の概念を用いなければならない。偏微分$\frac{\partial}{\partial x_i} f(\boldsymbol{x})$は点$\boldsymbol{x}$において$x_i$だけが増加するたびに$f$がどれだけ変化するかを測定する。勾配[gradient]とは、微分の考え方をその微分がベクトルに関するものである場合に一般化したものである。つまり、$f$の勾配はすべての偏微分から成るベクトルであり、$\nabla_\boldsymbol{x} f(\boldsymbol{x})$と表記される。勾配の要素$i$は$x_i$に関する$f$の偏微分である。複数の次元では、臨界点はすべての勾配の要素がゼロに等しい点である。
方向$\boldsymbol{u}$（単位ベクトル）における方向微分[directional derivative]とは方向$u$における関数$f$の傾斜である。別の言い方をすれば、方向微分とは$\alpha$に関する関数$f(\boldsymbol{x} + \alpha \boldsymbol{u})$の微分であり、$\alpha = 0$で計算される。連鎖律を用いて、$\frac{\partial}{\partial \alpha} f(\boldsymbol{x} + \alpha \boldsymbol{u})$は$\alpha = 0$であるときに$\boldsymbol{u}^\top \nabla_\boldsymbol{x} f(\boldsymbol{x})$に計算される。
$f$を最小化するため、我々は$f$が最速で減少する方向を求めたい。方向微分を用いてこれを行うことができる。

$$
\min_{\boldsymbol{u}, \boldsymbol{u}^\top \boldsymbol{u} = 1} \boldsymbol{u}^\top \nabla_\boldsymbol{x} f(\boldsymbol{x})
$$

$$
= \min_{\boldsymbol{u}, \boldsymbol{u}^\top \boldsymbol{u} = 1} \| \boldsymbol{u} \|_2 \| \nabla_\boldsymbol{x} f(\boldsymbol{x}) \|_2 \cos \theta
$$

ここで、$\theta$は$\boldsymbol{u}$とその勾配のなす角である。$\| \boldsymbol{u} \|_2 = 1$で置き換えて、$\boldsymbol{u}$に依存しない因数を無視すると、これは$\min_\boldsymbol{u} \cos \theta$に単純化される。これは$\boldsymbol{u}$が勾配と反対方向を指すときに最小化される。別の言い方をすれば、勾配は真っ直ぐの上り坂を指し、負の勾配は真っ直ぐの下り坂を指す。我々は負の勾配の方向に動かすことで$f$を減少させることができる。これは最急降下法[steepest descent]とか勾配降下法[gradient descent]として知られる。

最急降下法は新しい点を提案する。

$$
\boldsymbol{x}' = \boldsymbol{x} - \epsilon \nabla_\boldsymbol{x} f(\boldsymbol{x})
$$

ここで、$\epsilon$はステップの大きさを決定する正のスカラである学習率[learning rate]である。我々はいくつかの異なる方法で$\epsilon$を選択することができる。人気のアプローチは$\epsilon$を小さな定数に設定することである。時折、我々は方向微分を消失させるステップサイズを解くことができる。もうひとつのアプローチはいくつかの$\epsilon$の値に対して$f(\boldsymbol{x} - \epsilon \nabla_\boldsymbol{x} f(\boldsymbol{x}))$を計算して、目的関数の値が最も小さくなるものを選ぶという方法である。この後者の戦略は直線探索[line search]と呼ばれる。
最急降下法はすべての勾配の要素がゼロである（または、実際には、ゼロに非常に近い）ときに収束する。いくつかの場合では、この反復的なアルゴリズムを実行するのを回避して、$\boldsymbol{x}$に対して式$\nabla_\boldsymbol{x} f(\boldsymbol{x})$を解くことで臨界点に直接ジャンプすることができるかもしれない。
勾配降下法は連続空間における最適化に限定されるにもかかわらず、より良いconfigurationsに向けて反復的に（近似的に最適な小さな移動で）小さく移動させるという汎用的な概念は離散空間に一般化できる。離散パラメータの目的関数をascendingすることはhill climbingと呼ばれている[@Russel2003]。

### 勾配の先へ：ヤコビ行列とヘッセ行列

時折、我々はその入力と出力が共にベクトルである関数のすべての偏微分を求める必要がある。そのようなすべての偏微分から成る行列はヤコビ行列[Jacobian matrix]として知られる。具体的には、関数$\boldsymbol{f} : \mathbb{R}^m \rightarrow \mathbb{R}^n$がある場合、$\boldsymbol{f}$のヤコビ行列$\boldsymbol{J} \in \mathbb{R}^{n \times m}$は$J_{i,j} = \frac{\partial}{\partial x_j} f(\boldsymbol{x})_i$のように定義される。
我々は時折、微分の微分にも関心が出てくる。これは二階微分[second derivative]として知られる。例えば、関数$\mathbb{R}^n \rightarrow \mathbb{R}$に対して、$f$の$x_j$に関する微分の$x_i$に関する微分は$\frac{\partial^2}{\partial x_i \partial x_j} f$と表記される。単一の次元では、$f''(x)$で$\frac{d^2}{dx^2}f$を表記する。二階微分は一階微分がその入力を変化させるたびにどのように変化するかを教えてくれる。これは、勾配のstepがその勾配stepのみに基づいて期待されるであろうものと同量のimprovementを引き起こすことを教えてくれるので、重要である。我々は二階微分を曲率[curvature]を計測することとみなすことができる。二次方程式があるとする（実践で発生する多くの関数は二次方程式ではないが、少なくとも局所的には二次方程式でうまく近似できる）。そのような関数がゼロの二階微分を持つならば、それは曲面を持たない。それは完璧に平坦な線であり、その値は勾配のみを用いて推定できる。勾配が$1$であるならば、負の勾配に沿って大きさ$\epsilon$のステップを作ることができるので、コスト関数は$\epsilon$で減少するだろう。二階微分が負である場合、その関数は下向きに曲がっているので、コスト関数は実際には$\epsilon$以上に減少するだろう。そして、二階微分が正である場合、その関数は上向きに曲がっているので、コスト関数は$\epsilon$以下で減少し得る。様々な曲率の形状が勾配と真の値で推定されるコスト関数の値の間の関係にどのような影響を与えるかについては[@fig:4.4]を参照のこと。

![二階微分は関数の曲率を決定する。ここでは、様々な曲率を持つ二次関数を示す。破線は下向きの勾配ステップを作るのと同様に勾配情報だけに基づいて期待されるであろうコスト関数の値を示す。負の曲率では、コスト関数は実際には曲率の推定より速く減少する。曲率ゼロでは、勾配はその減少を正確に予測する。正の曲率では、その関数は予測よりも遅く減少し、最終的には増加し始める。つまり、大きすぎるステップは関数を実際にはうっかり増加させ得る。](fig/4-4.png){#fig:4.4}

我々の関数が複数の入力次元を持つとき、多くの二階微分が存在する。これらの微分はヘッセ行列[Hessian matrix]と呼ばれる行列に一緒に集めることができる。ヘッセ行列$\boldsymbol{H}(f)(\boldsymbol{x})$は以下のように定義される。

$$
\boldsymbol{H}(f)(\boldsymbol{x})_{i, j} = \frac{\partial^2}{\partial x_i \partial x_j} f(\boldsymbol{x})
$$

同等に、ヘッセ行列は勾配のヤコビ行列である。
二階偏微分が連続であるところならどこでも、微分作用素は可換的であり、すなわち、これらの順序は入れ替えることができる。

$$
\frac{\partial^2}{\partial x_i \partial x_j} f(\boldsymbol{x}) = \frac{\partial^2}{\partial x_j \partial x_i} f(\boldsymbol{x})
$$

これは$H_{i,j} = H_{j,i}$を暗示するので、ヘッセ行列はそのような点で対称的である。深層学習の文脈で出会う関数のほとんどはほとんどいたるところで対象的なヘッセ行列を持つ。ヘッセ行列は実対称行列であるので、実固有値の集合と固有ベクトルの直交基底に分解できる。単位ベクトル$\boldsymbol{d}$によって表現される特定の方向における二階微分は$\boldsymbol{d}^\top \boldsymbol{H} \boldsymbol{d}$で求められる。$\boldsymbol{d}$が$\boldsymbol{H}$の固有ベクトルであるとき、その方向における二階微分は対応する固有値で求められる。$\boldsymbol{d}$の他の方向に対して、その二階方向微分は、$0$から$1$の間の重みを持つ、すべての固有値の加重平均であり、$\boldsymbol{d}$との角度が小さい固有ベクトルほどより大きな重みを受け取る。最大の固有値は最大の二階微分を定め、最小の固有値は最小の二階微分を定める。
（方向）二階微分は、行われる勾配降下法のステップをどれだけうまく予測できるかを教えてくれる。我々は現在の点$\boldsymbol{x}^{(0)}$の周りの関数$f(\boldsymbol{x})$に対する二次のテイラー級数近似を作ることができる。

$$
f(\boldsymbol{x}) \approx f(\boldsymbol{x}^{(0)}) - (\boldsymbol{x} - \boldsymbol{x}^{(0)})^\top \boldsymbol{g} + \frac{1}{2}(\boldsymbol{x} - \boldsymbol{x}^{(0)})^\top \boldsymbol{H}(\boldsymbol{x} - \boldsymbol{x}^{(0)})
$$

ここで、$\boldsymbol{g}$はその勾配であり、$\boldsymbol{H}$は$\boldsymbol{x}^{(0)}$におけるヘッセ行列である。$\epsilon$の学習率を用いるならば、新しい点$\boldsymbol{x}$は$\boldsymbol{x}^{(0)} - \epsilon \boldsymbol{g}$で求められるだろう。これを近似に置き換えると、以下を得る。

$$
f(\boldsymbol{x}^{(0)} - \epsilon \boldsymbol{g}) \approx f(\boldsymbol{x}^{(0)}) - \epsilon \boldsymbol{g}^\top \boldsymbol{g} + \frac{1}{2} \epsilon^2 \boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}
$$

ここには3つの項がある。すなわち、関数の元々の値、関数の傾斜による推定されるimprovement、関数の曲率を計算に入れるために適用されなければならない補正、の3つである。この最後の項が大きすぎるとき、勾配降下法のステップは実際には上向きに移動する可能性がある。$\boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}$がゼロか負であるとき、テイラー級数近似は延々と$\epsilon$を増やすと延々に$f$が減少することを予測する。実際には、テイラー級数は大きな$\epsilon$に対して正確さを維持する可能性は低いので、この場合ではよりヒューリスティックな$\epsilon$の選択に頼らなければならない。$\boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}$が正であるとき、最も多く関数のテイラー級数近似を減少させる最適なステップサイズを解くと、以下を得る。

$$
\epsilon^* = \frac{\boldsymbol{g}^\top \boldsymbol{g}}{\boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}}
$$

最悪の場合、***
