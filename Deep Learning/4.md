# 数値計算

機械学習アルゴリズムは通常、大量の数値計算を必要とする。これは一般に、正しい解のための記号的な式をもたらすために数式を解析的に導出するのではなく、反復的な過程を通して解の推定値を更新する手法によって数学的問題を解くアルゴリズムを参照する。一般的な操作には最適化（関数を最小化または最大化する引数の値を求めること）や一次方程式の系を解くことが含まれる。ディジタルなコンピュータ上で数学的な関数を計算することだけでも、その関数が実数値を伴うとき、有限のメモリ量を用いて正確に表現できないので、困難である可能性がある。

## オーバーフローとアンダーフロー

ディジタルなコンピュータ上で連続的な数学を処理することの根本的な困難さは、有限のビットパターンで無限に多い実数値を表現する必要があることにある。これは、ほぼすべての実数値に対して、コンピュータで値を表現するとき、いくらかの近似誤差が発生することを意味する。多くの場合、これは単なる丸め誤差である。丸め誤差は、特に多数の操作に渡って構成されるときに問題となり、丸め誤差の蓄積を最小化するよう設計されていない場合に理論ではうまく機能するアルゴリズムが実践ではうまく機能しなくなることが起こり得る。
特に壊滅的な丸め誤差の一種をアンダーフロー[underflow]という。アンダーフローはゼロに近い値がゼロに丸められるときに発生する。多くの関数は、その引数が小さな正の数ではなくゼロであるとき、性質上異なる振る舞いを見せる。例えば、我々は通常ではゼロによる除算（あるソフトウェア環境はこれが発生したときに例外を投げるだろうし、その他は代用として非数の値で結果を返すだろう）やゼロの対数を取ること（これは通常では$-\infty$として扱われ、さらなる算術演算で使われると非数になる）を避けたい。
もうひとつの非常に有害な数値誤差の一種はオーバーフロー[overflow]である。オーバーフローは大きな絶対値を持つ値が$\infty$または$-\infty$として近似されるときに発生する。さらなる算術は通常ではこれらの無限大の値を非数に変化させるだろう。
アンダーフローやオーバーフローに対して安定化させなければならない関数の一例としてsoftmax関数がある。softmax関数はしばしばmultinoulli分布に関連付けられる確率を予測するのに使われる。softmax関数は以下のように定義される。

$$
\text{softmax}(\boldsymbol{x})_i = \frac{\exp(x_i)}{\sum_{j=1}^n \exp(x_j)}
$$

すべての$x_i$がある定数$c$に等しいとき何が起こるかを考えよう。解析的には、すべての出力が$\frac{1}{n}$に等しいはずであることが分かる。数値的には、これは$c$が大きな絶対値を持つときには起こらないかもしれない。$c$が非常に大きな負数である場合、$\exp(c)$はアンダーフローを起こすだろう。これはsoftmaxの分母が$0$になるであろうことを意味する。つまり、最終結果は未定義である。$c$が非常に大きな正数であるとき、$\exp(c)$はオーバーフローを起こし、同様に、式全体が未定義となるだろう。これら両方の困難さは$\boldsymbol{z} = \boldsymbol{x} - \max_i x_i$とする$\text{softmax}(\boldsymbol{z})$を代わりに計算することで解決することができる。単純な代数学は、softmax関数の値が入力ベクトルからスカラを足したり引いたりすることによって解析的に変化しないことを示す。$\max_i x_i$を引くことは$\exp$のlargest argumentを$0$にすることになる。これは、オーバーフローの可能性を排除する[rule out]。同様に、少なくとも1つの分母の項が$1$の値を持つ。これは、ゼロによる除算をもたらす分母におけるアンダーフローの可能性を排除する。
1つの小さな問題が依然として存在する。分子におけるアンダーフローは依然として式全体をゼロにしてしまう可能性がある。これは、はじめにsoftmaxのサブルーチンを実行し、その後に$\log$関数へその結果を渡すことで$\log \text{softmax}(\boldsymbol{x})$を実装すれば、誤って$-\infty$を得ることができてしまうであろうことを意味する。代わりに、我々は数値的に安定な方法で$\log \text{softmax}$を計算する別個の関数を実装しなければならない。$\log \text{softmax}$関数はsoftmax関数を安定化するのに使ったのと同じトリックを用いて安定化することができる。
ほとんどの部分で、我々は本書で述べられる様々なアルゴリズムを実装する際に伴う数値的な懸念事項のすべてを明示的に詳述したりはしない。低レベルライブラリの開発者は深層学習アルゴリズムを実装するときに数値的な問題を気に留めておくべきである。ほとんどの本書の読者は安定的な実装を提供する低レベルライブラリに単純に頼ることができる。ある場合には、新しいアルゴリズムを実装し、自動的に安定化された新しい実装を得ることができる。Theano[@Bergstra2010; @Bastien2012]は深層学習の文脈で発生する多数の一般的な数値的に不安定な式を自動的に検出して安定化するソフトウェアパッケージの一例である。

## 悪条件

条件[conditioning]は関数がその入力における小さな変化に関してどれだけ急激に変化するかを示す。入力がわずかに摂動するときに急激に変化する関数は、入力における丸め誤差が出力の大きな変化になる可能性があるので、科学計算にとって問題となり得る。
関数$f(\boldsymbol{x}) = \boldsymbol{A}^{-1} \boldsymbol{x}$を考えよう。$\boldsymbol{A} \in \mathbb{R}^{n \times n}$が固有値分解を持つとき、その条件数[condition number]は以下となる。

$$
\max_{i,j} \left| \frac{\lambda_i}{\lambda_j} \right|
$$

これは最大および最小の固有値の大きさの比である。この値が大きいとき、逆行列は入力における誤差に特に鋭敏である。
この感度は行列それ自体の生来の特性であり、逆行列を求める最中の丸め誤差の結果ではない。悪条件[poorly conditioned]の行列は、真の逆行列で乗算するとき、既存の誤差を増幅する。実際には、その誤差は逆行列を求めること自体の数値誤差によって更に悪化するだろう。

## 勾配ベースの最適化

ほとんどの深層学習アルゴリズムはある種の最適化を伴う。最適化とは、$\boldsymbol{x}$を代替することによってある関数$f(\boldsymbol{x})$を最小化または最大化のいずれかを行う作業を指す。我々は通常、$f(\boldsymbol{x})$の最小化に関するほとんどの最適化問題を表現するのに使う。最大化は$-f(\boldsymbol{x})$を最小化することで最小化アルゴリズムを介して達成できるだろう。
最小化または最大化したい関数は目的関数[objective function]とかcriterionと呼ばれる。これを最小化するとき、これをコスト関数[cost function]とか損失関数[loss function]とか誤差関数[error function]とも呼ぶかもしれない。本書では、ある機械学習の文献がこれらの用語のいくつかに特別な意味を割り当てているとしても、これらの用語を同義的に用いる。
我々はしばしば上付き文字$*$で関数を最小化または最大化する値を表記する。例えば、$\boldsymbol{x}^* = \argmin f(\boldsymbol{x})$と言えるだろう。
我々は読者がすでに微分積分学[calculus]に親しみがあると想定しているが、ここでは、微分積分学の概念がどのように最適化と関係しているかの簡潔な復習を提供する。
関数$y = f(x)$があるとする。ここで、$x$と$y$の両方は実数である。この関数の微分[derivative]は***
