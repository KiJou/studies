---
title: Deep Learning [@Goodfellow2016]
---
# はじめに

発明家たちは考える機械を作り出すことを長らく夢見ていた。この願望は少なくとも古代ギリシアの代に遡る。神話上の人物[mythical figures]であるPygmalion、Daedalus、Hephaestusはすべて伝説的な発明家と解釈することができ、Galatea、Talos、Pandoraはすべて人工的な生命とみなすことができる。([@Ovid2004; @Sparkes1996; @Tandy1997])
プログラム可能なコンピュータが最初に発想された[conceived]とき、人々はそのような機械が賢くなれるかどうかを、それが作られる以前に100年以上に渡って、知りたいと思っていた。こんにち、人工知能(AI)は多くの実用的なアプリケーションや活発な研究テーマ[research topics]がある盛んな分野[thriving field]である。我々は手順の決まった作業[routine labor]を自動化したり、音声や画像を理解したり、診断を行ったり[make diagnoses in medicine]、基礎科学研究を補助したりする知的なソフトウェアに目を向けている。
人工知能分野の初期の頃は、人間の頭では難しい[intelectually difficult for human beings]がコンピュータには比較的簡単[relatively straightforward]である問題、すなわち、形式的で数学的な規則のリストによって説明できる問題に急速に取り組んで解決していた。後に、人工知能の真の課題は、人々が行うには容易だが形式的に説明するのが難しい作業、すなわち、話された言葉や画像中の顔を認識するような、意識せず、直観的に解いている問題を解決することであると分かった。
本書はこれらのより直観的な問題への解法について書かれている。この解法はコンピュータが経験から学び、より単純な概念との関係を通して定義される個々の概念による概念の階層の観点から世界を理解することを可能にする。経験から知識を収集することで、このアプローチはコンピュータが必要とするすべての知識を形式的に指定する人間のオペレータを必要としない。概念の階層はより単純なものからそれらを構築することでコンピュータが複雑な概念を学習することを可能にする。これらの概念がどのように積み重なって構築されるかを示すために図を描くとすると、その図は深い[deep]、すなわち、多くの層を持っている。この理由により、我々はこのアプローチをAIの深層学習[deep learning]と呼んでいる。
AIの初期の成果の多くは比較的に邪魔の無い[sterile]で形式的な環境で行われ、コンピュータはその世界に関する多くの知識を持つ必要がなかった。例えば、IBMのDeep Blueというチェスシステムは1997年に世界チャンピオンのGarry Kasparovを破った[@Hsu2002]。チェスは、勿論、64個のマスと厳密に決められた方法のみに移動できる32個のコマだけで構成される非常に単純な世界である。出来の良いチェスの戦略を考案することは途方もない成果であるが、その課題は一連のチェスのコマや可能な移動をコンピュータに説明することの難しさによるものではない。チェスは完全に形式的な規則の非常に簡単なリストによって完璧に説明できる。これはプログラマによって前もって容易に提供される。
皮肉なことに、人間にとって中でも最も難しい精神的な仕事である抽象的で形式的な作業はコンピュータにとって中でも最も簡単である。コンピュータは最強の人間のチェスプレイヤーさえも破ることが長らく可能であったが、物体や言葉を認識する平均的な人間の能力の一部に匹敵するように最近なったばかりである。人間の日々の生活は膨大な量の世界に関する知識を必要とする。この知識の大半は主観的かつ直観的であり、それゆえに、形式的な方法で明確に表現する[articulate]は難しい。コンピュータは知的な方法で振る舞うためにこれと同じ知識を捕捉する必要がある。人工知能における重要な課題のひとつはこの形式的でない知識をコンピュータにどのようにして与えるかということである。
いくつかの人工知能プロジェクトは形式言語で世界に関する知識をハードコードすることを目指していた。コンピュータは論理的な推論規則を用いてこれらの形式言語における命題[statement]に関して自動的に理由付けを行うことができる。これは人工知能の知識ベースのアプローチとして知られる。これらのプロジェクトは大きな成果には結びつかなかった。このようなプロジェクトの最も有名なもののひとつとしてCycがある[@Lenat1989]。CycはCycLと呼ばれる言語における推論エンジンおよび命題データベースである。これらの命題は人間の教師[supervisor]のひとりによって入力される。これはやっかいな[unwieldy]プロセスである。人々は世界を正確に記述するために十分な複雑さを持つ形式的な規則を考案しようと取り組む。例えば、Cycは朝に髭を剃るFredという名の人間についての話を理解できなかった[@Linde1992]。この推論エンジンは話の矛盾を検出した。つまり、人が電気的な部分を持たないことを知っているが、Fredが電気カミソリを持っていたので、"髭剃り中のFred"というエンティティが電気的な部分から構成されると理解した。そのため、Fredが髭剃り中でもヒトであるかと訪ねたのだった。
ハードコードされた知識に依存するシステムによって直面する困難はAIシステムが、生データからパターンを抽出することで、自らの知識を獲得する能力を必要としていることを示唆している。この能力は機械学習として知られる。機械学習の事始めはコンピュータが現実世界の知識を伴う問題に取り組み、主観的なように見える決定を下すことを可能にすることであった。ロジスティック回帰[logistic regression]とよばれる単純な機械学習アルゴリズムは帝王切開分娩[cesarean delivery]を推奨するかどうかを決定できる[@Mor-Yosef1990]。naive Bayesと呼ばれる単純な機械学習アルゴリズムは正当なメールとスパムメールを分けることができる。
これらの単純な機械学習アルゴリズムのパフォーマンスはそれらに与えられるデータの表現[representation]に強く依存する。例えば、ロジスティック回帰が帝王切開分娩を勧めるのに使われるとき、AIシステムは患者を直接検査しない。代わりに、医師が、子宮瘢痕[uterine scar]の有無といった、関連情報の一部をシステムに伝える。患者の表現に含まれる情報の各部分は特徴[feature]として知られる。ロジスティック回帰はこれらの患者の特徴のそれぞれが様々な結果とどれだけ関連しているかを学習する。しかし、特徴がどのように定義されているかにはまったくもって影響を与えることはない。ロジスティック回帰に、形式化された医師の報告ではなく、患者のMRIを与えた場合、有用な予測をさせることはできないだろう。MRI画像の個々のピクセルは分娩中に発生するかもしれないいずれかの合併症と僅かな相関を持っている。この表現への依存性はコンピュータ・サイエンスや日常生活にさえも至るところに現れる一般的な現象である。コンピュータサイエンスでは、データの集合を検索するような操作は、集合が上手に[intelligently]構造化され、インデックス化されると、指数関数的に速く進む。人々はアラビア数字で計算をすることは容易に可能だが、ローマ数字で計算を見出すにはさらにもっと多くの間がかかる。表現の選択が機械学習アルゴリズムのパフォーマンスに多大な影響をもたらすことは驚くことではない。単純な視覚的な例として、[@fig:1.1]を参考にすること。

![異なる表現の例：散布図に線を引いて2つのデータのカテゴリに分けたいとしよう。左図では、デカルト座標でいくつかのデータを表していて、線を引くことはできない。右図では、極座標でデータを表していて、縦線を引いて解けるほど単純になっている。（図はDavid Warde-Farleyとの共作）](fig/1-1.png){#fig:1.1}

多くの人工知能タスクはそのタスク用に抽出された正しい特徴セットを設計し、単純な機械学習アルゴリズムにそれらの特徴を与えることで解くことができる。例えば、音声から話者を識別するのに有用な特徴は話者の声道の大きさの推定値である。この特徴は話者が男性か女性か子供かといった強力な手がかりをもたらす。
しかし、多くのタスクでは、何の特徴を抽出すべきかを知ることは難しい。例えば、写真の中の車を検出するためのプログラムを書きたいとしよう。我々は車にホイールがあることを知っているので、特徴としてホイールの有無を利用したいと思うかもしれない。残念ながら、ピクセル値の観点からホイールがどう見えるかを厳密に説明することは難しい。ホイールは単純な幾何形状を持つが、その画像は、ホイールに落ちる影、ホイールの金属部分に反射する[glaring off]太陽、車のフェンダー、ホイールの一部を覆う手前にある物体によって複雑になりうる。
この問題への解法のひとつは表現から出力へのマッピングだけでなく表現それ自体を発見するために機械学習を使うことである。このアプローチは表現学習[representation learning]として知られる。学習した表現はしばしば手作業で設計された表現で得られるものよりも更に良いパフォーマンスとなる。これらは、最小限の人間の介入で、AIシステムが新しいタスクにすばやく適応することも可能にする。表現学習アルゴリズムは単純なタスクには分単位で、複雑なタスクには数間から数ヶ月単位で良好な特徴セットを見つけることができる。複雑なタスクに対して手動で特徴を設計することは膨大な量の人間の時間と労力を要する。つまり、研究者コミュニティ全体では何十年かかる可能性がある。
表現学習アルゴリズムの典型的な例はautoencoderである。autoencoderは入力データを異なる表現に変換するエンコーダ機能とその新しい表現を元のフォーマットに変換し直すデコーダ機能の組み合わせである。autoencoderは、入力がエンコーダとデコーダを通るとき、できるだけ多くの情報を保存するように訓練されるが、新しい表現が様々な良好な特性を持たせるようにも訓練される。異なる種類のautoencoderは異なる種類の特性を達成することを目的としている。
特徴や特徴を学習するためのアルゴリズムを設計するとき、我々の目標は通常観測データを説明するバラツキの要因[factors of variation]を分割することである。この文脈において、我々は「要因」という語を単に影響の源を分けることを指すためだけに用いる。すなわち、この要因は通常では乗算によって組み合わされない。そのような要因は直接観測される量ではしばしばない。代わりに、これらは観測可能な量に影響を与える物理世界における観測不可能な物体や観測不可能な力として存在し得る。これらは有用な簡単化した説明をもたらす人間の脳内の構築物や観測データの推測される原因としても存在し得る。これらはそのデータにおける高度なバラツキを理解するのに役立つ概念や抽象化として考えられる。発言の録音を解析するとき、バラツキの要因は話者の年齢、性別、訛り、話している言葉を含む。車の画像を解析するとき、バラツキの要因は車の位置、色、角度、太陽の明るさを含む。
多くの現実世界の人工知能アプリケーションにおける困難さの主な原因はバラツキの要因の多くがありとあらゆる観測可能なデータの断片に影響を及ぼすことである。赤い車の画像にある個々のピクセルは夜中ではほとんど真っ黒となるかもしれない。車の輪郭の形状は見る角度に依存する。ほとんどのアプリケーションはバラツキの要因を紐解き[disentangle]、考慮しないものを破棄する必要がある。
もちろん、そのような高レベルで抽象的な特徴を生データから抽出することは非常に困難である可能性がある。話者の訛りのような、これらのバラツキの要因の多くは洗練されたほぼ人間レベルのデータの理解を用いることでのみ識別できる。元の問題を解くのとほぼ同じくらい表現を得ることが難しいとき、表現学習は一見役に立つようには見えない。
深層学習[deep learning]は表現学習におけるこの中心的問題を他のより単純な表現の観点から説明される表現を導入することによって解決する。深層学習はコンピュータがより単純な概念から複雑な概念を構築することを可能にする。[@fig:1.2]はどのように深層学習システムがヒトの画像の概念をカドや輪郭のような単純な概念を組み合わせることで表現する事ができるかを示す。そして、カドや輪郭は同様にエッジで定義される。

![深層学習モデルの図解。コンピュータにとって、ピクセル値の集合で表されるこの画像のような、そのままの感覚の入力データの意味を理解することは難しい。一連のピクセルから物体の正体へマッピングする関数は非常に複雑である。このマッピングを学習または評価することは直接的に取り組んだ場合には打開不可能であるように思える。深層学習は望まれる複雑なマッピングを、それぞれがモデルの異なる層によって記述される、入れ子になった単純なマッピングの列に分解することでこの難しさを解決する。この入力は、観測可能な変数を含むという理由で名付けられた、可視層[visible layer]で表現される。そして、隠れ層[hidden layer]の列は画像から抽象的な特徴を徐々に抽出する。これらの層は値がデータに含まれずことから「隠れ」と呼ばれる。その代わりに、モデルは概念が観測データにおける関係を説明するのに有用であるかを決定しなければならない。ここにある画像は各隠れユニットによって表現される特徴の種類の可視化したものである。ピクセルがあるとすると、第1層は、近傍のピクセルの明るさを比較することで、エッジを容易に識別できる。第1隠れ層のエッジの説明があるとすると、第2隠れ層はカドや拡張された輪郭を容易に検索できる。これは、エッジの集合として識別可能である。第2隠れ層のカドや輪郭に関する画像の説明があるとすると、第3隠れ層は、カドや輪郭の特定の集合を見つけることで、特定の物体の全体像を検出できる。最後に、この物体部分に関する画像の説明は画像中に物体の存在を識別するのに使うことができる。画像は @Zeiler2014 の許可を得て転載した。](fig/1-2.png){#fig:1.2}

深層学習モデルの典型的な例はfeedforward deep networkや多層パーセプトロン[multilayer perceptron; MLP]である。多層パーセプトロンはある入力値の集合を出力値にマッピングする単なる数学的な関数である。この関数は多数のより単純な関数を組み立てることで形作られる。異なる数学的な関数のそれぞれのアプリケーションを入力の新しい表現をもたらすとみなすことができる。
データに対して正しい表現を学習するというアイデアは深層学習に対する一側面である。深層学習の別の側面は深さがコンピュータに多段のコンピュータプログラムを学習することを可能にするということである。表現の各層は並列に別の命令セットを実行した後のコンピュータのメモリの状態として考えることができる。より大きな深さを持つネットワークはより多くの命令を次第に実行できる。順次命令は、以後の命令が以前の命令の結果を参照し直すことができるので、強大な力をもたらす。深層学習のこの視点により、層のactivationにあるすべての情報は入力を説明するバラツキの要因を必ずしもエンコードしているわけではない。その表現は入力を理解できるプログラムを実行するのに役立つ状態情報をも保存する。この状態情報は伝統的なコンピュータ・プログラムにおけるカウンタやポインタに類似するかもしれない。入力の中身には全くもって関係ないが、モデルがその処理をまとめるのに役立つ。
モデルの深さを測る主要な方法は2つある。ひとつの見方はそのアーキテクチャを計算するのに実行する必要がある順次命令の数に基づく。我々はこれを入力を考慮してモデルの各出力を計算する方法を述べるフローチャートを通る最長経路の長さとみなすことができる。2つの等価なコンピュータプログラムがどの言語でプログラムが書かれているかに依存して異なる長さを持つであろうことと同様に、同じ関数はどの関数がフローチャートにおける個々のステップとして利用する事ができるかに依存して異なる深さを持つフローチャートとして描かれ得る。[@fig:1.3]はどのようにしてこの言語の選択が同じアーキテクチャに対して2つの異なる値[measurements]をもたらし得るかを描いている。

![各ノードが操作を行うような、入力を出力にマッピングする計算グラフの図解。深さは入力から出力への最長経路の長さであるが、取り得る計算ステップを構成するやり方の定義に依存する。これらのグラフで図示される計算はロジスティック回帰モデル$\sigma(w^T x)$の出力である。ここで、$\sigma$はlogistic sigmoid functionである。コンピュータ言語の要素として加算、乗算、logistic sigmoidを使う場合、このモデルは3の深度を持つ。ロジスティック回帰それ自体を1つの要素と見る場合、このモデルは1の深度を持つ。](fig/1-3.png){#fig:1.3}

もうひとつのアプローチは、深い確率的なモデルで使われるが、モデルの深さを計算グラフの深さではなく概念が互いにどう関係するかを記述するグラフの深さであるとみなす。この場合、各概念の表現を計算する必要がある計算のフローチャートの深さは概念それ自体のグラフよりも深いかもしれない。これは、より複雑な概念に関する情報を与えられれば、より単純な概念のシステムの理解を改良できるためである。例えば、影の中にある1つの眼を持つ顔の画像を観察するAIシステムは最初のうちは1つの眼のみを理解するかもしれない。顔が存在することを検出した後は、そのシステムは第2の眼が同様におそらく存在することを推論できる。この場合、概念のグラフは2つの層、すなわち、眼のための層と顔のための層のみを含むが、計算のグラフはn回を与えるように各概念の推定値を改良するならば、2n個の層を含む。
これら2つの見方、すなわち、計算グラフの深さと確率的なモデルのグラフの深さ、のどちらが最適であるかは常に明確ではないので、そして、これらのグラフを構築する一連の最小要素は人によりけりなので、コンピュータプログラムの長さに対する単一の正しい値が存在しないのと同様に、アーキテクチャの深度に対する単一の正しい値は存在しない。また、モデルが「深層」足り得るためにどれだけの深さが必要であるかに関してのコンセンサスは存在しない。しかし、深層学習は関数を学習したり概念を学習したりする構造の量を伝統的な機械学習が行う場合よりも多く伴うモデルの研究であると問題なくみなすことができる。
まとめると、本書の主題である深層学習はAIへのアプローチである。具体的には、機械学習の一種であり、コンピュータシステムが経験とデータによって改善することを可能にする技術である。我々は機械学習が複雑な現実世界の環境で操作できるAIシステムを構築するための唯一実行可能なアプローチであると主張する。深層学習は、より単純な概念との関係で定義される各概念とより抽象度の低いものの観点から計算されるより抽象的な表現を持つ入れ子になった概念の階層として世界を表現することで大きな力と柔軟性を達成する機械学習の特定の種類である。[@fig:1.4]はこれらの異なるAI分野の間の関係を図示する。[@fig:1.5]はそれぞれがどのように機能するかの高レベルな図式を提示する。

![どんな深層学習が表現学習の一種であるかということ、同様に機械学習の一種であるということ、いろんなものに使われているがAIへのアプローチのすべてではないということを示すベン図。ベン図の各セクションはAI技術の例を含む。](fig/1-4.png){#fig:1.4}

![AIシステムの異なる部分が異なるAI分野内と互いにどのように関連し合うかを示すフローチャート。色付き[shaded boxes]はデータから学習可能である要素を示す。](fig/1-5.png){#fig:1.5}

## 本書を読むべきは誰か？

- 要約：
    - この本は機械学習を学ぶ学生や機械学習を使いたいソフトウェアエンジニアを読者として想定している
    - パート1は応用数学と機械学習の基礎、パート2実践的な深度学習アルゴリズム、パート3は最新研究動向
    - 必要無いと感じた箇所は順次読み飛ばしてもらって構わない

![本書の高レベルな構成図。ある章からある章への矢印は前者が後者を理解することへの前提となる資料であることを示す。](fig/1-6.png){#fig:1.6}

## 深層学習における歴史的傾向

ある歴史的な文脈で深度学習を理解することは最も簡単である。深度学習の詳細な歴史を提供する代わりに、いくつかの重要な傾向を見ていく。

- 深度学習は長く濃い歴史を歩んできたが、異なる哲学的な視点を反映するように、いろんな名前で行われ、人気には上がり下がりがあった。
- 深度学習は扱える訓練データの量が増えるにつれて有用になっていった。
- 深度学習モデルは深度学習に対するコンピュータインフラ（ハードウェアとソフトウェアの両方）が改良するにつれて時間とともに大型化してきた。
- 深度学習は時間とともに正確性を向上させながら、ますます複雑になるアプリケーションを解いてきた。

### 様々な呼び名とニューラルネットワークの運命の変化

我々は、本書の多くの読者が深層学習をワクワクする新技術として耳にしてきたが、新興の分野に関する本で「歴史」と書かれているのを目にして驚いている、と予測している。事実、深層学習は1940年代にまで遡ることができる。現在の人気に先立って何年もの間比較的人気がなかった、また、最近では「深層学習」とだけ呼ばれているが、様々な異なる呼び名で広まっていたことから、深度学習は単に新しいように見えるだけである。この分野は、異なる研究者や異なる見方の影響を反映するために、幾度となくリブランディングを繰り返してきた。
深層学習の包括的な歴史は本書の範疇を逸脱する。しかし、いくつかの基本的な前後関係[context]は深層学習を理解するために有用である。大まかに言えば、3つの発展の波があった。1940年代から1960年代におけるcyberneticsとして知られる深層学習、1980年代から1990年代におけるconnectionismとして知られる深層学習、2006年に始まる深層学習の名の下の現在の再起である。これは[@fig:1.7]で定量的に図示される。

![Google Booksに従って、"cybernetics"、"connectionism"または"neural networks"としての語句の頻度によって計測される通りの、人工ニューラルネット研究の3つの歴史的な波のうちの2つ（第三波は出現が最近すぎる）。第一波は、biological learning[@McCulloch1943; @Hebb1949]の理論群の発展や単一のニューロンの訓練を可能にするパーセプトロン[@Rosenblatt1958]のような最初のモデルの実装とともに、1940年代から1960年代にcyberneticsとして始まった。第二波は、1つか2つの隠れ層を持つニューラルネットワークを訓練するための逆伝播法[@Rumelhart1986a]とともに、1980年から1995年の間のconnectionistアプローチとして始まった。現在の第三波である深層学習は2006年頃に始まり[@Hinton2006; @Bengio2007; @Ranzato2007a]、2016年現在には本の形としてたった今出現している。他の2つの波は関連する科学活動が起こってからかなり後に本の形として同じように現れた。](fig/1-7.png){#fig:1.7}

我々がこんにち認識している最初期の学習アルゴリズムのいくつかはbiological learningの計算モデル、すなわち、学習が脳内でどのように起こる、または、起こり得るかについてのモデルであることが意図されていた。結果として、深層学習が広まったときの名前のひとつに人工ニューラルネットワーク[artifiicial neural networks; ANN]というものがある。これに対応する深層学習モデルの見方は、これらが（人間の脳かその他の動物の脳のいずれかの）生物学的な脳に触発された工学システムであるという点である。機械学習にt買われるニューラルネットワークの種類は時折に脳の機能を理解するために使われてきた[@Hinton1991]が、これらは現実的な生物学的な機能のモデルとして一般に設計されていない。深層学習でのニューラル的視点は2つの主なアイデアによって動機付けられる。アイデアのひとつは脳が知的な振る舞いが可能であるという例えによる証明をもたらすということであり、知能を構築するのに概念的に簡単な道筋は脳の背後にある計算の原則をリバースエンジニアリングして、その機能性を複製することである。もうひとつの見方は脳や人間の知能の基礎をなす原則を理解することがとても興味深いことになるだろうということである。つまり、これらの基礎科学の疑問に光を当てる機械学習モデルは工学アプリケーションを解決する能力以外では有用である。
モダンな用語としての「深度学習」は機械学習モデルの現在の種類に関する神経科学的な視点を超えている。これは複数の構成レベルを学習するというより一般的な原則に訴える。これは、神経に触発される必要のない機械学習フレームワークで適用させることができる。
モダンな深層学習の最初期のものは神経科学的な視点から動機付けられる単純な線形モデルであった。これらのモデルは$n$個の入力値$x_1, \dots, x_n$を取り、出力$y$に関係づけるように設計された。これらのモデルは重み$w_1, \dots, w_n$を学習し、出力$f(\mathbb{x}, \mathbb{w}) = x_1 w_1 + \dots = x_n w_n$を計算するだろう。このニューラルネットワーク研究の第一波は、[@fig:1.7]に記されるように、cyberneticsとして知られた。
McCulloch-Pittsのニューロン[@McCulloch1943]は初期の脳機能のモデルであった。この線形モデルは$f(x, w)$が正か負かを確かめることで入力の2つの異なるカテゴリを認識できた。もちろん、モデルがカテゴリの望ましい定義に一致するためには、重みを正確に設定する必要があった。これらの重みは人間のオペレータによって設定させることができた。1950年代には、パーセプトロン[@Rosenblatt1958; @Rosenblat1962]は各カテゴリから入力例を与えることでカテゴリを定義得る重みを学習する事ができた最初のモデルとなった。ほぼ同時期に始まったadaptive linear element (ADALINE)は実数を予測するために$f(x)$の値自体を単純に返す[@Widrow1960]。また、データからこれらの値を予測するように学習することができた。
これらの単純な学習アルゴリズムは近年の機械学習の状況に大きく影響した。ADALINEの重みを適合するのに使われる訓練アルゴリズムは確率的勾配降下法[stochastic gradient descent]と呼ばれるアルゴリズムの特殊な場合であった。確率的勾配降下法アルゴリズムの微修正版はこんにちの深度学習モデルに対する支配的な訓練アルゴリズムとして存続している。
パーセプトロンやADALINEで使われる$f(x, w)$に基づくモデルは線形モデルと呼ばれる。これらのモデルは最も広く使われる機械学習モデルのひとつとして存続しているが、多くの場合、元のモデルが訓練した方法ではない異なる方法で訓練される。
線形モデルは多くの制限がある。最も有名なものとして、これらはXOR関数を学習できない。ここで、$f([0, 1], w) = 1$かつ$f([1, 0], w) = 1$だが、$f([1, 1], w) = 0$かつ$f([0, 0], w) = 0$である。線形モデルにおけるこれらの欠点を見つけた批評家たちは一般に生物学に触発された学習に対して反発を起こした[@Minsky1969]。これはニューラルネットワークの人気における最初の大きな落ち込み[first major dip]であった。
こんにち、神経科学は深層学習の研究者にとっての重要なインスピレーション源とみなされているが、当分野に対する主要な手引き[predominant guide]ではもはやない。
こんにちの深層学習研究における神経科学の役割の縮小に対する主な理由は単純に手引きとして使うには脳に関して十分な情報を持っていないということにある。脳で使われる実際のアルゴリズムの深い理解を得るには、（最低限でも）何千の相互接続されたニューロンの活動を同時にモニターすることができる必要があるだろう。我々はこれをすることができないので、最も単純でよく研究されている脳の一部のいくつかでさえもなかなか理解できていない[@Olshausen2005]。
神経科学は単一の深層学習アルゴリズムが多くの異なるタスクを解くことが可能であってほしいと思う理由をもたらした。神経科学者はフェレットが視覚の信号を脳の聴覚を処理する領域に送るようにつなぎ替えるとそこで「見る」ことを学習することができることを発見した[@VonMelchner2000]。これは多くの哺乳類の脳がその脳で処理される様々なタスクのほとんどを解くために単一のアルゴリズムを用いているのだろうということを示唆している。この仮説の以前では、機械学習研究は、自然言語処理、視覚、経路計画[motion planning]、発話認識を研究する研究者の異なるコミュニティがあり、更に分かれていた。こんにち、これらのアプリケーションのコミュニティは依然として別れたままだが、同時に深層学習の研究グループがこれらの応用分野の多く、さらには、すべてを研究することは一般的である。
我々は神経科学由来のいくつかの大まかなガイドラインを描くことができる。お互いの相互作用を介することでのみ知的になる多くの計算ユニットを持つことの基本的なアイデアは脳に触発されている。neocognitron[@Fukushima1980]は哺乳類の視覚系の構造に触発された画像処理のための強力なモデルのアーキテクチャをもたらした。これは後に、[@sec:9.10]に見られる、モダンなconvolutional networkの基礎となった[@LeCun1998b]。こんにちの殆どのニューラルネットワークは正規化線形ユニット[rectified linear unit]と呼ばれるモデルのニューロンに基づいている。オリジナルのcognitron[@Fukushima1975]は我々の脳機能の知識に強く触発されたより複雑なバージョンを導入した。単純化されたモダンなバージョンは、1つのinfluenceとして神経科学を引用する@Nair2010や@Glorot2011a、より工学指向なinfluencesを引用する@Jarrett2009といった、多くの視点からのアイデアを合体させて開発された。神経科学は重要なインスピレーション源である一方、厳格な手引きと受け取る必要はない。我々は実際のニューロンがモダンな正規化線形ユニット以上に非常に様々な関数を計算することを知っているが、ニューロンを現実に近づけることは機械学習のパフォーマンスに改善を未だもたらしていない。また、神経科学はいくつかのニューラルネットワークのアーキテクチャにうまくインスピレーションを与えたが、我々はこれらのアーキテクチャを訓練するのに使う学習アルゴリズムに対するさらなる手引きを提示するための、神経科学に対するbiological learningに関して未だ十分に知らない。報道記事は脳と深層学習の類似性をしばしば強調する。これは、kernel machinesやベイズ統計のような他の機械学習分野で働いている研究者よりも、深層学習の研究者が1つの影響として脳を引用する可能性が高いという点では正しいが、脳をシミュレートする試みとして深層学習を見るべきではない。モダンな深層学習は多数の分野、特に、線形代数、確率論、情報理論、数値最適化のような応用数学基盤からインスピレーションを受けいる。幾人かの深層学習の研究者は重要なインスピレーション源として神経科学を引用するが、その他の人たちは神経科学を全く考慮しない。
アルゴリズムレベルで脳がどのように働くかを理解する努力が健在であることは記しておきたい。この努力は「計算論的神経科学[computational neuroscience]」として主に知られ、深層学習とは研究分野が分かれている。研究者にとって両分野を行ったり来たりすることは普通のことである。深層学習の分野は知能を必要とするタスクをうまく解く事ができるコンピュータシステムを構築する方法と主に関係しており、計算論的神経科学は脳が実際にどのように機能しているかのより正確なモデルを構築することと主に関係している。1980年代には、connectionismやparalell distributed processingと呼ばれるムーブメントを介してニューラルネットワーク研究の第二波が大半出現した[@Rumelhart1986c; @McClelland1995]。connectionismは認知科学の文脈で発生した。認知科学は複数の異なる解析レベルを組み合わせて意識を理解する学際的アプローチである。1980年の初頭では、ほとんどの認知科学者は記号推論[symbolic reasoning]のモデルを研究した。その人気にもかかわらず、記号モデルはどのようにして脳がニューロンを用いて実際に実装できるだろうかという観点から説明するのが難しかった。connectionistsはニューロン的な実装に実際に根拠を置くことができる認知のモデルを研究し始めた[@Touretzky1985]。これは1940年代の心理学者Donald Hebbの研究まで遡る多くのアイデアを復活させた。
connectionismの中心的なアイデアは多数の単純な計算ユニットが一緒にネットワーク化されるとき知的な振る舞いを達成できることである。この洞察は計算モデルにおける隠れユニットへ適用されるように生物学的な神経系におけるニューロンにも同様に適用される。
1980年代のconnectionism運動の間には、こんにちの深度学習の中心にあり続けるいくつかの重要な概念が生じた。
これらの概念のひとつは分散表現[distributed representation]についてである[@Hinton1986]。これはシステムへの各入力が多くの特徴によって表現されるべきであり、各特徴が多くの取り得る入力の表現を伴うべきであるというアイデアである。例えば、車、トラック、鳥を認識できる視覚システムがあり、これらの物体が赤、緑、青のいずれかとしよう。表現する方法の一つとして、これらの入力は、赤いトラック、赤い車、赤い鳥、緑のトラック、以下略といった9つの取り得る組み合わせのそれぞれに対して活性化する個別のニューロンまたは隠れユニットを持つことだろう。これは9つの異なるニューロンを必要とし、各ニューロンは色の概念と物体の正体を独立して学習しなければならない。この状況を改善する方法のひとつとして、色を述べる3つのニューロンと物体の正体を述べる3つのニューロンによる分散表現を使うことがある。これは合計9つではなく6つのニューロンのみを必要とし、赤さを説明するニューロンは、ある特定の物体カテゴリの画像からだけでなく、車、トラック、鳥の画像から赤さについて学習できる。分散表現の概念は本書の中心であり、[@sec:15]でより詳細に記述してある。
もうひとつのconnectionist運動の成果は内部表現を伴うディープニューラルネットワークを訓練するために逆伝播法をうまく利用したことと、逆伝播法を普及させたことである[@Rumelhart1986a; @LeCun1987]。このアルゴリズムは人気が上がったり下がったりしてきたが、本書を執筆している現在では、深層モデルを訓練する支配的なアプローチである。
1990年代に、研究者たちはニューラルネットワークのシーケンスのモデリングについて重要な発展を成し遂げた。@Hochreiter1991と@Bengio1994は、[@sec:10.7]で示されるように、長いシーケンスのモデリングにおいて基盤の数学的な困難さのいくつかを識別した。@Hochreiter1997はこれらの困難さのいくつかを解くためのlong short-term memory (LSTM)ネットワークを導いた。こんにち、LSTMは、Googleでの多くの自然言語処理タスクを含めた、多数のシーケンスモデリングのタスクに広く使われている。
ニューラルネットワーク研究の第二波は1990年代中頃まで続いた。ニューラルネットワークや他のAI技術に基づく投機は投資を求めつつ非現実で野心的な主張に変わり始めた。AI研究がこれらの不当な期待を果たさなかったとき、投資家は失望した。同時に、機械学習のその他の分野は発展を遂げた。kernel machines[@Boser1992; @Cortes1995; @Scholkopf1999]やグラフィカルモデル[@Jordan1998]の両方は多くの重要なタスクで良い成果を出した。これら2つの要因は2007年まで続いたニューラルネットワークの人気の衰退をもたらした。
時を同じくして、ニューラルネットワークはいくつかのタスクで印象的なパフォーマンスを得続けた[@LeCun1998b; @Bengio2001]。Canadian Institute for Advanced Research (CIFAR)はそのNeural Computation and Adaptive Perception (NCAP)研究イニシアティブを介してニューラルネットワーク研究を生きながらえさせるのを援助したこの計画はトロント大学のGeoffrey Hinton、モントリオール大学のYoshua Bengio、ニューヨーク大学のYann LeCunに率いられる機械学習研究グループを結成した。学際的なCIFAR NCAP研究イニシアティブは神経科学者、人間の視覚やコンピュータビジョンの専門家も含まれた。
この時点で、ディープネットワークは訓練するのが非常に難しいと一般に理解されていた。現在の我々は1980年代から存在しているアルゴリズムが非常に有効であることを知っているが、2006年頃は明らかではなかった。この問題はおそらく単純に、これらのアルゴリズムが当時に使えるハードウェアで多くの実験を可能にするには計算コストが高すぎた、といことである。
ニューラルネットワーク研究の第三波は2006年のブレイクスルーを以て始まった。Geoffrey Hintonはdeep belief networkと呼ばれるニューラルネットワークの一種がgreedy layer-wise pretrainingと呼ばれる戦略を用いて効率的に訓練できることを示した[@Hinton2006]。これは、[@sec:15.1]にさらなる詳細を記載している。他のCIFAR関連[CIFAR-affiliated]の研究グループは同様の戦略が多くの他の種類のディープネットワークを訓練するのに使えること[@Bengio2007; @Ranzato2007a]、テスト例の一般化を改良するのに役立つことをまたたく間に示した。このニューラルネットワーク研究の波は、今や研究者が以前に可能であったよりも深いニューラルネットワークを訓練できることを強調し、深さの理論的重要性に注意を向けるために「深層学習」という用語の使用を広めた[@Bengio2007; @Delalleau2011; @Pascanu2014a; @Montufar2014]。このとき、ディープニューラルネットワークは手作業で設計された機能性と同程度の他の機械学習に基づく競合のAIシステムより勝っていた。このニューラルネットワークの人気の第三波は執筆当時まで続いているが、深層学習研究の焦点はこの波のさなかであって目まぐるしく変化してきた。この第三波は新しい教師なし学習技術と小さなデータセットから上手に一般化する深層モデルの能力に焦点を当てて始まったが、こんにちでは、より古い教師あり学習アルゴリズムと大きなラベル付きデータセットを活用する深層モデルの能力により大きな関心が集まっている。

### データセットサイズの増加

読者の中には、人工ニューラルネットワークをの最初の実験が1950年代に執り行われたとしても、深層学習が重要な技術として最近認識されるようになっただけではないかと疑問に思う方もいるかもしれない。深層学習は1990年代から商用アプリケーションに上手に使われてきたが、最近まで、技術ではなく芸術の一部であるとか専門家のみが使える何かとしてしばしばみなされた。深層学習アルゴリズムから良好なパフォーマンスを得るにはいくつかのスキルが必要とされるという点では正しい。幸いにも、必要なスキルの量は訓練データの量が増えるに連れて減少している。こんにちの複雑なタスクにおける人間のパフォーマンスに達する学習アルゴリズムは1980年代に単純化した例題[toy problem]を解くために取り組んだ学習アルゴリズムとほぼ同一であるが、これらのアルゴリズムで訓練するモデルは非常に深いアーキテクチャの訓練を単純化するよう変化した。最も重要な新しい発展はこんにち我々がこれらがうまくいくために必要な資料とともにこれらのアルゴリズムを提供できるということである。[@fig:1.8]はベンチマークのデータセットのサイズが時間とともにどれだけ顕著に肥大してきたかを示す。この傾向は社会のディジタル化が加速することで引き起こされる。我々の活動がコンピュータで行われるようになればなるほど、より多くの活動が記録される。コンピュータがともにますます接続するたび、これらの記録を一極集中させ、機械学習アプリケーションに適したデータセットにこれらをキュレートすることはより用意になる。「ビッグデータ」の時代は、統計的な推量の重要なburden、すなわち、少ないデータ量のみを観察した後に新しいデータにうまく一般化することが相当にlightenされたので、機械学習をより容易にする。2016年現在、大まかな経験則として、教師あり深層学習アルゴリズムはカテゴリあたり約5000個のラベル付き例で許容できるパフォーマンスを一般に達成するだろうということ、少なくとも1000万個のラベル付き例を含むデータセットで訓練すると人間のパフォーマンスに匹敵するか超えるだろうということがある。

![時とともに増加するデータセットサイズ。1990年初頭、統計学者は手で集めた数百から数千の計測値を用いたデータセットを研究していた[@Garson1900; @Gosset1908; @Anderson1935; @Fisher1936]。1950年代から1980年代にかけて、生物学に触発された機械学習の先駆者たちは、文字の低解像度ビットマップのような、小さな人造[synthetic]データセットでしばしば研究していた。これは、計算コストが低くなるように設計するためであり、ニューラルネットワークが特定の種類の関数を学習できることを実証するためであった[@Widrow1960; @Rumelhart1986b]。1980年代と1990年代には、機械学習はより統計的になり、手書きの数字のスキャンの（[@fig:1.9]に示される）MNISTデータセットのような数万個の例を含むより大きなデータセットを活用し始めた[@LeCun1998b]。2000年代の最初の10年には、CIFAR-10データセットのようなこれと同サイズのより洗練されたデータセット[@Krizhevsky2009]が生み出され続けた。その終わり頃から2010年代の初めの5年にかけて、何十万から数千万の例を含む極めて大きなデータセットが深層学習で可能なことを一変させた。これらのデータセットにはパブリックなStreet View House Numbersデータセット[@Netzer2011]、様々なバージョンのImageNetデータセット[@Deng2009; @Russakovsky2014a]、Soprts-1Mデータセット[@Karpathy2014]が含まれる。図の上部には、Canadian Hansardから構築されたIBMのデータセット[@Brown1990]やWMT 2014 English to Frenchデータセット[@Schwenk2014]のような、訳文[translated sentences]のデータセットが他のデータセットサイズの遥か先を行っていることがわかる。](fig/1-8.png){#fig:1.8}

![MNISTデータセットからの入力の一例。NISTはNational Institute of Standards and Technologyの頭文字であり、このデータを初めに収集した機関である。データが機械学習アルゴリズムでより容易に使うために前処理されたので、"M"は"Modified"（修正版）の頭文字である。MNISTデータセットは手書きの数字のスキャン画像と0から9の数字が各画像に含まれているという関連するラベルから構成される。この単純な分類問題は深層学習研究において最も単純で広く使われるテストのひとつである。これはモダンな技術が解くことは極めて簡単であるにもかかわらず未だに人気である。Geoffrey Hintonは、機械学習の研究者が制御された実験室条件[laboratory conditions]でこれらのアルゴリズムを研究する事ができるという意味として、生物学者がしばしばミバエ[fruit flies]を研究するのと同様に、「機械学習のショウジョウバエ[drosophila]」と説明した。](fig/1-9.png){#fig:1.9}

### モデルサイズの増加

1980年代から比較的小さな成功を収めた後のこんにちにニューラルネットワークが広く成果を上げているもうひとつの重要な理由はこんにちには更に大きなモデルを実行するための計算資源があるということである。connectionismの主な洞察のひとつはニューロンのお起きがともに動作するときに動物が知的になるということである。個々のニューロンや小さなニューロンの集まりは特別に有用ではない。
生物学的なニューロンは特に密に接続されていない。[@fig:1.10]に見える通り、我々の機械学習モデルは何十年の間、哺乳類の脳でさえ数桁の範囲内で、ニューロン1つあたりに多くの接続を持っていた。

![時間に対するニューロンあたりの接続数。初期には、人工ニューラルネットワークにおけるニューロン間の接続数はハードウェア能力によって制限された。こんにち、ニューロン間の接続数はほぼ設計上の検討事項である。いくつかの人工ニューラルネットワークはほとんど猫と同じくらいのニューロンあたりの接続数を持つ。他のニューラルネットワークではネズミのような小型哺乳類と同じくらいのニューロンあたりの接続数を持つことは極めて一般的である。人間の脳でもそこまで法外なニューロンあたりの接続数を持っているわけではない。生物学的なニューラルネットワークの大きさは @Wikipedia2015から。
1. Adaptive linear element [@Widrow1960]
2. Neocognitron [@Fukushima1980]
3. GPU-accelerated convolutional network [@Chellapilla2006]
4. Deep Boltzmann machine [@Salakhutdinov2009a]
5. Unsupervised convolutional network [@Jarrett2009]
6. GPU-accelerated multilayer perceptron [@Ciresan2010]
7. Distributed autoencoder [@Le2012]
8. Multi-GPU convolutional network [@Krizhevsky2012]
9. COTS HPC unsupervised convolutional network [@Coates2013]
10. GoogLeNet [@Szegedy2014a]](fig/1-10.png){#fig:1.10}

ニューロンの総数という観点において、ニューラルネットワークは、[@fig:1.11]に示される通り、ごく最近まで驚くほど小さかった。隠れユニットの導入以来、人工ニューラルネットワークは2.4年ごとに大体2倍のサイズになった。この成長はより大きなメモリを持つより高速なコンピュータによって、そして、より大きなデータセットが利用できるようになることによって育まれた。より大きなネットワークはより複雑なタスクでより高い正確さを達成することを可能にする。この傾向は数十年の間続くことが確実視されている[look set to]。新しい技術がこの成長を速めない限り[enable faster scaling]、人工ニューラルネットワークは少なくとも2050年代まで人間の脳と同じニューロン数を持たないだろう。生物学的なニューロンは現在の人口ニューロンより複雑な機能を表現し得る。そのため、生物学的なニューラルネットワークはこの筋書きが描くよりももっと大きくなるかもしれない。

![時間に対するニューラルネットワークサイズの増加。隠れユニットの導入以来、人工ニューラルネットワークは2.4年ごとに大まかに二倍の大きさになってきた。生物学的なニューラルネットワークサイズは@Wikipedia2015から。
1. Perceptron [@Rosenblatt1958; @Rosenblatt1962]
2. Adaptive linear element [@Widrow1960]
3. Neocognitron [@Fukushima1980]
4. Early back-propagation network [@Rumelhart1986b]
5. Recurrent neural network for speech recognition [@Robinson1991]
6. Multilayer perceptron for speech recognition [@Bengio1991]
7. Mean ﬁeld sigmoid belief network [@Saul1996]
8. LeNet-5 [@LeCun1998b]
9. Echo state network [@Jaeger2004]
10. Deep belief network [@Hinton2006]
11. GPU-accelerated convolutional network [@Chellapilla2006]
12. Deep Boltzmann machine [@Salakhutdinov2009a]
13. GPU-accelerated deep belief network [@Raina2009]
14. Unsupervised convolutional network [@Jarrett2009]
15. GPU-accelerated multilayer perceptron [@Ciresan2010]
16. OMP-1 network [@Coates2011]
17. Distributed autoencoder [@Le2012]
18. Multi-GPU convolutional network [@Krizhevsky2012]
19. COTS HPC unsupervised convolutional network [@Coates2013]
20. GoogLeNet [@Szegedy2014a]](fig/1-11.png){#fig:1.11}

今思えば、ヒル[leech]より少ないニューロンをもつニューラルネットワークが洗練された人工知能の問題を解けないということは特別に驚くことではない。こんにちのネットワークでさえ、カエルのような比較的に原始的な脊椎動物の神経系より小さい。
高速なCPUが使えたり、（[@sec:12.1.2]で説明される）汎用用途のGPUが現れたり、高速なネットワークが接続できたり、分散コンピューティングに対するソフトウェアインフラがよくなったりすることを理由にして、時とともにモデルサイズが増加することは深層学習の歴史における最も重要な傾向のひとつである。この傾向は将来的に続いていくと一般に予測される。

### 正確さ、複雑さ、現実世界の影響の増加

1980年代以降、深層学習は正確な認識や予測をもたらすようにその能力を着実に改善してきた。更に、深層学習はますます広範囲のアプリケーションへ上手に着実に適用していった。
最初期の深層モデルはガッツリとクロッピングされた極小の画像で個々の物体を認識するのに使われた[@Rumelhart1986a]。それから、ニューラルネットワークが処理できる画像サイズが緩やかに増加していった。モダンな物体認識のネットワークはリッチな高解像度の写真を処理し、写真を認識させる物体の近くでクロッピングする必要がない[@Krizhevsky2012]。同様に、最初期のネットワークは2種類の物体（または、ある場合、1種類の物体の有無）だけを認識することができたが、これらのモダンなネットワークは一般に少なくとも1000個の異なる物体カテゴリを認識する。物体認識における最大のコンテストは毎年開催されるImageNet Large Scale Visual Recognition Challenge (ILSVRC)である。convolutional networkが初めてこのchallengeに、最優秀[state-of-the-art]のtop-5誤り率を26.1%から15.3%へと引き下げるような、大差で勝利したとき、深層学習のトップに上り詰める劇的な瞬間が訪れた。これは、convolutional networkが各画像に対して取り得るカテゴリのランク付けされたリストを生成し、その正解カテゴリがこのリストの最初の5つのエントリの中に84.7%の確率で現れるということを意味する。それ以来、これらの競技会は深層畳み込みネットによって着実に勝利を収められ、本書を執筆している現在では、[@fig:1.12]に示される通り、深層学習の進歩がこのコンテストの最新top-5誤り率を3.6%まで引き下げた。

![時間に対する誤り率の減少。ディープネットワークはImageNet Scale Visual Recognition Challengeに参加するのに必要なスケールに達して以来、毎回ますます低い誤り率を生み出しながら、毎年の競技会で着実に勝利してきた。データは@Russakovsky2014bと@He2015から。](fig/1-12.png){#fig:1.12}

深層学習は発話認識でも劇的な影響を与えてきた。1990年代を通して改善した後、発話認識の誤り率は2000年頃から停滞した。発話認識への深層学習の導入[@Dahl2010; @Deng2010b; @Hinton2012a]は誤差率を、あるものは半分になるなど、激減させることとなった。この歴史は[@sec:12.3]でより詳しく説明する。
ディープネットワークは歩行者検出や画像セグメンテーションで目覚ましい成功を収め[@Sermanet2013; @Farabet2013; @Couprie2013]、道路標識の分類では人間を超えるパフォーマンスを生み出した[@Ciresan2012]。
ディープネットワークのスケールち正確さが増加してきたのと同時に、それらが解けるタスクの複雑さも増加してきた。@Goodfellow2014dはニューラルネットワークが、単一の物体を識別するだけではなく、画像から複写した文字列全体を出力するように学習できることを示した。以前には、この種の学習はその列の個々の要素のラベリングを必要とすると広く理解された[@Gulcehre2013]。前に触れたLSTM sequence model のようなRecurrent neural networksは単なる固定入力ではなくシーケンスと他のシーケンスとの間の簡易をモデル化するのに使われる。このsequence-to-sequence学習はもうひとつのアプリケーション、すなわち、機械翻訳[@Sutskever2014; @Bahdanau2015]の進化の最前線にいるように見える。
複雑さが増加するこの傾向は、メモリセルから読み出したりメモリセルに任意の内容を書き込んだりするために学習を行う、ニューラルチューリングマシンの導出という論理的帰結に行き着いた[@Graves2014]。そのようなニューラルネットワークは望ましい振る舞いの例から単純なプログラムを学習できる。例えば、それらはソートされたりされなかったりするシーケンスの例が与えられると数字のリストをソートするように学習できる。この自己プログラミング技術は発展途上にあるが、将来的には、原則的にほとんどあらゆるタスクに適用できるだろう。
もうひとつの深層学習のこの上ない成果は強化学習[reinforcement learning]の領域への拡張である。強化学習の文脈において、自律エージェント[autonomous agent]は、人間の操作によるいかなるガイドもなしに、トライアンドエラーによってタスクを処理するように学習しなければならない。DeepMindは深層学習に基づく強化学習システムがAtariのビデオゲームのプレイを学習する能力があることを実証した。これは、多くのタスクで人間レベルのパフォーマンスに達している[@Mnih2015]。深層学習はroboticsに対する強化学習のパフォーマンスも大幅に改善した[@Finn2015]。
これらの深層学習のアプリケーションの多くはかなり有益である。今や深層学習は、Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflix、NVIDIA、NECを含めた、多くの技術系トップ企業で使われる。
深層学習の進歩はソフトウェアインフラの進歩に強く依存してもいる。Theano[@Bergstra2010; @Bastien2012]、PyLearn2[@Goodfellow2013c]、Torch[@Collobert2011b]、DistBelief[@Dean2012]、Caffe[@Jia2013]、MXNet[@Chen2015]、TensorFlow[@Abadi2015]のようなソフトウェアライブラリはすべて重要な研究プロジェクトや商用商品をサポートした。
深層学習は他の科学にも貢献をもたらした。物体認識のためのモダンな畳み込みネットワークは神経科学者が研究できる視覚処理のモデルをもたらす[@DiCarlo2013]。深層学習は膨大な量のデータを処理したり科学の分野で有用な予測を行うための有用なツールももたらす。これは、製薬会社が新薬を設計するのを助けたり[@Dahl2014]、亜原子粒子を研究したり[@Baldi2014]、人間の脳の三次元マップを作るのに使われる顕微鏡画像を自動的に解析[parse]したり[@KnowlesBarley2014]するために分子がどのように相互作用するであろうかを予測するのにうまく使われた。
まとめると、深層学習は、過去何十年かけて発展してきたので、人間の脳、統計学、応用数学の知識をかなり利用した機械学習のアプローチである。近年には、深層学習は、主に、より強力なコンピュータ、より大きなデータセット、より深いネットワークを訓練するための技術の結果として、その人気や有用性においてものすごい成長を遂げている。これからは更にもっと深層学習を改善したり新境地にもたらしたりするための課題や機会に満ちている。

# 線形代数

線形代数は科学や工学を通して広く用いられる数学の分野である。未だ線形代数は離散的な数学ではなく連続的な形式なので、多くの計算機科学者はそれの経験が乏しい。線形代数をきちんと理解することは多くの機械学習アルゴリズム、特に、深層学習アルゴリズムに対して理解したり扱ったりするために必須である。故に、我々は重要な線形代数の前提知識の集中的なプレゼンテーションから深層学習のイントロダクションを始める。
あなたがすでに線形代数に親しんでいるならば、この章をスキップしても一向に構わない。あなたがこれらの概念に関して以前に経験しているが、重要な式を復習するための詳細な参照表[reference sheet]を必要とするならば、我々はThe Matrix Cookbook[@Petersen2006]をオススメする。線形代数にまったく触れたことがないならば、この章は本書を読むのに十分なことを教えてくれるだろうが、我々は、[@Shilov1977]のような、線形代数を教えることのみに焦点を当てている他の資料も調べることを推奨している。この章は深層学習を理解するのに必須でない、多くの重要な線形代数の項目を完全に省略している。

## スカラ、ベクトル、行列、テンソル

線形代数の研究はいくつかの種類の数学的対象[mathematical objects]を伴う。

- スカラ：線形数学で研究される他のほとんどの対象が通常では複数の数値の列であるのに対して、スカラはただの単一の数値である。我々はスカラをイタリック体で記述する。我々は通常、スカラに小文字の変数名を与える。これらを導入するとき、それらが何の種類の数値であるかを指定する。例えば、実数のスカラを定義するのに「線の傾斜を$s \in \mathbb{R}$とする」と言えるし、自然数のスカラを定義するのに「単位の数を$n \in \mathbb{N}$とする」と言える。
- ベクトル：ベクトルは数値の列である。数値は順に配置される。我々はその順番の番号でそれぞれ個別の数値を識別できる。一般に、我々はベクトルに、$\mathbf{x}$のように、太字の小文字の名前を与える。ベクトルの要素は下付き文字を持つイタリック体でその名前を記述することで識別される。＄$\mathbf{x}$の1番目の要素は$x_1$であり、2番目の要素は$x_2$であり、3番目以降も同様である。我々はどんな種類の数値がベクトルに格納されるかを言う必要もある。各要素が$\mathbb{R}$にあり、ベクトルが$n$個の要素を持つならば、ベクトルは、$\mathbb{R}^n$と表記される、$\mathbb{R}$のデカルト積を$n$回行うことで形成される集合の中にある。ベクトルの要素を明示的に区別する必要があるとき、角括弧でトジされる列として記述する。

$$
\mathbf{x} = \left[ \begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array} \right]
$$

我々はベクトルが、異なる軸に沿った座標をもたらす各要素を持つ、空間上の点を識別するとみなすことができる。
時折、我々はベクトルの要素の集合を指し示す必要がある。この場合、我々はその番号を含む集合を定義し、下付き文字として集合を記述する。例えば、$x_1$、$x_3$、$x_6$にアクセスするためには、集合$S = {1, 3, 6}$を定義し、$\mathbf{x}_S$と記述する。我々は補集合を示すために$-$記号を用いる。例えば、$\mathbf{x}_{-1}$は$x_1$を除く$\mathbf{x}$のすべての要素を含むベクトルであり、$\mathbf{x}_{-S}$は$x_1$、$x_3$、$x_6$を除く$\mathbf{x}$のすべての要素を含むベクトルである。
- 行列：行列は数値の２次元配列である。つまり、各要素は1つではなく2つの番号で識別される。我々は通常、$\mathbf{A}$のように、行列にボールド体で大文字の変数名を与える。実数値の行列$\mathbf{A}$が高さ$h$と幅$n$を持つならば、$\mathbf{A} \in \mathbb{R}^{m \times n}$と言う。我々は通常、行列の要素をボールド体ではなくイタリックでその名前を用いて識別する。また、その番号はコンマで分けて並べられる。例えば、$A_{1,1}$は$\mathbf{A}$の左上のエントリであり、$A_{m,n}$は右下のエントリである。我々は水平座標として$:$を記述することで垂直座標$i$のすべての数値を識別できる。例えば、$\mathbf{A}_{i,:}$は垂直座標$i$と$\mathbf{A}$の水平の交差領域を示す。これは$\mathbf{A}$の$i$番目の行として知られる。同様に、$\mathbf{A}_{:,i}$は$\mathbf{A}$の$i$番目の列である。我々が行列の要素を明示的に認識する必要があるとき、各カッコでトジされる列として記述する。

$$
\left[ \begin{array}{cc}
A_{1,1} & A_{1,2} \\
A_{3,1} & A_{3,2}
\end{array} \right]
$$

時折、我々は単一の文字でない行列の値の表記を示す必要があるかもしれない。この場合、ひその表記の後に下付き文字を用いるが、いずれも小文字に変換しない。例えば、$f(\mathbf{A})_{i,j}$は関数$f$を$\mathbf{A}$に適用して計算した行列の要素$(i,j)$をもたらす。
- テンソル：いくつかの場合、我々は2つより多い軸を持つ配列を必要とするだろう。一般的な場合、軸の変数を持つ規則的な格子上に配置される数値の配列はテンソルとして知られる。我々は"A"という名前のテンソルを$\mathsf{A}$の書体で表記する。我々は$\mathit{\mathsf{A}}_{i,j,k}$と記述することで座標$(i,j,k)$における$\mathsf{A}$の要素を識別する。

行列での重要な操作のひとつは転置である。行列の転置は対角線を境にした行列の鏡像である。この対角線は主対角線と呼ばれ、左上のカドから始まり、右下へと向かう。この操作の図解は[@fig:2.1]を参照のこと。我々は行列$\mathbf{A}$の転置を$\mathbf{A}^\top$と示し、これは以下のように定義される。

$$
(\mathbf{A}^\top)_{i,j} = A_{j,i}
$$

ベクトルは1列のみを含む行列として考えることができる。故に、ベクトルの転置は1行のみを持つ行列である。時折、我々は1行の行列として文中にその要素を書き出すことで、例えば$\mathbf{x} = [x_1, x_2, x_3]$のように、ベクトルを定義する。

![行列の転置は主対角線を境にした鏡像としてみなすことができる。](fig/2-1.png){#fig:2.1}

スカラは単一のエントリのみを持つ行列として考えることができる。これにより、スカラはそれ自身の転置$a = a^\top$であることがわかる。
我々は、それらが同じ形である場合に限り、単に対応する要素を足し合わせることで、行列を互いに足すことができる。すなわち、$\mathbf{C} = \mathbf{A} + \mathbf{B}$は$C_{i,j} = A_{i,j} + B_{i,j}$である。
我々は、行列の各要素にその操作を行うことで、スカラを行列に足したり掛けたりできる。すなわち、$\mathbf{D} = a \cdot \mathbf{B} + c$は$D_{i,j} = a \cdot B_{i,j} + c$である。
深層学習の文脈では、我々はいくつかの従来とは異なる表記法も用いる。我々は行列とベクトルを足し合わせて別の行列を生み出すことを許可する。すまわち、$\mathbf{C} = \mathbf{A} + \mathbf{b}$は$C_{i,j} = A_{i,j} + b_{j}$である。言い換えれば、ベクトル$\mathbf{b}$は行列の各行に足される。この省略表記は加算を行う前に各行にコピーされる$\mathbf{b}$で行列を定義する必要性を取り除く。この多くの場所への$\mathbf{b}$の暗黙的なコピーはbroadcastingと呼ばれる。

## 行列とベクトルの乗算

行列を伴う最も重要な操作のひとつは2つの行列の掛け算である。行列$\mathbf{A}$と$\mathbf{B}$の行列積は第三の行列$\mathbf{C}$である。この積が定義されるために、$\mathbf{A}$は$\mathbf{B}$が持つ行数と同じ列数を持たなければならない。$\mathbf{A}$が$m \times n$の形であり、$\mathbf{B}$が$n \times p$であれば、$\mathbf{C}$は$m \times p$の形である。我々は単に2つ以上の行列同士を配置することで行列積を記述できる。例えば以下のようになる。

$$
\mathbf{C} = \mathbf{A} \mathbf{B}
$$

この積の演算は以下のように定義される。

$$
C_{i,j} = \sum_k A_{i,k} B_{k,j}
$$

2つの行列の標準的な積は単に個々の要素の積を含む行列ではないことに注意したい。そのような操作は存在し、要素ごとの積[element-wise product]とかアダマール積[Hadamard product]と呼ばれ、$\mathbf{A}　\odot \mathbf{B}$と表記する。
同じ次元のベクトル$\mathbf{x}$と$\mathbf{y}$の内積は行列の積$\mathbf{x}^\top \mathbf{y}$である。我々は行列の積$\mathbf{C} = \mathbf{A} \mathbf{B}$を$\mathbf{A}$の$i$行目と$\mathbf{B}$の$j$列目の内積として$C_{i,j}$を計算するとみなすことができる。
行列の積の演算は行列の数学的な解析をより便利にする多くの有用な特性を持つ。例えば、行列の乗算は分配法則を満たす[distributive]。

$$
\mathbf{A} (\mathbf{B} + \mathbf{C}) = \mathbf{A} \mathbf{B} + \mathbf{A} \mathbf{C}
$$

また、結合法則も満たす[associative]。

$$
\mathbf{A} (\mathbf{B} \mathbf{C}) = (\mathbf{A} \mathbf{B}) \mathbf{C}
$$
{#eq:2.8}

行列の乗算は、スカラの乗算とは異なり、交換法則を満たさない[not commutative]（$\mathbf{A} \mathbf{B} = \mathbf{B} \mathbf{A}$は常に成り立つとは限らない）。しかし、2つのベクトルの内積は交換法則を満たす。

$$
\mathbf{x}^\top \mathbf{y} = \mathbf{y}^\top \mathbf{x}
$$

行列の積の転置は以下のような単純な形式を持つ。

$$
(\mathbf{A} \mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top
$$

これは、そのような積の値がスカラであるということ活用することで[@eq:2.8]を実証することができるようになる。故に、それ自身の転置と等価である。

$$
\mathbf{x}^\top \mathbf{y} = (mathbf{x}^\top \mathbf{y})^\top = \mathbf{y}^\top \mathbf{x}
$$

本書の焦点は線形代数ではないので、我々はここで行列の積の有用な特性の包括的なリストを明らかにしようとはしないが、読者の皆は更にたくさん存在することに気付いているはずである。
今や我々は一次方程式の系を書き記すための線形代数の表記法を十分に知っている。

$$
\mathbf{A} \mathbf{x} = \mathbf{b}
$$
{#eq:2.11}

ここで、$\mathbf{A} \in \mathbb{R}^{m \times n}$は既知の行列であり、$\mathbf{b} \in \mathbb{R}^m$は既知のベクトルであり、$\mathbf{x} \in \mathbb{R}^n$は解きたい未知の変数のベクトルである。$\mathbf{x}$の各要素$x_i$はこれらの未知の変数の1つである。$\mathbf{A}$の各行と$\mathbf{b}$の各要素は別の制約をもたらす。我々は[@eq:2.11]を以下のように書き直すことができる。

$$
\mathbf{A}_{1,:} \mathbf{x} = b_1
$$

$$
\mathbf{A}_{2,:} \mathbf{x} = b_2
$$

$$
\dots
$$

$$
\mathbf{A}_{m,:} \mathbf{x} = b_m
$$

また、より明示的にするならば、

$$
\mathbf{A}_{1,1} x_1 + \mathbf{A}_{1,2} x_2 + \cdots \mathbf{A}_{1,n} x_n = b_1
$$

$$
\mathbf{A}_{2,1} x_1 + \mathbf{A}_{2,2} x_2 + \cdots \mathbf{A}_{2,n} x_n = b_2
$$

$$
\dots
$$

$$
\mathbf{A}_{m,1} x_1 + \mathbf{A}_{m,2} x_2 + \cdots \mathbf{A}_{m,n} x_n = b_m
$$

行列対ベクトルの積の表記法はこの形式の式に対するよりコンパクトな表現をもたらす。

## 単位行列と逆行列

線形代数は$\mathbf{A}$の多くの値に対して[@eq:2.11]を解析的に解くことができるようになる逆行列と呼ばれる強力なツールをもたらす。
逆行列を説明するためには、まず、単位行列の概念を定義する必要がある。単位行列は、その行列をベクトルにかけたとき、いかなるベクトルも変化させない行列である。我々は$n$次元のベクトルを保持する単位行列を$\mathbf{I}_n$と表記する。正式には、$\mathbf{I}_n \in \mathbb{R}^{n \times n}$であり、

$$
\forall \mathbf{x} \in \mathbf{R}^n, \mathbf{I}_n \mathbf{x} = \mathbf{x}
$$

単位行列の構造は単純である。主対角線に沿ったすべてのエントリは1であり、その他のすべてのエントリは0である。例として[@fig:2.2]を参照のこと。

![単位行列の例。これは$\mathbf{I}_3$](fig/2-2.png){#fig:2.2}

$\mathbf{A}$の逆行列は$\mathbf{A}^{-1}$と表記し、以下のような行列として定義される。

$$
\mathbf{A}^{-1} \mathbf{A} = \mathbf{I}_n
$$

すると、以下の手順を用いて[@eq:2.11]を解くことができる。

$$
\mathbf{A} \mathbf{x} = \mathbf{b}
$$

$$
\mathbf{A}^{-1} \mathbf{A} \mathbf{x} = \mathbf{A}^{-1} \mathbf{b}
$$

$$
\mathbf{I}_n \mathbf{x} = \mathbf{A}^{-1} \mathbf{b}
$$

$$
\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}
$$

もちろん、この処理は$\mathbf{A}^{-1}$を見つけられるということに依存する。我々は以下の章で$\mathbf{A}^{-1}$が存在する条件を検討する。
$\mathbf{A}^{-1}$が存在するとき、いくつかの異なるアルゴリズムは閉形式においてこれを見つけることができる。理論上、同じ逆行列は$\mathbf{b}$の様々な値に対して何度もその式を解くことに使える。$\mathbf{A}^{-1}$は主に理論的なツールとして役に立つが、実際には多くのソフトウェアアプリケーションに対して実践で使わるべきではない。$\mathbf{A}^{-1}$はディジタルコンピュータ上では限定的な精度でのみ表現される可能性があるので、$\mathbf{b}$の値を使うアルゴリズムは通常、$\mathbf{x}$のより正確な推定値を得ることができる。

## 線形従属とspan

存在する$\mathbf{A}^{-1}$に対して、[@eq:2.11]は$\mathbf{b}$の値ごとにたった1つの解のみをもたなければならない。式の系がある$\mathbf{b}$の値に対する解を持たなかったり無限に多くの解を持ったりする可能性もある。しかし、特定の$\mathbf{b}$に対して無限より少なく1つより多い解を取りえない。つまり、$\mathbf{x}$と$\mathbf{y}$の両方が解でありならば、以下はいかなる実数$\alpha$において解でもある。

$$
\mathbf{z} = \alpha \mathbf{x} + (1 - \alpha) \mathbf{y}
$$

その式がいくつの解を持つかを解析するため、$\mathbf{A}$の列を原点（すべて0のベクトルで指定される点）から向かうことができる様々な方向を指定すると考え、そして、$\mathbf{b}$に至る道がいくつあるかを決定する。この視点では、$\mathbf{x}$の各要素はこれらの方向のそれぞれにどれだけ距離だけ向かうべきであるかを指定する。$i$列目の方向にどれだけの距離を移動するかを指定する$x_i$として、

$$
\mathbf{A} \mathbf{x} = \sum_i x_i \mathbf{A}_{:,i}
$$

一般に、この種の操作は***。

## 9.10 {#sec:9.10}

## 10.7 {#sec:10.7}

### 12.1.2 {#sec:12.1.2}

## 12.3 {#sec:12.3}

# 15 {#sec:15}

## 15.1 {#sec:15.1}
