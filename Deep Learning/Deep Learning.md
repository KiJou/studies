---
title: Deep Learning [@Goodfellow2016]
---
# はじめに

発明家たちは考える機械を作り出すことを長らく夢見ていた。この願望は少なくとも古代ギリシアの代に遡る。神話上の人物[mythical figures]であるPygmalion、Daedalus、Hephaestusはすべて伝説的な発明家と解釈することができ、Galatea、Talos、Pandoraはすべて人工的な生命とみなすことができる。([@Ovid2004; @Sparkes1996; @Tandy1997])
プログラム可能なコンピュータが最初に発想された[conceived]とき、人々はそのような機械が賢くなれるかどうかを、それが作られる以前に100年以上に渡って、知りたいと思っていた。こんにち、人工知能(AI)は多くの実用的なアプリケーションや活発な研究テーマ[research topics]がある盛んな分野[thriving field]である。我々は手順の決まった作業[routine labor]を自動化したり、音声や画像を理解したり、診断を行ったり[make diagnoses in medicine]、基礎科学研究を補助したりする知的なソフトウェアに目を向けている。
人工知能分野の初期の頃は、人間の頭では難しい[intelectually difficult for human beings]がコンピュータには比較的簡単[relatively straightforward]である問題、すなわち、形式的で数学的な規則のリストによって説明できる問題に急速に取り組んで解決していた。後に、人工知能の真の課題は、人々が行うには容易だが形式的に説明するのが難しい作業、すなわち、話された言葉や画像中の顔を認識するような、意識せず、直観的に解いている問題を解決することであると分かった。
本書はこれらのより直観的な問題への解法について書かれている。この解法はコンピュータが経験から学び、より単純な概念との関係を通して定義される個々の概念による概念の階層の観点から世界を理解することを可能にする。経験から知識を収集することで、このアプローチはコンピュータが必要とするすべての知識を形式的に指定する人間のオペレータを必要としない。概念の階層はより単純なものからそれらを構築することでコンピュータが複雑な概念を学習することを可能にする。これらの概念がどのように積み重なって構築されるかを示すために図を描くとすると、その図は深い[deep]、すなわち、多くの層を持っている。この理由により、我々はこのアプローチをAIの深層学習[deep learning]と呼んでいる。
AIの初期の成果の多くは比較的に邪魔の無い[sterile]で形式的な環境で行われ、コンピュータはその世界に関する多くの知識を持つ必要がなかった。例えば、IBMのDeep Blueというチェスシステムは1997年に世界チャンピオンのGarry Kasparovを破った[@Hsu2002]。チェスは、勿論、64個のマスと厳密に決められた方法のみに移動できる32個のコマだけで構成される非常に単純な世界である。出来の良いチェスの戦略を考案することは途方もない成果であるが、その課題は一連のチェスのコマや可能な移動をコンピュータに説明することの難しさによるものではない。チェスは完全に形式的な規則の非常に簡単なリストによって完璧に説明できる。これはプログラマによって前もって容易に提供される。
皮肉なことに、人間にとって中でも最も難しい精神的な仕事である抽象的で形式的な作業はコンピュータにとって中でも最も簡単である。コンピュータは最強の人間のチェスプレイヤーさえも破ることが長らく可能であったが、物体や言葉を認識する平均的な人間の能力の一部に匹敵するように最近なったばかりである。人間の日々の生活は膨大な量の世界に関する知識を必要とする。この知識の大半は主観的かつ直観的であり、それゆえに、形式的な方法で明確に表現する[articulate]は難しい。コンピュータは知的な方法で振る舞うためにこれと同じ知識を捕捉する必要がある。人工知能における重要な課題のひとつはこの形式的でない知識をコンピュータにどのようにして与えるかということである。
いくつかの人工知能プロジェクトは形式言語で世界に関する知識をハードコードすることを目指していた。コンピュータは論理的な推論規則を用いてこれらの形式言語における命題[statement]に関して自動的に理由付けを行うことができる。これは人工知能の知識ベースのアプローチとして知られる。これらのプロジェクトは大きな成果には結びつかなかった。このようなプロジェクトの最も有名なもののひとつとしてCycがある[@Lenat1989]。CycはCycLと呼ばれる言語における推論エンジンおよび命題データベースである。これらの命題は人間の教師[supervisor]のひとりによって入力される。これはやっかいな[unwieldy]プロセスである。人々は世界を正確に記述するために十分な複雑さを持つ形式的な規則を考案しようと取り組む。例えば、Cycは朝に髭を剃るFredという名の人間についての話を理解できなかった[@Linde1992]。この推論エンジンは話の矛盾を検出した。つまり、人が電気的な部分を持たないことを知っているが、Fredが電気カミソリを持っていたので、"髭剃り中のFred"というエンティティが電気的な部分から構成されると理解した。そのため、Fredが髭剃り中でもヒトであるかと訪ねたのだった。
ハードコードされた知識に依存するシステムによって直面する困難はAIシステムが、生データからパターンを抽出することで、自らの知識を獲得する能力を必要としていることを示唆している。この能力は機械学習として知られる。機械学習の事始めはコンピュータが現実世界の知識を伴う問題に取り組み、主観的なように見える決定を下すことを可能にすることであった。ロジスティック回帰[logistic regression]とよばれる単純な機械学習アルゴリズムは帝王切開分娩[cesarean delivery]を推奨するかどうかを決定できる[@Mor-Yosef1990]。naive Bayesと呼ばれる単純な機械学習アルゴリズムは正当なメールとスパムメールを分けることができる。
これらの単純な機械学習アルゴリズムのパフォーマンスはそれらに与えられるデータの表現[representation]に強く依存する。例えば、ロジスティック回帰が帝王切開分娩を勧めるのに使われるとき、AIシステムは患者を直接検査しない。代わりに、医師が、子宮瘢痕[uterine scar]の有無といった、関連情報の一部をシステムに伝える。患者の表現に含まれる情報の各部分は特徴[feature]として知られる。ロジスティック回帰はこれらの患者の特徴のそれぞれが様々な結果とどれだけ関連しているかを学習する。しかし、特徴がどのように定義されているかにはまったくもって影響を与えることはない。ロジスティック回帰に、形式化された医師の報告ではなく、患者のMRIを与えた場合、有用な予測をさせることはできないだろう。MRI画像の個々のピクセルは分娩中に発生するかもしれないいずれかの合併症と僅かな相関を持っている。この表現への依存性はコンピュータ・サイエンスや日常生活にさえも至るところに現れる一般的な現象である。コンピュータサイエンスでは、データの集合を検索するような操作は、集合が上手に[intelligently]構造化され、インデックス化されると、指数関数的に速く進む。人々はアラビア数字で計算をすることは容易に可能だが、ローマ数字で計算を見出すにはさらにもっと多くの間がかかる。表現の選択が機械学習アルゴリズムのパフォーマンスに多大な影響をもたらすことは驚くことではない。単純な視覚的な例として、[@fig:1.1]を参考にすること。

![異なる表現の例：散布図に線を引いて2つのデータのカテゴリに分けたいとしよう。左図では、デカルト座標でいくつかのデータを表していて、線を引くことはできない。右図では、極座標でデータを表していて、縦線を引いて解けるほど単純になっている。（図はDavid Warde-Farleyとの共作）](fig/1-1.png){#fig:1.1}

多くの人工知能タスクはそのタスク用に抽出された正しい特徴セットを設計し、単純な機械学習アルゴリズムにそれらの特徴を与えることで解くことができる。例えば、音声から話者を識別するのに有用な特徴は話者の声道の大きさの推定値である。この特徴は話者が男性か女性か子供かといった強力な手がかりをもたらす。
しかし、多くのタスクでは、何の特徴を抽出すべきかを知ることは難しい。例えば、写真の中の車を検出するためのプログラムを書きたいとしよう。我々は車にホイールがあることを知っているので、特徴としてホイールの有無を利用したいと思うかもしれない。残念ながら、ピクセル値の観点からホイールがどう見えるかを厳密に説明することは難しい。ホイールは単純な幾何形状を持つが、その画像は、ホイールに落ちる影、ホイールの金属部分に反射する[glaring off]太陽、車のフェンダー、ホイールの一部を覆う手前にある物体によって複雑になりうる。
この問題への解法のひとつは表現から出力へのマッピングだけでなく表現それ自体を発見するために機械学習を使うことである。このアプローチは表現学習[representation learning]として知られる。学習した表現はしばしば手作業で設計された表現で得られるものよりも更に良いパフォーマンスとなる。これらは、最小限の人間の介入で、AIシステムが新しいタスクにすばやく適応することも可能にする。表現学習アルゴリズムは単純なタスクには分単位で、複雑なタスクには数間から数ヶ月単位で良好な特徴セットを見つけることができる。複雑なタスクに対して手動で特徴を設計することは膨大な量の人間の時間と労力を要する。つまり、研究者コミュニティ全体では何十年かかる可能性がある。
表現学習アルゴリズムの典型的な例はautoencoderである。autoencoderは入力データを異なる表現に変換するエンコーダ機能とその新しい表現を元のフォーマットに変換し直すデコーダ機能の組み合わせである。autoencoderは、入力がエンコーダとデコーダを通るとき、できるだけ多くの情報を保存するように訓練されるが、新しい表現が様々な良好な特性を持たせるようにも訓練される。異なる種類のautoencoderは異なる種類の特性を達成することを目的としている。
特徴や特徴を学習するためのアルゴリズムを設計するとき、我々の目標は通常観測データを説明するバラツキの要因[factors of variation]を分割することである。この文脈において、我々は「要因」という語を単に影響の源を分けることを指すためだけに用いる。すなわち、この要因は通常では乗算によって組み合わされない。そのような要因は直接観測される量ではしばしばない。代わりに、これらは観測可能な量に影響を与える物理世界における観測不可能な物体や観測不可能な力として存在し得る。これらは有用な簡単化した説明をもたらす人間の脳内の構築物や観測データの推測される原因としても存在し得る。これらはそのデータにおける高度なバラツキを理解するのに役立つ概念や抽象化として考えられる。発言の録音を解析するとき、バラツキの要因は話者の年齢、性別、訛り、話している言葉を含む。車の画像を解析するとき、バラツキの要因は車の位置、色、角度、太陽の明るさを含む。
多くの現実世界の人工知能アプリケーションにおける困難さの主な原因はバラツキの要因の多くがありとあらゆる観測可能なデータの断片に影響を及ぼすことである。赤い車の画像にある個々のピクセルは夜中ではほとんど真っ黒となるかもしれない。車の輪郭の形状は見る角度に依存する。ほとんどのアプリケーションはバラツキの要因を紐解き[disentangle]、考慮しないものを破棄する必要がある。
もちろん、そのような高レベルで抽象的な特徴を生データから抽出することは非常に困難である可能性がある。話者の訛りのような、これらのバラツキの要因の多くは洗練されたほぼ人間レベルのデータの理解を用いることでのみ識別できる。元の問題を解くのとほぼ同じくらい表現を得ることが難しいとき、表現学習は一見役に立つようには見えない。
深層学習[deep learning]は表現学習におけるこの中心的問題を他のより単純な表現の観点から説明される表現を導入することによって解決する。深層学習はコンピュータがより単純な概念から複雑な概念を構築することを可能にする。[@fig:1.2]はどのように深層学習システムがヒトの画像の概念をカドや輪郭のような単純な概念を組み合わせることで表現する事ができるかを示す。そして、カドや輪郭は同様にエッジで定義される。

![深層学習モデルの図解。コンピュータにとって、ピクセル値の集合で表されるこの画像のような、そのままの感覚の入力データの意味を理解することは難しい。一連のピクセルから物体の正体へマッピングする関数は非常に複雑である。このマッピングを学習または評価することは直接的に取り組んだ場合には打開不可能であるように思える。深層学習は望まれる複雑なマッピングを、それぞれがモデルの異なる層によって記述される、入れ子になった単純なマッピングの列に分解することでこの難しさを解決する。この入力は、観測可能な変数を含むという理由で名付けられた、可視層[visible layer]で表現される。そして、隠れ層[hidden layer]の列は画像から抽象的な特徴を徐々に抽出する。これらの層は値がデータに含まれずことから「隠れ」と呼ばれる。その代わりに、モデルは概念が観測データにおける関係を説明するのに有用であるかを決定しなければならない。ここにある画像は各隠れユニットによって表現される特徴の種類の可視化したものである。ピクセルがあるとすると、第1層は、近傍のピクセルの明るさを比較することで、エッジを容易に識別できる。第1隠れ層のエッジの説明があるとすると、第2隠れ層はカドや拡張された輪郭を容易に検索できる。これは、エッジの集合として識別可能である。第2隠れ層のカドや輪郭に関する画像の説明があるとすると、第3隠れ層は、カドや輪郭の特定の集合を見つけることで、特定の物体の全体像を検出できる。最後に、この物体部分に関する画像の説明は画像中に物体の存在を識別するのに使うことができる。画像は @Zeiler2014 の許可を得て転載した。](fig/1-2.png){#fig:1.2}

深層学習モデルの典型的な例はfeedforward deep networkや多層パーセプトロン[multilayer perceptron; MLP]である。多層パーセプトロンはある入力値の集合を出力値にマッピングする単なる数学的な関数である。この関数は多数のより単純な関数を組み立てることで形作られる。異なる数学的な関数のそれぞれのアプリケーションを入力の新しい表現をもたらすとみなすことができる。
データに対して正しい表現を学習するというアイデアは深層学習に対する一側面である。深層学習の別の側面は深さがコンピュータに多段のコンピュータプログラムを学習することを可能にするということである。表現の各層は並列に別の命令セットを実行した後のコンピュータのメモリの状態として考えることができる。より大きな深さを持つネットワークはより多くの命令を次第に実行できる。順次命令は、以後の命令が以前の命令の結果を参照し直すことができるので、強大な力をもたらす。深層学習のこの視点により、層のactivationにあるすべての情報は入力を説明するバラツキの要因を必ずしもエンコードしているわけではない。その表現は入力を理解できるプログラムを実行するのに役立つ状態情報をも保存する。この状態情報は伝統的なコンピュータ・プログラムにおけるカウンタやポインタに類似するかもしれない。入力の中身には全くもって関係ないが、モデルがその処理をまとめるのに役立つ。
モデルの深さを測る主要な方法は2つある。ひとつの見方はそのアーキテクチャを計算するのに実行する必要がある順次命令の数に基づく。我々はこれを入力を考慮してモデルの各出力を計算する方法を述べるフローチャートを通る最長経路の長さとみなすことができる。2つの等価なコンピュータプログラムがどの言語でプログラムが書かれているかに依存して異なる長さを持つであろうことと同様に、同じ関数はどの関数がフローチャートにおける個々のステップとして利用する事ができるかに依存して異なる深さを持つフローチャートとして描かれ得る。[@fig:1.3]はどのようにしてこの言語の選択が同じアーキテクチャに対して2つの異なる値[measurements]をもたらし得るかを描いている。

![各ノードが操作を行うような、入力を出力にマッピングする計算グラフの図解。深さは入力から出力への最長経路の長さであるが、取り得る計算ステップを構成するやり方の定義に依存する。これらのグラフで図示される計算はロジスティック回帰モデル$\sigma(w^T x)$の出力である。ここで、$\sigma$はlogistic sigmoid functionである。コンピュータ言語の要素として加算、乗算、logistic sigmoidを使う場合、このモデルは3の深度を持つ。ロジスティック回帰それ自体を1つの要素と見る場合、このモデルは1の深度を持つ。](fig/1-3.png){#fig:1.3}

もうひとつのアプローチは、深い確率的なモデルで使われるが、モデルの深さを計算グラフの深さではなく概念が互いにどう関係するかを記述するグラフの深さであるとみなす。この場合、各概念の表現を計算する必要がある計算のフローチャートの深さは概念それ自体のグラフよりも深いかもしれない。これは、より複雑な概念に関する情報を与えられれば、より単純な概念のシステムの理解を改良できるためである。例えば、影の中にある1つの眼を持つ顔の画像を観察するAIシステムは最初のうちは1つの眼のみを理解するかもしれない。顔が存在することを検出した後は、そのシステムは第2の眼が同様におそらく存在することを推論できる。この場合、概念のグラフは2つの層、すなわち、眼のための層と顔のための層のみを含むが、計算のグラフはn回を与えるように各概念の推定値を改良するならば、2n個の層を含む。
これら2つの見方、すなわち、計算グラフの深さと確率的なモデルのグラフの深さ、のどちらが最適であるかは常に明確ではないので、そして、これらのグラフを構築する一連の最小要素は人によりけりなので、コンピュータプログラムの長さに対する単一の正しい値が存在しないのと同様に、アーキテクチャの深度に対する単一の正しい値は存在しない。また、モデルが「深層」足り得るためにどれだけの深さが必要であるかに関してのコンセンサスは存在しない。しかし、深層学習は関数を学習したり概念を学習したりする構造の量を伝統的な機械学習が行う場合よりも多く伴うモデルの研究であると問題なくみなすことができる。
まとめると、本書の主題である深層学習はAIへのアプローチである。具体的には、機械学習の一種であり、コンピュータシステムが経験とデータによって改善することを可能にする技術である。我々は機械学習が複雑な現実世界の環境で操作できるAIシステムを構築するための唯一実行可能なアプローチであると主張する。深層学習は、より単純な概念との関係で定義される各概念とより抽象度の低いものの観点から計算されるより抽象的な表現を持つ入れ子になった概念の階層として世界を表現することで大きな力と柔軟性を達成する機械学習の特定の種類である。[@fig:1.4]はこれらの異なるAI分野の間の関係を図示する。[@fig:1.5]はそれぞれがどのように機能するかの高レベルな図式を提示する。

![どんな深層学習が表現学習の一種であるかということ、同様に機械学習の一種であるということ、いろんなものに使われているがAIへのアプローチのすべてではないということを示すベン図。ベン図の各セクションはAI技術の例を含む。](fig/1-4.png){#fig:1.4}

![AIシステムの異なる部分が異なるAI分野内と互いにどのように関連し合うかを示すフローチャート。色付き[shaded boxes]はデータから学習可能である要素を示す。](fig/1-5.png){#fig:1.5}

## 本書を読むべきは誰か？

- 要約：
    - この本は機械学習を学ぶ学生や機械学習を使いたいソフトウェアエンジニアを読者として想定している
    - パート1は応用数学と機械学習の基礎、パート2実践的な深度学習アルゴリズム、パート3は最新研究動向
    - 必要無いと感じた箇所は順次読み飛ばしてもらって構わない

![本書の高レベルな構成図。ある章からある章への矢印は前者が後者を理解することへの前提となる資料であることを示す。](fig/1-6.png){#fig:1.6}

## 深層学習における歴史的傾向

ある歴史的な文脈で深度学習を理解することは最も簡単である。深度学習の詳細な歴史を提供する代わりに、いくつかの重要な傾向を見ていく。

- 深度学習は長く濃い歴史を歩んできたが、異なる哲学的な視点を反映するように、いろんな名前で行われ、人気には上がり下がりがあった。
- 深度学習は扱える訓練データの量が増えるにつれて有用になっていった。
- 深度学習モデルは深度学習に対するコンピュータインフラ（ハードウェアとソフトウェアの両方）が改良するにつれて時間とともに大型化してきた。
- 深度学習は時間とともに正確性を向上させながら、ますます複雑になるアプリケーションを解いてきた。

### 様々な呼び名とニューラルネットワークの運命の変化

我々は、本書の多くの読者が深層学習をワクワクする新技術として耳にしてきたが、新興の分野に関する本で「歴史」と書かれているのを目にして驚いている、と予測している。事実、深層学習は1940年代にまで遡ることができる。現在の人気に先立って何年もの間比較的人気がなかった、また、最近では「深層学習」とだけ呼ばれているが、様々な異なる呼び名で広まっていたことから、深度学習は単に新しいように見えるだけである。この分野は、異なる研究者や異なる見方の影響を反映するために、幾度となくリブランディングを繰り返してきた。
深層学習の包括的な歴史は本書の範疇を逸脱する。しかし、いくつかの基本的な前後関係[context]は深層学習を理解するために有用である。大まかに言えば、3つの発展の波があった。1940年代から1960年代におけるcyberneticsとして知られる深層学習、1980年代から1990年代におけるconnectionismとして知られる深層学習、2006年に始まる深層学習の名の下の現在の再起である。これは[@fig:1.7]で定量的に図示される。

![Google Booksに従って、"cybernetics"、"connectionism"または"neural networks"としての語句の頻度によって計測される通りの、人工ニューラルネット研究の3つの歴史的な波のうちの2つ（第三波は出現が最近すぎる）。第一波は、biological learning[@McCulloch1943; @Hebb1949]の理論群の発展や単一のニューロンの訓練を可能にするパーセプトロン[@Rosenblatt1958]のような最初のモデルの実装とともに、1940年代から1960年代にcyberneticsとして始まった。第二波は、1つか2つの隠れ層を持つニューラルネットワークを訓練するための逆伝播法[@Rumelhart1986a]とともに、1980年から1995年の間のconnectionistアプローチとして始まった。現在の第三波である深層学習は2006年頃に始まり[@Hinton2006; @Bengio2007; @Ranzato2007a]、2016年現在には本の形としてたった今出現している。他の2つの波は関連する科学活動が起こってからかなり後に本の形として同じように現れた。](fig/1-7.png){#fig:1.7}

我々がこんにち認識している最初期の学習アルゴリズムのいくつかはbiological learningの計算モデル、すなわち、学習が脳内でどのように起こる、または、起こり得るかについてのモデルであることが意図されていた。結果として、深層学習が広まったときの名前のひとつに人工ニューラルネットワーク[artifiicial neural networks; ANN]というものがある。これに対応する深層学習モデルの見方は、これらが（人間の脳かその他の動物の脳のいずれかの）生物学的な脳に触発された工学システムであるという点である。機械学習にt買われるニューラルネットワークの種類は時折に脳の機能を理解するために使われてきた[@Hinton1991]が、これらは現実的な生物学的な機能のモデルとして一般に設計されていない。深層学習でのニューラル的視点は2つの主なアイデアによって動機付けられる。アイデアのひとつは脳が知的な振る舞いが可能であるという例えによる証明をもたらすということであり、知能を構築するのに概念的に簡単な道筋は脳の背後にある計算の原則をリバースエンジニアリングして、その機能性を複製することである。もうひとつの見方は脳や人間の知能の基礎をなす原則を理解することがとても興味深いことになるだろうということである。つまり、これらの基礎科学の疑問に光を当てる機械学習モデルは工学アプリケーションを解決する能力以外では有用である。
モダンな用語としての「深度学習」は機械学習モデルの現在の種類に関する神経科学的な視点を超えている。これは複数の構成レベルを学習するというより一般的な原則に訴える。これは、神経に触発される必要のない機械学習フレームワークで適用させることができる。
モダンな深層学習の最初期のものは神経科学的な視点から動機付けられる単純な線形モデルであった。これらのモデルは$n$個の入力値$x_1, \dots, x_n$を取り、出力$y$に関係づけるように設計された。これらのモデルは重み$w_1, \dots, w_n$を学習し、出力$f(\mathbb{x}, \mathbb{w}) = x_1 w_1 + \dots = x_n w_n$を計算するだろう。このニューラルネットワーク研究の第一波は、[@fig:1.7]に記されるように、cyberneticsとして知られた。
McCulloch-Pittsのニューロン[@McCulloch1943]は初期の脳機能のモデルであった。この線形モデルは$f(x, w)$が正か負かを確かめることで入力の2つの異なるカテゴリを認識できた。もちろん、モデルがカテゴリの望ましい定義に一致するためには、重みを正確に設定する必要があった。これらの重みは人間のオペレータによって設定させることができた。1950年代には、パーセプトロン[@Rosenblatt1958; @Rosenblat1962]は各カテゴリから入力例を与えることでカテゴリを定義得る重みを学習する事ができた最初のモデルとなった。ほぼ同時期に始まったadaptive linear element (ADALINE)は実数を予測するために$f(x)$の値自体を単純に返す[@Widrow1960]。また、データからこれらの値を予測するように学習することができた。
これらの単純な学習アルゴリズムは近年の機械学習の状況に大きく影響した。ADALINEの重みを適合するのに使われる訓練アルゴリズムは確率的勾配降下法[stochastic gradient descent]と呼ばれるアルゴリズムの特殊な場合であった。確率的勾配降下法アルゴリズムの微修正版はこんにちの深度学習モデルに対する支配的な訓練アルゴリズムとして存続している。
パーセプトロンやADALINEで使われる$f(x, w)$に基づくモデルは線形モデルと呼ばれる。これらのモデルは最も広く使われる機械学習モデルのひとつとして存続しているが、多くの場合、元のモデルが訓練した方法ではない異なる方法で訓練される。
線形モデルは多くの制限がある。最も有名なものとして、これらはXOR関数を学習できない。ここで、$f([0, 1], w) = 1$かつ$f([1, 0], w) = 1$だが、$f([1, 1], w) = 0$かつ$f([0, 0], w) = 0$である。線形モデルにおけるこれらの欠点を見つけた批評家たちは一般に生物学に触発された学習に対して反発を起こした[@Minsky1969]。これはニューラルネットワークの人気における最初の大きな落ち込み[first major dip]であった。
こんにち、神経科学は深層学習の研究者にとっての重要なインスピレーション源とみなされているが、当分野に対する主要な手引き[predominant guide]ではもはやない。
こんにちの深層学習研究における神経科学の役割の縮小に対する主な理由は単純に手引きとして使うには脳に関して十分な情報を持っていないということにある。脳で使われる実際のアルゴリズムの深い理解を得るには、（最低限でも）何千の相互接続されたニューロンの活動を同時にモニターすることができる必要があるだろう。我々はこれをすることができないので、最も単純でよく研究されている脳の一部のいくつかでさえもなかなか理解できていない[@Olshausen2005]。
神経科学は単一の深層学習アルゴリズムが多くの異なるタスクを解くことが可能であってほしいと思う理由をもたらした。神経科学者はフェレットが視覚の信号を脳の聴覚を処理する領域に送るようにつなぎ替えるとそこで「見る」ことを学習することができることを発見した[@VonMelchner2000]。これは多くの哺乳類の脳がその脳で処理される様々なタスクのほとんどを解くために単一のアルゴリズムを用いているのだろうということを示唆している。この仮説の以前では、機械学習研究は、自然言語処理、視覚、経路計画[motion planning]、発話認識を研究する研究者の異なるコミュニティがあり、更に分かれていた。こんにち、これらのアプリケーションのコミュニティは依然として別れたままだが、同時に深層学習の研究グループがこれらの応用分野の多く、さらには、すべてを研究することは一般的である。
我々は神経科学由来のいくつかの大まかなガイドラインを描くことができる。お互いの相互作用を介することでのみ知的になる多くの計算ユニットを持つことの基本的なアイデアは脳に触発されている。neocognitron[@Fukushima1980]は哺乳類の視覚系の構造に触発された画像処理のための強力なモデルのアーキテクチャをもたらした。これは後に、[@sec:9.10]に見られる、モダンなconvolutional networkの基礎となった[@LeCun1998b]。こんにちの殆どのニューラルネットワークは正規化線形ユニット[rectified linear unit]と呼ばれるモデルのニューロンに基づいている。オリジナルのcognitron[@Fukushima1975]は我々の脳機能の知識に強く触発されたより複雑なバージョンを導入した。単純化されたモダンなバージョンは、1つのinfluenceとして神経科学を引用する@Nair2010や@Glorot2011a、より工学指向なinfluencesを引用する@Jarrett2009といった、多くの視点からのアイデアを合体させて開発された。神経科学は重要なインスピレーション源である一方、厳格な手引きと受け取る必要はない。我々は実際のニューロンがモダンな正規化線形ユニット以上に非常に様々な関数を計算することを知っているが、ニューロンを現実に近づけることは機械学習のパフォーマンスに改善を未だもたらしていない。また、神経科学はいくつかのニューラルネットワークのアーキテクチャにうまくインスピレーションを与えたが、我々はこれらのアーキテクチャを訓練するのに使う学習アルゴリズムに対するさらなる手引きを提示するための、神経科学に対するbiological learningに関して未だ十分に知らない。報道記事は脳と深層学習の類似性をしばしば強調する。これは、kernel machinesやベイズ統計のような他の機械学習分野で働いている研究者よりも、深層学習の研究者が1つの影響として脳を引用する可能性が高いという点では正しいが、脳をシミュレートする試みとして深層学習を見るべきではない。モダンな深層学習は多数の分野、特に、線形代数、確率論、情報理論、数値最適化のような応用数学基盤からインスピレーションを受けいる。幾人かの深層学習の研究者は重要なインスピレーション源として神経科学を引用するが、その他の人たちは神経科学を全く考慮しない。
アルゴリズムレベルで脳がどのように働くかを理解する努力が健在であることは記しておきたい。この努力は「計算論的神経科学[computational neuroscience]」として主に知られ、深層学習とは研究分野が分かれている。研究者にとって両分野を行ったり来たりすることは普通のことである。深層学習の分野は知能を必要とするタスクをうまく解く事ができるコンピュータシステムを構築する方法と主に関係しており、計算論的神経科学は脳が実際にどのように機能しているかのより正確なモデルを構築することと主に関係している。1980年代には、connectionismやparalell distributed processingと呼ばれるムーブメントを介してニューラルネットワーク研究の第二波が大半出現した[@Rumelhart1986c; @McClelland1995]。connectionismは認知科学の文脈で発生した。認知科学は複数の異なる解析レベルを組み合わせて意識を理解する学際的アプローチである。1980年の初頭では、ほとんどの認知科学者は記号推論[symbolic reasoning]のモデルを研究した。その人気にもかかわらず、記号モデルはどのようにして脳がニューロンを用いて実際に実装できるだろうかという観点から説明するのが難しかった。connectionistsはニューロン的な実装に実際に根拠を置くことができる認知のモデルを研究し始めた[@Touretzky1985]。これは1940年代の心理学者Donald Hebbの研究まで遡る多くのアイデアを復活させた。
connectionismの中心的なアイデアは多数の単純な計算ユニットが一緒にネットワーク化されるとき知的な振る舞いを達成できることである。この洞察は計算モデルにおける隠れユニットへ適用されるように生物学的な神経系におけるニューロンにも同様に適用される。
1980年代のconnectionism運動の間には、こんにちの深度学習の中心にあり続けるいくつかの重要な概念が生じた。
これらの概念のひとつは分散表現[distributed representation]についてである[@Hinton1986]。これはシステムへの各入力が多くの特徴によって表現されるべきであり、各特徴が多くの取り得る入力の表現を伴うべきであるというアイデアである。例えば、車、トラック、鳥を認識できる視覚システムがあり、これらの物体が赤、緑、青のいずれかとしよう。表現する方法の一つとして、これらの入力は、赤いトラック、赤い車、赤い鳥、緑のトラック、以下略といった9つの取り得る組み合わせのそれぞれに対して活性化する個別のニューロンまたは隠れユニットを持つことだろう。これは9つの異なるニューロンを必要とし、各ニューロンは色の概念と物体の正体を独立して学習しなければならない。この状況を改善する方法のひとつとして、色を述べる3つのニューロンと物体の正体を述べる3つのニューロンによる分散表現を使うことがある。これは合計9つではなく6つのニューロンのみを必要とし、赤さを説明するニューロンは、ある特定の物体カテゴリの画像からだけでなく、車、トラック、鳥の画像から赤さについて学習できる。分散表現の概念は本書の中心であり、[@sec:15]でより詳細に記述してある。
もうひとつのconnectionist運動の成果は内部表現を伴うディープニューラルネットワークを訓練するために逆伝播法をうまく利用したことと、逆伝播法を普及させたことである[@Rumelhart1986a; @LeCun1987]。このアルゴリズムは人気が上がったり下がったりしてきたが、本書を執筆している現在では、深層モデルを訓練する支配的なアプローチである。
1990年代に、研究者たちはニューラルネットワークのシーケンスのモデリングについて重要な発展を成し遂げた。@Hochreiter1991と@Bengio1994は、[@sec:10.7]で示されるように、長いシーケンスのモデリングにおいて基盤の数学的な困難さのいくつかを識別した。@Hochreiter1997はこれらの困難さのいくつかを解くためのlong short-term memory (LSTM)ネットワークを導いた。こんにち、LSTMは、Googleでの多くの自然言語処理タスクを含めた、多数のシーケンスモデリングのタスクに広く使われている。
ニューラルネットワーク研究の第二波は1990年代中頃まで続いた。ニューラルネットワークや他のAI技術に基づく投機は投資を求めつつ非現実で野心的な主張に変わり始めた。AI研究がこれらの不当な期待を果たさなかったとき、投資家は失望した。同時に、機械学習のその他の分野は発展を遂げた。kernel machines[@Boser1992; @Cortes1995; @Scholkopf1999]やグラフィカルモデル[@Jordan1998]の両方は多くの重要なタスクで良い成果を出した。これら2つの要因は2007年まで続いたニューラルネットワークの人気の衰退をもたらした。
時を同じくして、ニューラルネットワークはいくつかのタスクで印象的なパフォーマンスを得続けた[@LeCun1998b; @Bengio2001]。Canadian Institute for Advanced Research (CIFAR)はそのNeural Computation and Adaptive Perception (NCAP)研究イニシアティブを介してニューラルネットワーク研究を生きながらえさせるのを援助したこの計画はトロント大学のGeoffrey Hinton、モントリオール大学のYoshua Bengio、ニューヨーク大学のYann LeCunに率いられる機械学習研究グループを結成した。学際的なCIFAR NCAP研究イニシアティブは神経科学者、人間の視覚やコンピュータビジョンの専門家も含まれた。
この時点で、ディープネットワークは訓練するのが非常に難しいと一般に理解されていた。現在の我々は1980年代から存在しているアルゴリズムが非常に有効であることを知っているが、2006年頃は明らかではなかった。この問題はおそらく単純に、これらのアルゴリズムが当時に使えるハードウェアで多くの実験を可能にするには計算コストが高すぎた、といことである。
ニューラルネットワーク研究の第三波は2006年のブレイクスルーを以て始まった。Geoffrey Hintonはdeep belief networkと呼ばれるニューラルネットワークの一種がgreedy layer-wise pretrainingと呼ばれる戦略を用いて効率的に訓練できることを示した[@Hinton2006]。これは、[@sec:15.1]にさらなる詳細を記載している。他のCIFAR関連[CIFAR-affiliated]の研究グループは同様の戦略が多くの他の種類のディープネットワークを訓練するのに使えること[@Bengio2007; @Ranzato2007a]、テスト例の一般化を改良するのに役立つことをまたたく間に示した。このニューラルネットワーク研究の波は、今や研究者が以前に可能であったよりも深いニューラルネットワークを訓練できることを強調し、深さの理論的重要性に注意を向けるために「深層学習」という用語の使用を広めた[@Bengio2007; @Delalleau2011; @Pascanu2014a; @Montufar2014]。このとき、ディープニューラルネットワークは手作業で設計された機能性と同程度の他の機械学習に基づく競合のAIシステムより勝っていた。このニューラルネットワークの人気の第三波は執筆当時まで続いているが、深層学習研究の焦点はこの波のさなかであって目まぐるしく変化してきた。この第三波は新しい教師なし学習技術と小さなデータセットから上手に一般化する深層モデルの能力に焦点を当てて始まったが、こんにちでは、より古い教師あり学習アルゴリズムと大きなラベル付きデータセットを活用する深層モデルの能力により大きな関心が集まっている。

### データセットサイズの増加

読者の中には、人工ニューラルネットワークをの最初の実験が1950年代に執り行われたとしても、深層学習が重要な技術として最近認識されるようになっただけではないかと疑問に思う方もいるかもしれない。深層学習は1990年代から商用アプリケーションに上手に使われてきたが、最近まで、技術ではなく芸術の一部であるとか専門家のみが使える何かとしてしばしばみなされた。深層学習アルゴリズムから良好なパフォーマンスを得るにはいくつかのスキルが必要とされるという点では正しい。幸いにも、必要なスキルの量は訓練データの量が増えるに連れて減少している。こんにちの複雑なタスクにおける人間のパフォーマンスに達する学習アルゴリズムは1980年代に単純化した例題[toy problem]を解くために取り組んだ学習アルゴリズムとほぼ同一であるが、これらのアルゴリズムで訓練するモデルは非常に深いアーキテクチャの訓練を単純化するよう変化した。最も重要な新しい発展はこんにち我々がこれらがうまくいくために必要な資料とともにこれらのアルゴリズムを提供できるということである。[@fig:1.8]はベンチマークのデータセットのサイズが時間とともにどれだけ顕著に肥大してきたかを示す。この傾向は社会のディジタル化が加速することで引き起こされる。我々の活動がコンピュータで行われるようになればなるほど、より多くの活動が記録される。コンピュータがともにますます接続するたび、これらの記録を一極集中させ、機械学習アプリケーションに適したデータセットにこれらをキュレートすることはより用意になる。「ビッグデータ」の時代は、統計的な推量の重要なburden、すなわち、少ないデータ量のみを観察した後に新しいデータにうまく一般化することが相当にlightenされたので、機械学習をより容易にする。2016年現在、大まかな経験則として、教師あり深層学習アルゴリズムはカテゴリあたり約5000個のラベル付き例で許容できるパフォーマンスを一般に達成するだろうということ、少なくとも1000万個のラベル付き例を含むデータセットで訓練すると人間のパフォーマンスに匹敵するか超えるだろうということがある。

![時とともに増加するデータセットサイズ。1990年初頭、統計学者は手で集めた数百から数千の計測値を用いたデータセットを研究していた[@Garson1900; @Gosset1908; @Anderson1935; @Fisher1936]。1950年代から1980年代にかけて、生物学に触発された機械学習の先駆者たちは、文字の低解像度ビットマップのような、小さな人造[synthetic]データセットでしばしば研究していた。これは、計算コストが低くなるように設計するためであり、ニューラルネットワークが特定の種類の関数を学習できることを実証するためであった[@Widrow1960; @Rumelhart1986b]。1980年代と1990年代には、機械学習はより統計的になり、手書きの数字のスキャンの（[@fig:1.9]に示される）MNISTデータセットのような数万個の例を含むより大きなデータセットを活用し始めた[@LeCun1998b]。2000年代の最初の10年には、CIFAR-10データセットのようなこれと同サイズのより洗練されたデータセット[@Krizhevsky2009]が生み出され続けた。その終わり頃から2010年代の初めの5年にかけて、何十万から数千万の例を含む極めて大きなデータセットが深層学習で可能なことを一変させた。これらのデータセットにはパブリックなStreet View House Numbersデータセット[@Netzer2011]、様々なバージョンのImageNetデータセット[@Deng2009; @Russakovsky2014a]、Soprts-1Mデータセット[@Karpathy2014]が含まれる。図の上部には、Canadian Hansardから構築されたIBMのデータセット[@Brown1990]やWMT 2014 English to Frenchデータセット[@Schwenk2014]のような、訳文[translated sentences]のデータセットが他のデータセットサイズの遥か先を行っていることがわかる。](fig/1-8.png){#fig:1.8}

![MNISTデータセットからの入力の一例。NISTはNational Institute of Standards and Technologyの頭文字であり、このデータを初めに収集した機関である。データが機械学習アルゴリズムでより容易に使うために前処理されたので、"M"は"Modified"（修正版）の頭文字である。MNISTデータセットは手書きの数字のスキャン画像と0から9の数字が各画像に含まれているという関連するラベルから構成される。この単純な分類問題は深層学習研究において最も単純で広く使われるテストのひとつである。これはモダンな技術が解くことは極めて簡単であるにもかかわらず未だに人気である。Geoffrey Hintonは、機械学習の研究者が制御された実験室条件[laboratory conditions]でこれらのアルゴリズムを研究する事ができるという意味として、生物学者がしばしばミバエ[fruit flies]を研究するのと同様に、「機械学習のショウジョウバエ[drosophila]」と説明した。](fig/1-9.png){#fig:1.9}

### モデルサイズの増加

1980年代から比較的小さな成功を収めた後のこんにちにニューラルネットワークが広く成果を上げているもうひとつの重要な理由はこんにちには更に大きなモデルを実行するための計算資源があるということである。connectionismの主な洞察のひとつはニューロンのお起きがともに動作するときに動物が知的になるということである。個々のニューロンや小さなニューロンの集まりは特別に有用ではない。
生物学的なニューロンは特に密に接続されていない。[@fig:1.10]に見える通り、我々の機械学習モデルは何十年の間、哺乳類の脳でさえ数桁の範囲内で、ニューロン1つあたりに多くの接続を持っていた。

![時間に対するニューロンあたりの接続数。初期には、人工ニューラルネットワークにおけるニューロン間の接続数はハードウェア能力によって制限された。こんにち、ニューロン間の接続数はほぼ設計上の検討事項である。いくつかの人工ニューラルネットワークはほとんど猫と同じくらいのニューロンあたりの接続数を持つ。他のニューラルネットワークではネズミのような小型哺乳類と同じくらいのニューロンあたりの接続数を持つことは極めて一般的である。人間の脳でもそこまで法外なニューロンあたりの接続数を持っているわけではない。生物学的なニューラルネットワークの大きさは @Wikipedia2015から。
1. Adaptive linear element [@Widrow1960]
2. Neocognitron [@Fukushima1980]
3. GPU-accelerated convolutional network [@Chellapilla2006]
4. Deep Boltzmann machine [@Salakhutdinov2009a]
5. Unsupervised convolutional network [@Jarrett2009]
6. GPU-accelerated multilayer perceptron [@Ciresan2010]
7. Distributed autoencoder [@Le2012]
8. Multi-GPU convolutional network [@Krizhevsky2012]
9. COTS HPC unsupervised convolutional network [@Coates2013]
10. GoogLeNet [@Szegedy2014a]](fig/1-10.png){#fig:1.10}

ニューロンの総数という観点において、ニューラルネットワークは、[@fig:1.11]に示される通り、ごく最近まで驚くほど小さかった。隠れユニットの導入以来、人工ニューラルネットワークは2.4年ごとに大体2倍のサイズになった。この成長はより大きなメモリを持つより高速なコンピュータによって、そして、より大きなデータセットが利用できるようになることによって育まれた。より大きなネットワークはより複雑なタスクでより高い正確さを達成することを可能にする。この傾向は数十年の間続くことが確実視されている[look set to]。新しい技術がこの成長を速めない限り[enable faster scaling]、人工ニューラルネットワークは少なくとも2050年代まで人間の脳と同じニューロン数を持たないだろう。生物学的なニューロンは現在の人口ニューロンより複雑な機能を表現し得る。そのため、生物学的なニューラルネットワークはこの筋書きが描くよりももっと大きくなるかもしれない。

![時間に対するニューラルネットワークサイズの増加。隠れユニットの導入以来、人工ニューラルネットワークは2.4年ごとに大まかに二倍の大きさになってきた。生物学的なニューラルネットワークサイズは@Wikipedia2015から。
1. Perceptron [@Rosenblatt1958; @Rosenblatt1962]
2. Adaptive linear element [@Widrow1960]
3. Neocognitron [@Fukushima1980]
4. Early back-propagation network [@Rumelhart1986b]
5. Recurrent neural network for speech recognition [@Robinson1991]
6. Multilayer perceptron for speech recognition [@Bengio1991]
7. Mean ﬁeld sigmoid belief network [@Saul1996]
8. LeNet-5 [@LeCun1998b]
9. Echo state network [@Jaeger2004]
10. Deep belief network [@Hinton2006]
11. GPU-accelerated convolutional network [@Chellapilla2006]
12. Deep Boltzmann machine [@Salakhutdinov2009a]
13. GPU-accelerated deep belief network [@Raina2009]
14. Unsupervised convolutional network [@Jarrett2009]
15. GPU-accelerated multilayer perceptron [@Ciresan2010]
16. OMP-1 network [@Coates2011]
17. Distributed autoencoder [@Le2012]
18. Multi-GPU convolutional network [@Krizhevsky2012]
19. COTS HPC unsupervised convolutional network [@Coates2013]
20. GoogLeNet [@Szegedy2014a]](fig/1-11.png){#fig:1.11}

今思えば、ヒル[leech]より少ないニューロンをもつニューラルネットワークが洗練された人工知能の問題を解けないということは特別に驚くことではない。こんにちのネットワークでさえ、カエルのような比較的に原始的な脊椎動物の神経系より小さい。
高速なCPUが使えたり、（[@sec:12.1.2]で説明される）汎用用途のGPUが現れたり、高速なネットワークが接続できたり、分散コンピューティングに対するソフトウェアインフラがよくなったりすることを理由にして、時とともにモデルサイズが増加することは深層学習の歴史における最も重要な傾向のひとつである。この傾向は将来的に続いていくと一般に予測される。

### 正確さ、複雑さ、現実世界の影響の増加

1980年代以降、深層学習は正確な認識や予測をもたらすようにその能力を着実に改善してきた。更に、深層学習はますます広範囲のアプリケーションへ上手に着実に適用していった。
最初期の深層モデルはガッツリとクロッピングされた極小の画像で個々の物体を認識するのに使われた[@Rumelhart1986a]。それから、ニューラルネットワークが処理できる画像サイズが緩やかに増加していった。モダンな物体認識のネットワークはリッチな高解像度の写真を処理し、写真を認識させる物体の近くでクロッピングする必要がない[@Krizhevsky2012]。同様に、最初期のネットワークは2種類の物体（または、ある場合、1種類の物体の有無）だけを認識することができたが、これらのモダンなネットワークは一般に少なくとも1000個の異なる物体カテゴリを認識する。物体認識における最大のコンテストは毎年開催されるImageNet Large Scale Visual Recognition Challenge (ILSVRC)である。convolutional networkが初めてこのchallengeに、最優秀[state-of-the-art]のtop-5誤り率を26.1%から15.3%へと引き下げるような、大差で勝利したとき、深層学習のトップに上り詰める劇的な瞬間が訪れた。これは、convolutional networkが各画像に対して取り得るカテゴリのランク付けされたリストを生成し、その正解カテゴリがこのリストの最初の5つのエントリの中に84.7%の確率で現れるということを意味する。それ以来、これらの競技会は深層畳み込みネットによって着実に勝利を収められ、本書を執筆している現在では、[@fig:1.12]に示される通り、深層学習の進歩がこのコンテストの最新top-5誤り率を3.6%まで引き下げた。

![時間に対する誤り率の減少。ディープネットワークはImageNet Scale Visual Recognition Challengeに参加するのに必要なスケールに達して以来、毎回ますます低い誤り率を生み出しながら、毎年の競技会で着実に勝利してきた。データは@Russakovsky2014bと@He2015から。](fig/1-12.png){#fig:1.12}

深層学習は発話認識でも劇的な影響を与えてきた。1990年代を通して改善した後、発話認識の誤り率は2000年頃から停滞した。発話認識への深層学習の導入[@Dahl2010; @Deng2010b; @Hinton2012a]は誤差率を、あるものは半分になるなど、激減させることとなった。この歴史は[@sec:12.3]でより詳しく説明する。
ディープネットワークは歩行者検出や画像セグメンテーションで目覚ましい成功を収め[@Sermanet2013; @Farabet2013; @Couprie2013]、道路標識の分類では人間を超えるパフォーマンスを生み出した[@Ciresan2012]。
ディープネットワークのスケールち正確さが増加してきたのと同時に、それらが解けるタスクの複雑さも増加してきた。@Goodfellow2014dはニューラルネットワークが、単一の物体を識別するだけではなく、画像から複写した文字列全体を出力するように学習できることを示した。以前には、この種の学習はその列の個々の要素のラベリングを必要とすると広く理解された[@Gulcehre2013]。前に触れたLSTM sequence model のようなRecurrent neural networksは単なる固定入力ではなくシーケンスと他のシーケンスとの間の簡易をモデル化するのに使われる。このsequence-to-sequence学習はもうひとつのアプリケーション、すなわち、機械翻訳[@Sutskever2014; @Bahdanau2015]の進化の最前線にいるように見える。
複雑さが増加するこの傾向は、メモリセルから読み出したりメモリセルに任意の内容を書き込んだりするために学習を行う、ニューラルチューリングマシンの導出という論理的帰結に行き着いた[@Graves2014]。そのようなニューラルネットワークは望ましい振る舞いの例から単純なプログラムを学習できる。例えば、それらはソートされたりされなかったりするシーケンスの例が与えられると数字のリストをソートするように学習できる。この自己プログラミング技術は発展途上にあるが、将来的には、原則的にほとんどあらゆるタスクに適用できるだろう。
もうひとつの深層学習のこの上ない成果は強化学習[reinforcement learning]の領域への拡張である。強化学習の文脈において、自律エージェント[autonomous agent]は、人間の操作によるいかなるガイドもなしに、トライアンドエラーによってタスクを処理するように学習しなければならない。DeepMindは深層学習に基づく強化学習システムがAtariのビデオゲームのプレイを学習する能力があることを実証した。これは、多くのタスクで人間レベルのパフォーマンスに達している[@Mnih2015]。深層学習はroboticsに対する強化学習のパフォーマンスも大幅に改善した[@Finn2015]。
これらの深層学習のアプリケーションの多くはかなり有益である。今や深層学習は、Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflix、NVIDIA、NECを含めた、多くの技術系トップ企業で使われる。
深層学習の進歩はソフトウェアインフラの進歩に強く依存してもいる。Theano[@Bergstra2010; @Bastien2012]、PyLearn2[@Goodfellow2013c]、Torch[@Collobert2011b]、DistBelief[@Dean2012]、Caffe[@Jia2013]、MXNet[@Chen2015]、TensorFlow[@Abadi2015]のようなソフトウェアライブラリはすべて重要な研究プロジェクトや商用商品をサポートした。
深層学習は他の科学にも貢献をもたらした。物体認識のためのモダンな畳み込みネットワークは神経科学者が研究できる視覚処理のモデルをもたらす[@DiCarlo2013]。深層学習は膨大な量のデータを処理したり科学の分野で有用な予測を行うための有用なツールももたらす。これは、製薬会社が新薬を設計するのを助けたり[@Dahl2014]、亜原子粒子を研究したり[@Baldi2014]、人間の脳の三次元マップを作るのに使われる顕微鏡画像を自動的に解析[parse]したり[@KnowlesBarley2014]するために分子がどのように相互作用するであろうかを予測するのにうまく使われた。
まとめると、深層学習は、過去何十年かけて発展してきたので、人間の脳、統計学、応用数学の知識をかなり利用した機械学習のアプローチである。近年には、深層学習は、主に、より強力なコンピュータ、より大きなデータセット、より深いネットワークを訓練するための技術の結果として、その人気や有用性においてものすごい成長を遂げている。これからは更にもっと深層学習を改善したり新境地にもたらしたりするための課題や機会に満ちている。

# 線形代数

線形代数は科学や工学を通して広く用いられる数学の分野である。未だ線形代数は離散的な数学ではなく連続的な形式なので、多くの計算機科学者はそれの経験が乏しい。線形代数をきちんと理解することは多くの機械学習アルゴリズム、特に、深層学習アルゴリズムに対して理解したり扱ったりするために必須である。故に、我々は重要な線形代数の前提知識の集中的なプレゼンテーションから深層学習のイントロダクションを始める。
あなたがすでに線形代数に親しんでいるならば、この章をスキップしても一向に構わない。あなたがこれらの概念に関して以前に経験しているが、重要な式を復習するための詳細な参照表[reference sheet]を必要とするならば、我々はThe Matrix Cookbook[@Petersen2006]をオススメする。線形代数にまったく触れたことがないならば、この章は本書を読むのに十分なことを教えてくれるだろうが、我々は、[@Shilov1977]のような、線形代数を教えることのみに焦点を当てている他の資料も調べることを推奨している。この章は深層学習を理解するのに必須でない、多くの重要な線形代数の項目を完全に省略している。

## スカラ、ベクトル、行列、テンソル

線形代数の研究はいくつかの種類の数学的対象[mathematical objects]を伴う。

- スカラ：線形数学で研究される他のほとんどの対象が通常では複数の数値の列であるのに対して、スカラはただの単一の数値である。我々はスカラをイタリック体で記述する。我々は通常、スカラに小文字の変数名を与える。これらを導入するとき、それらが何の種類の数値であるかを指定する。例えば、実数のスカラを定義するのに「線の傾斜を$s \in \mathbb{R}$とする」と言えるし、自然数のスカラを定義するのに「単位の数を$n \in \mathbb{N}$とする」と言える。
- ベクトル：ベクトルは数値の列である。数値は順に配置される。我々はその順番の番号でそれぞれ個別の数値を識別できる。一般に、我々はベクトルに、$\boldsymbol{x}$のように、太字の小文字の名前を与える。ベクトルの要素は下付き文字を持つイタリック体でその名前を記述することで識別される。＄$\boldsymbol{x}$の1番目の要素は$x_1$であり、2番目の要素は$x_2$であり、3番目以降も同様である。我々はどんな種類の数値がベクトルに格納されるかを言う必要もある。各要素が$\mathbb{R}$にあり、ベクトルが$n$個の要素を持つならば、ベクトルは、$\mathbb{R}^n$と表記される、$\mathbb{R}$のデカルト積を$n$回行うことで形成される集合の中にある。ベクトルの要素を明示的に区別する必要があるとき、角括弧でトジされる列として記述する。

$$
\boldsymbol{x} = \left[ \begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array} \right]
$$

我々はベクトルが、異なる軸に沿った座標をもたらす各要素を持つ、空間上の点を識別するとみなすことができる。
時折、我々はベクトルの要素の集合を指し示す必要がある。この場合、我々はその番号を含む集合を定義し、下付き文字として集合を記述する。例えば、$x_1$、$x_3$、$x_6$にアクセスするためには、集合$S = {1, 3, 6}$を定義し、$\boldsymbol{x}_S$と記述する。我々は補集合を示すために$-$記号を用いる。例えば、$\boldsymbol{x}_{-1}$は$x_1$を除く$\boldsymbol{x}$のすべての要素を含むベクトルであり、$\boldsymbol{x}_{-S}$は$x_1$、$x_3$、$x_6$を除く$\boldsymbol{x}$のすべての要素を含むベクトルである。
- 行列：行列は数値の２次元配列である。つまり、各要素は1つではなく2つの番号で識別される。我々は通常、$\boldsymbol{A}$のように、行列にボールド体で大文字の変数名を与える。実数値の行列$\boldsymbol{A}$が高さ$h$と幅$n$を持つならば、$\boldsymbol{A} \in \mathbb{R}^{m \times n}$と言う。我々は通常、行列の要素をボールド体ではなくイタリックでその名前を用いて識別する。また、その番号はコンマで分けて並べられる。例えば、$A_{1,1}$は$\boldsymbol{A}$の左上の成分であり、$A_{m,n}$は右下の成分である。我々は水平座標として$:$を記述することで垂直座標$i$のすべての数値を識別できる。例えば、$\boldsymbol{A}_{i,:}$は垂直座標$i$と$\boldsymbol{A}$の水平の交差領域を示す。これは$\boldsymbol{A}$の$i$番目の行として知られる。同様に、$\boldsymbol{A}_{:,i}$は$\boldsymbol{A}$の$i$番目の列である。我々が行列の要素を明示的に認識する必要があるとき、各カッコでトジされる列として記述する。

$$
\left[ \begin{array}{cc}
A_{1,1} & A_{1,2} \\
A_{3,1} & A_{3,2}
\end{array} \right]
$$

時折、我々は単一の文字でない行列の値の表記を示す必要があるかもしれない。この場合、ひその表記の後に下付き文字を用いるが、いずれも小文字に変換しない。例えば、$f(\boldsymbol{A})_{i,j}$は関数$f$を$\boldsymbol{A}$に適用して計算した行列の要素$(i,j)$をもたらす。
- テンソル：いくつかの場合、我々は2つより多い軸を持つ配列を必要とするだろう。一般的な場合、軸の変数を持つ規則的な格子上に配置される数値の配列はテンソルとして知られる。我々は"A"という名前のテンソルを$\mathsf{A}$の書体で表記する。我々は$\mathit{\mathsf{A}}_{i,j,k}$と記述することで座標$(i,j,k)$における$\mathsf{A}$の要素を識別する。

行列での重要な操作のひとつは転置である。行列の転置は対角線を境にした行列の鏡像である。この対角線は主対角線と呼ばれ、左上のカドから始まり、右下へと向かう。この操作の図解は[@fig:2.1]を参照のこと。我々は行列$\boldsymbol{A}$の転置を$\boldsymbol{A}^\top$と示し、これは以下のように定義される。

$$
(\boldsymbol{A}^\top)_{i,j} = A_{j,i}
$$

ベクトルは1列のみを含む行列として考えることができる。故に、ベクトルの転置は1行のみを持つ行列である。時折、我々は1行の行列として文中にその要素を書き出すことで、例えば$\boldsymbol{x} = [x_1, x_2, x_3]$のように、ベクトルを定義する。

![行列の転置は主対角線を境にした鏡像としてみなすことができる。](fig/2-1.png){#fig:2.1}

スカラは単一の成分のみを持つ行列として考えることができる。これにより、スカラはそれ自身の転置$a = a^\top$であることがわかる。
我々は、それらが同じ形である場合に限り、単に対応する要素を足し合わせることで、行列を互いに足すことができる。すなわち、$\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{B}$は$C_{i,j} = A_{i,j} + B_{i,j}$である。
我々は、行列の各要素にその操作を行うことで、スカラを行列に足したり掛けたりできる。すなわち、$\boldsymbol{D} = a \cdot \boldsymbol{B} + c$は$D_{i,j} = a \cdot B_{i,j} + c$である。
深層学習の文脈では、我々はいくつかの従来とは異なる表記法も用いる。我々は行列とベクトルを足し合わせて別の行列を生み出すことを許可する。すまわち、$\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{b}$は$C_{i,j} = A_{i,j} + b_{j}$である。言い換えれば、ベクトル$\boldsymbol{b}$は行列の各行に足される。この省略表記は加算を行う前に各行にコピーされる$\boldsymbol{b}$で行列を定義する必要性を取り除く。この多くの場所への$\boldsymbol{b}$の暗黙的なコピーはbroadcastingと呼ばれる。

## 行列とベクトルの乗算

行列を伴う最も重要な操作のひとつは2つの行列の掛け算である。行列$\boldsymbol{A}$と$\boldsymbol{B}$の行列積は第三の行列$\boldsymbol{C}$である。この積が定義されるために、$\boldsymbol{A}$は$\boldsymbol{B}$が持つ行数と同じ列数を持たなければならない。$\boldsymbol{A}$が$m \times n$の形であり、$\boldsymbol{B}$が$n \times p$であれば、$\boldsymbol{C}$は$m \times p$の形である。我々は単に2つ以上の行列同士を配置することで行列積を記述できる。例えば以下のようになる。

$$
\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}
$$

この積の演算は以下のように定義される。

$$
C_{i,j} = \sum_k A_{i,k} B_{k,j}
$$

2つの行列の標準的な積は単に個々の要素の積を含む行列ではないことに注意したい。そのような操作は存在し、要素ごとの積[element-wise product]とかアダマール積[Hadamard product]と呼ばれ、$\boldsymbol{A}　\odot \boldsymbol{B}$と表記する。
同じ次元のベクトル$\boldsymbol{x}$と$\boldsymbol{y}$の内積は行列の積$\boldsymbol{x}^\top \boldsymbol{y}$である。我々は行列の積$\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}$を$\boldsymbol{A}$の$i$行目と$\boldsymbol{B}$の$j$列目の内積として$C_{i,j}$を計算するとみなすことができる。
行列の積の演算は行列の数学的な解析をより便利にする多くの有用な特性を持つ。例えば、行列の乗算は分配法則を満たす[distributive]。

$$
\boldsymbol{A} (\boldsymbol{B} + \boldsymbol{C}) = \boldsymbol{A} \boldsymbol{B} + \boldsymbol{A} \boldsymbol{C}
$$

また、結合法則も満たす[associative]。

$$
\boldsymbol{A} (\boldsymbol{B} \boldsymbol{C}) = (\boldsymbol{A} \boldsymbol{B}) \boldsymbol{C}
$$
{#eq:2.8}

行列の乗算は、スカラの乗算とは異なり、交換法則を満たさない[not commutative]（$\boldsymbol{A} \boldsymbol{B} = \boldsymbol{B} \boldsymbol{A}$は常に成り立つとは限らない）。しかし、2つのベクトルの内積は交換法則を満たす。

$$
\boldsymbol{x}^\top \boldsymbol{y} = \boldsymbol{y}^\top \boldsymbol{x}
$$

行列の積の転置は以下のような単純な形式を持つ。

$$
(\boldsymbol{A} \boldsymbol{B})^\top = \boldsymbol{B}^\top \boldsymbol{A}^\top
$$

これは、そのような積の値がスカラであるということ活用することで[@eq:2.8]を実証することができるようになる。故に、それ自身の転置と等価である。

$$
\boldsymbol{x}^\top \boldsymbol{y} = (\boldsymbol{x}^\top \boldsymbol{y})^\top = \boldsymbol{y}^\top \boldsymbol{x}
$$

本書の焦点は線形代数ではないので、我々はここで行列の積の有用な特性の包括的なリストを明らかにしようとはしないが、読者の皆は更にたくさん存在することに気付いているはずである。
今や我々は一次方程式の系を書き記すための線形代数の表記法を十分に知っている。

$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}
$$
{#eq:2.11}

ここで、$\boldsymbol{A} \in \mathbb{R}^{m \times n}$は既知の行列であり、$\boldsymbol{b} \in \mathbb{R}^m$は既知のベクトルであり、$\boldsymbol{x} \in \mathbb{R}^n$は解きたい未知の変数のベクトルである。$\boldsymbol{x}$の各要素$x_i$はこれらの未知の変数の1つである。$\boldsymbol{A}$の各行と$\boldsymbol{b}$の各要素は別の制約をもたらす。我々は[@eq:2.11]を以下のように書き直すことができる。

$$
\boldsymbol{A}_{1,:} \boldsymbol{x} = b_1
$$

$$
\boldsymbol{A}_{2,:} \boldsymbol{x} = b_2
$$

$$
\dots
$$

$$
\boldsymbol{A}_{m,:} \boldsymbol{x} = b_m
$$

また、より明示的にするならば、

$$
\boldsymbol{A}_{1,1} x_1 + \boldsymbol{A}_{1,2} x_2 + \cdots \boldsymbol{A}_{1,n} x_n = b_1
$$

$$
\boldsymbol{A}_{2,1} x_1 + \boldsymbol{A}_{2,2} x_2 + \cdots \boldsymbol{A}_{2,n} x_n = b_2
$$

$$
\dots
$$

$$
\boldsymbol{A}_{m,1} x_1 + \boldsymbol{A}_{m,2} x_2 + \cdots \boldsymbol{A}_{m,n} x_n = b_m
$$

行列対ベクトルの積の表記法はこの形式の式に対するよりコンパクトな表現をもたらす。

## 単位行列と逆行列

線形代数は$\boldsymbol{A}$の多くの値に対して[@eq:2.11]を解析的に解くことができるようになる逆行列と呼ばれる強力なツールをもたらす。
逆行列を説明するためには、まず、単位行列の概念を定義する必要がある。単位行列は、その行列をベクトルにかけたとき、いかなるベクトルも変化させない行列である。我々は$n$次元のベクトルを保持する単位行列を$\boldsymbol{I}_n$と表記する。正式には、$\boldsymbol{I}_n \in \mathbb{R}^{n \times n}$であり、

$$
\forall \boldsymbol{x} \in \boldsymbol{R}^n, \boldsymbol{I}_n \boldsymbol{x} = \boldsymbol{x}
$$

単位行列の構造は単純である。主対角線に沿ったすべての成分は1であり、その他のすべての成分は0である。例として[@fig:2.2]を参照のこと。

![単位行列の例。これは$\boldsymbol{I}_3$](fig/2-2.png){#fig:2.2}

$\boldsymbol{A}$の逆行列は$\boldsymbol{A}^{-1}$と表記し、以下のような行列として定義される。

$$
\boldsymbol{A}^{-1} \boldsymbol{A} = \boldsymbol{I}_n
$$

すると、以下の手順を用いて[@eq:2.11]を解くことができる。

$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}
$$

$$
\boldsymbol{A}^{-1} \boldsymbol{A} \boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}
$$

$$
\boldsymbol{I}_n \boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}
$$

$$
\boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}
$$

もちろん、この処理は$\boldsymbol{A}^{-1}$を見つけられるということに依存する。我々は以下の章で$\boldsymbol{A}^{-1}$が存在する条件を検討する。
$\boldsymbol{A}^{-1}$が存在するとき、いくつかの異なるアルゴリズムは閉形式においてこれを見つけることができる。理論上、同じ逆行列は$\boldsymbol{b}$の様々な値に対して何度もその式を解くことに使える。$\boldsymbol{A}^{-1}$は主に理論的なツールとして役に立つが、実際には多くのソフトウェアアプリケーションに対して実践で使わるべきではない。$\boldsymbol{A}^{-1}$はディジタルコンピュータ上では限定的な精度でのみ表現される可能性があるので、$\boldsymbol{b}$の値を使うアルゴリズムは通常、$\boldsymbol{x}$のより正確な推定値を得ることができる。

## 線形従属と線形包

存在する$\boldsymbol{A}^{-1}$に対して、[@eq:2.11]は$\boldsymbol{b}$の値ごとにたった1つの解のみをもたなければならない。式の系がある$\boldsymbol{b}$の値に対する解を持たなかったり無限に多くの解を持ったりする可能性もある。しかし、特定の$\boldsymbol{b}$に対して無限より少なく1つより多い解を取りえない。つまり、$\boldsymbol{x}$と$\boldsymbol{y}$の両方が解でありならば、以下はいかなる実数$\alpha$において解でもある。

$$
\boldsymbol{z} = \alpha \boldsymbol{x} + (1 - \alpha) \boldsymbol{y}
$$

その式がいくつの解を持つかを解析するため、$\boldsymbol{A}$の列を原点（すべて0のベクトルで指定される点）から向かうことができる様々な方向を指定すると考え、そして、$\boldsymbol{b}$に至る道がいくつあるかを決定する。この視点では、$\boldsymbol{x}$の各要素はこれらの方向のそれぞれにどれだけ距離だけ向かうべきであるかを指定する。$i$列目の方向にどれだけの距離を移動するかを指定する$x_i$として、

$$
\boldsymbol{A} \boldsymbol{x} = \sum_i x_i \boldsymbol{A}_{:,i}
$$

一般に、この種の操作は線形結合[linear combination]と呼ばれる。正式には、あるベクトルの集合${\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}}$の線形結合は対応するスカラの係数を各ベクトル$v^{(i)}$にかけ、その結果を足し合わせることで求められる。

$$
\sum_i c_i \boldsymbol{v}^{(i)}
$$

ベクトルの集合の張る空間[span]は元のベクトルの線形結合によって得られるすべての点の集合である。故に、$\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}$が解を持つかどうかを決定することは$\boldsymbol{b}$が$\boldsymbol{A}$の列の張る空間の中にあるかどうかを確かめることに等しい。この特定の張る空間は$\boldsymbol{A}$の列空間とか値域[range]として知られる。
したがって、系$\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}$が$\boldsymbol{b} \in \mathbb{R}^m$のすべての値に対して解を持つために、我々はその列空間が$\mathbb{R}^m$全体であることを必要とする。$\mathbb{R}^m$におけるいずれかの点がその列空間から外れている場合、その点は解を持たない$\boldsymbol{b}$の潜在的な値である。$\boldsymbol{A}$の列空間が$\mathbb{R}^m$全体であるという必要条件は$\boldsymbol{A}$が少なくとも$m$個の列を持つことを即座に暗示する。このとき、$n \ge m$である。そうでなければ、列空間の次元は$m$より小さくなるだろう。例えば、3x2行列を考えよう。対象の$\boldsymbol{b}$は3次元であるが、$\boldsymbol{x}$は2次元である。なので、最善の状態で$\boldsymbol{x}$の値を修正することは2次元平面を$\mathbb{R}^3$の内に描き出すことを可能にする。この式は$\boldsymbol{b}$がその平面上にある場合に限り解を持つ。
$n \ge m$は解を持つためのすべての点に対するただ一つの必要条件である。これは、列のいくつかが冗長である可能性があるので、十分条件ではない。列の両方が同一である2x2行列を考えよう。これは、複製された列の1つ分のみを含むので、2x1行列と同じ列空間を持つ。言い換えれば、その列空間は依然として単なる線であり、2個の列があったとしても、$\mathbb{R}^2$全体を取り囲むことができない。
正式には、この種の冗長性は線形従属[linear dependence]として知られる。その集合のベクトルが他のベクトルの線形結合でないならば、ベクトルの集合は線形独立[linear independent]である。その集合における他のベクトルの線形結合である集合にベクトルを追加するならば、その新しいベクトルはその集合の張る空間にいかなる点も追加しない。これは行列の列空間が$\mathbb{R}^m$全体を取り囲むことを意味し、その行列は$m$個の線形独立の列の少なくとも1つの集合を含まなければならない。この条件は[@eq:2.11]が$\boldsymbol{b}$のすべての値に対して解を持つための必要十分条件である。その必要条件は集合が、少なくとも$m$個ではなく、厳密に$m$個の線形独立な列を持つことに注意すること。$m$次元のベクトルの集合は$m$個より多い相互に線形独立な列を持ち得ないが、$m$個より多い列を持つ行列はそのような集合を1より多く持ち得る。
行列が逆行列を持つためには、追加で[@eq:2.11]が$\boldsymbol{b}$の各値に対して多くとも1つの解を持つことを保証する必要がある。そうするために、その行列が多くとも$m$個の列を持つことを確かめる必要がある。そうでなければ、各解をパラメータ化する1つより多い方法が存在する。
両方とも、これは行列が正方[square]でなければならないことを意味する。つまり、$m = n$、かつ、すべての列が線形独立である。線形従属な列を持つ正方行列は特異[singular]として知られる。
$\boldsymbol{A}$が正則でない、または、正則だが特異であるならば、その式を解くことは依然として可能であるが、我々は解を求めるために行列の逆を求める手法を用いることが出来ない。
ここまで、我々は左から掛けられるものとして逆行列を述べてきた。右からかけられる逆行列を定義することも可能である。

$$
\boldsymbol{A} \boldsymbol{A}^{-1} = \boldsymbol{I}
$$

正則行列では、左の逆と右の逆は等しい。

## ノルム

時折、我々はベクトルの大きさを測る必要がある。機械学習において、我々は一般にノルムと呼ばれる関数を用いてベクトルの大きさを測る。正式には、$L^p$ノルムは$p \in \mathbb{R}, p \ge 1$に対して以下で求まる。


$$
\|\boldsymbol{x}\|_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$
{#eq:2.30}

$L^p$ノルムを含むノルムはベクトルを非負の値にマッピングする関数である。直観レベルでは、ベクトル$\boldsymbol{x}$のノルムは原点から点$\boldsymbol{x}$までの距離を測る。より厳密に言えば、ノルムは以下の性質を満たす任意の関数$f$である。

- $f(\boldsymbol{x}) = 0 \Leftarrow \boldsymbol{x} = \boldsymbol{0}$
- $f(\boldsymbol{x} + \boldsymbol{y}) \le f(\boldsymbol{x}) + f(\boldsymbol{y})$（三角不等式）
- $\forall \alpha \in \mathbb{R}, f(\alpha \boldsymbol{x}) = |\alpha| f(\boldsymbol{x})$

$p = 2$を持つ$L^2$ノルムは、単に原点から$\boldsymbol{x}$で示される点までのユークリッド距離である、ユークリッドノルム[Euclidean norm]として知られる。$L^2$ノルムは、下付きの2が省かれて、単に$\|\boldsymbol{x}\|$としてしばしば表記され、機械学習においてかなり頻繁に使われる。$\boldsymbol{x}^\top \boldsymbol{x}$として簡単に計算できる$L^2$ノルムの二乗を使ってベクトルの大きさを測ることも一般的である。
$L^2$ノルムの二乗は$L^2$ノルムそれ自体よりも数学的にも計算的にも更に便利に機能する。例えば、$\boldsymbol{x}$の各要素に関する$L^2$ノルムの二乗のそれぞれの微分は$\boldsymbol{x}$の関連する要素にのみ依存するが、$L^2$ノルムのすべての微分はベクトル全体に依存する。多くの状況で、$L^2$ノルムの二乗は、原点近くで非常にゆっくりと増加するので、望ましくないかもしれない。いくつかの機械学習アプリケーションでは、ピッタリ0である要素と0でない小さい要素との間を区別することが重要である。これらの場合、我々は、数学的な単純さを維持するがすべての位置で同じ割合で成長するような関数に切り替える。すなわち、$L^1$ノルムである。$L^1$ノルムは以下のように単純化され得る。

$$
\|\boldsymbol{x}\|_1 = \sum_i |x_i|
$$

$L^1$ノルムはゼロと非ゼロの違いが非常に重要であるときに機械学習で一般的に使われる。毎回、$\boldsymbol{x}$の要素は$\epsilon$だけ0から離れる。つまり、$L^1$ノルムは$\epsilon$だけ増加する。
我々は時折に非ゼロの要素数を数えることでベクトルの大きさを測る。ある著者はこの関数を「$L^0$ノルム」としているが、これは不正確な用語である。ベクトルにおける非ゼロの成分の個数は、$\alpha$でベクトルをスケーリングしても非ゼロの成分の個数が変化しないので、ノルムではない。$L^1$ノルムは非ゼロの成分の個数の代わりとしてしばしば用いられる。
機械学習で一般的に現れる他のノルムのひとつとして$L^\infty$があり、最大ノルム[max norm]としても知られる。このノルムはベクトルにおいて最大の大きさを持つ要素の絶対値に単純化する。

$$
\|\boldsymbol{x}\|_\infty = \max_i |x_i|
$$

時折、我々は行列の大きさを測ることも望むかもしれない。深層学習の文脈において、これを行う最も一般的な方法は、ベクトルの$L^2$と似ている、otherwise obscure Frobenius normを用いることである。

$$
\|A\|_F = \sqrt{\sum_{i,j} A^2_{i,j}}
$$

2つのベクトルの内積はノルムに関して書き直すことができる。具体的には、

$$
\boldsymbol{x}^\top \boldsymbol{y} = \|\boldsymbol{x}\|_2 \|\boldsymbol{y}\|_2 \cos \theta
$$

ここで、$\theta$は$\boldsymbol{x}$と$\boldsymbol{y}$のなす角である。

## 特別な種類の行列およびベクトル

いくつかの特別な種類の行列やベクトルは特に有用である。
対角行列[diagonal matrix]はほとんどが0で構成され、主対角線に沿ってのみ非ゼロの成分を持つ。正式には、行列$\boldsymbol{D}$は$i \ne j$のすべてに対して$D_{i,j} = 0$である場合に限り対角である。我々は対角行列の例のひとつをすでに見てきている。それは、対角成分がすべて1である単位行列である。我々は、その対角成分がベクトル$\boldsymbol{v}$の成分によって与えられる正方対角行列を示すために$\text{diag}(\boldsymbol{v})$と記述する。対角行列は、対角行列との乗算が効率よく計算できるので、いくらか興味深い。$\text{diag}(\boldsymbol{v}) \boldsymbol{x}$をけいさんするためには、各要素$x_i$を$v_i$でスケールするだけでよい。言い換えれば、$\text{diag}(\boldsymbol{v}) \boldsymbol{x} = \boldsymbol{v} \odot \boldsymbol{x}$である。正方対角行列の逆もまた効率的である。その逆行列は対角成分が非ゼロである場合にのみ存在し、その場合、$\text{diag}(\boldsymbol{v})^{-1} = \text{diag}([1/v_1, \dots, 1/v_n]^\top)$である。多くの場合、我々は任意の行列に関して汎用的な機械学習アルゴリズムを導くかもしれないが、いくつかの行列を対角であると制限することでより安価な（かつ、説明の少ない[less descriptive]）アルゴリズムを得るかもしれない。
すべての対角行列が正方である必要はない。矩形[rectangular]の対角行列を構築することが可能である。正方でない対角行列は逆を持たないが、依然として安価に乗算することができる。正方でない対角行列$\boldsymbol{D}$に対して、積$\boldsymbol{D} \boldsymbol{x}$は$\boldsymbol{x}$の各要素をスケーリングすること、$\boldsymbol{D}$が縦長の場合はその結果にいくつかの0を連結するか、$\boldsymbol{D}$が横長の場合はベクトルの最後の要素のいくつかを破棄するか、のいずれかを行うことを伴うだろう。
対称行列は自身の転置と等しい行列である。

$$
\boldsymbol{A} = \boldsymbol{A}^\top
$$

対称行列[symmetric matrix]は、その成分が引数の順番に依存しない2つの引数の関数によって生成されるとき、しばしば発生する。例えば、$\boldsymbol{A}$が点$i$から点$j$までの距離を求める$\boldsymbol{A}_{i,j}$と持つ距離測定の行列であるならば、距離関数は対称であるので、$\boldsymbol{A}_{i,j} = \boldsymbol{A}_{j,i}$である。
単位ベクトル[unit vector]は単位ノルム[unit norm]を持つベクトルである。

$$
\|\boldsymbol{x}\|_2 = 1
$$

ベクトル$\boldsymbol{x}$とベクトル$\boldsymbol{y}$は、$\boldsymbol{x}^\top \boldsymbol{y} = 0$であるならば、互いに直交[orthogonal]である。両方のベクトルが非ゼロのノルムを持つならば、これは互いに90度の角度にあることを意味する。$\mathbb{R}^n$においｙｒ，多くとも$n$個のベクトルは非ゼロのノルムと互いに直交であるかもしれない。そのベクトルが直交であるだけでなく単位ベクトルも持つならば、我々はそれを正規直交[orthonormal]と呼ぶ。
直交行列[orthogonal matrix]は、その行が互いに正規直交であり、その列が互いに正規直交であるような正方行列である。

$$
\boldsymbol{A}^\top \boldsymbol{A} = \boldsymbol{A} \boldsymbol{A}^\top = \boldsymbol{I}
$$

これは暗に以下であることを示す。

$$
\boldsymbol{A}^{-1} = \boldsymbol{A}^\top
$$

つまり、その逆を計算するのが非常に安価であるので、直交行列は興味深い。直交行列の定義には細心の注意を払ってほしい。直観に反して、これらの行は直交であるだけではなく、まったくもって正規直交である。その行や列が直交であるが正規直交ではない行列に特別な用語は当てられていない。

## 固有値分解 {#sec:2.7}

多くの数学的対象は、それらを構成要素に分けること、または、そららに普遍的な特性を見つけること、によってより良く理解できるのであって、それらを表現しようと選んだ方法によってもたらされるわけではない。
例えば、整数は素因数に分解できる。数$12$を表現する方法は10進数と2進数のどちらで記述するかに依存して変化するだろうが、$12 = 2 \times 2 \times 3$は常に真であるだろう。この表現により、我々は、例えば、$12$は$5$で割れないとか、12の倍数は3で割れるといった、有用な特性を結論付けることができる。
我々が整数の真の性質に関することを素因数に分解することで発見できるのと同じように、我々は、要素の配列としての行列の表現では明らかでない機能特性に関する情報を示す方法で行列を分解することもできる。
最も広く使われる種類の行列分解のひとつは固有値分解[eigendecomposition]と呼ばれる。これは、行列を固有ベクトルと固有値の集合に分解する。
正方行列$\boldsymbol{A}$の固有ベクトル[eigenvector]は、$\boldsymbol{A}$による乗算が$\boldsymbol{v}$のスケーリングのみに置き換えるような非ゼロのベクトル$\boldsymbol{v}$である。

$$
\boldsymbol{A} \boldsymbol{v} = \lambda \boldsymbol{v}
$$

スカラ$\lambda$はこの固有ベクトルに対応する固有値[eigenvalue]として知られる（$\boldsymbol{v}^\top \boldsymbol{A} = \lambda \boldsymbol{v}^\top$のような左固有ベクトルを求めることもできるが、我々は通常、右固有ベクトルについて考える）。
$\boldsymbol{v}$が$\boldsymbol{A}$の固有ベクトルであるならば、$s \in \mathbb{R}, s \ne 0$に対する任意の再スケーリングされたベクトル$s \boldsymbol{v}$である。更に、$s \boldsymbol{v}$は依然として同じ固有値を持つ。この理由のため、我々は通常、単位固有ベクトルのみを探す。
行列$\boldsymbol{A}$が固有値${\lambda_1, \dots, \lambda_n}$に対応する$n$個の線形独立な固有ベクトル${\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}}$を持つとする。我々は列あたり1つの行列ベクトルを持つ$\boldsymbol{V}$を形作るためにすべての固有ベクトルを結合しても良い。つまり、$\boldsymbol{V} = [\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}]$である。同様に、我々はベクトル$\boldsymbol{\lambda} = [\lambda_1, \dots, \lambda_n]^\top$を形成するために固有値を結合できる。$\boldsymbol{A}$の固有値分解は以下で求められる。

$$
\boldsymbol{A} = \boldsymbol{V} \text{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}
$$

我々は特定の固有値と固有ベクトルを持つ行列を*作る*ことが望ましい方向に空間を引き伸ばすことを可能にすることを見たことがある。それでも、我々はしばしば行列を固有値と固有ベクトルに分解したい。そうすることは、整数を素因数に分解することが整数の振る舞いを理解するのに役立つ可能性があることと同様に、行列のある特性を解析するのに役立つ可能性がある。
すべての行列が固有値と固有ベクトルに分解できるわけではない。ある場合では、分解は存在するが、実数ではなく複素数を伴う。幸いにも、本書では、通常、単純な分解を持つ特定の種類の行列のみを分解する必要がある。具体的には、すべてが実対称行列は実数値の固有ベクトルと固有値のみを用いた式に分解することができる。

$$
\boldsymbol{A} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top
$$

ここで、$\boldsymbol{Q}$は行列$\boldsymbol{A}$の固有値から成る直交行列であり、$\boldsymbol{\Lambda}$は対角行列である。固有値$\Lambda_{i,i}$は、$\boldsymbol{Q}_{:,i}$と表記される$\boldsymbol{Q}$の列$i$における固有ベクトルと関連している。$\boldsymbol{Q}$は直交行列であるので、我々は$\boldsymbol{A}$を方向$\boldsymbol{v}^{(i)}$に$\lambda_i$だけ空間をスケーリングするとみなすことができる。例として[@fig:2.3]を参照のこと。

![固有ベクトルと固有値の影響の例。ここに、固有値$\lambda_1$を持つ$\boldsymbol{v}^{(1)}$と$\lambda_2$を持つ$\boldsymbol{v}^{(2)}$という2つの正規直交な固有ベクトルを持つ行列$\boldsymbol{A}$がある。（左）すべての単位ベクトル$\boldsymbol{u} \in \mathbb{R}^2$の集合を単位円としてプロットした。（右）すべての点$\boldsymbol{A} \boldsymbol{u}$の集合をプロットした。$\boldsymbol{A}$の単位円の歪ませ方を観察することで、方向$\boldsymbol{v}^{(i)}$に$\lambda_i$だけ空間をスケールすることを確認できる。](fig/2-3.png){#fig:2.3}

いかなる実対称行列$\boldsymbol{A}$も固有値分解を持つことが保証されているが、その固有値分解はユニークではないかもしれない。いずれかの2つ以上の固有ベクトルが同じ固有値を共有するならば、その張る空間にある任意の直交ベクトルの集合もその固有値を持つ固有ベクトルであり、代わりに、これらの固有ベクトルを用いて$\boldsymbol{Q}$を同等に選択することができるだろう。慣例により、我々は通常、降順に$\boldsymbol{A}$の成分を並べ替える。この慣例のもとでは、固有値分解は、すべての固有値がユニークである場合に限り、ユニークである。
行列の固有値分解はその行列に関する多くの有用な事実を教えてくれる。その行列は、固有値のいずれかがゼロである場合に限り、特異である。実対称行列の固有値分解は、$\|\boldsymbol{x}\|_2 = 1$に従う$f(\boldsymbol{x}) = \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x}$の形の二次方程式を最適化するのにも使うことができる。$\boldsymbol{x}$が$\boldsymbol{A}$の固有ベクトルに等しいならば、$f$は対応する固有値の値を取る。制約領域内での$f$の最大値は最大の固有値であり、制約領域内でのその最小値は最小の固有値である。
固有値がすべて正である行列は正定値[positive definite]と呼ばれる。固有値がすべて正かゼロである行列は半正定値[positive semidefinite]と呼ばれる。同様に、すべての固有値が負であれば、それは負定値[negative definite]であり、すべての固有値が負かゼロであれば、それは半負定値[negative semidefinite]である。半正定値行列は、$\forall \boldsymbol{x}, \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} \ge 0$であることがほしょうされるので、興味深い。正定値行列は加えて$\boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} = 0 \Rightarrow \boldsymbol{x} = \boldsymbol{0}$であることが保証される。

## 特異値分解

[@sec:2.7]では、行列を固有ベクトルと固有値に分解する方法を確認した。特異値分解[singular value decomposition; SVD]は行列を特異ベクトルと特異値に分解するもうひとつの方法をもたらす。SVDは固有値分解が明らかにするようないくつかの同種の情報を見つけることができる。しかし、SVDはより一般に適用できる。すべての実行列は特異値分解を持つが、固有値分解でも同様に真であるとは限らない。例えば、行列が正方でないならば、固有値分解は定義されず、代わりに、特異値分解を使わなければならない。
固有値分解が、以下のように$\boldsymbol{A}$を書き直せるような、固有ベクトルの行列$\boldsymbol{V}$と固有値のベクトル$\boldsymbol{\lambda}$を見つけるために行列$\boldsymbol{A}$を解析することを伴うことを思い出してほしい。

$$
\boldsymbol{A} = \boldsymbol{V} \text{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}
$$

特異値分解は、今度は3つの行列の積として$\boldsymbol{A}$を記述しようとすることを除いて、似ている。

$$
\boldsymbol{A} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{V}^\top
$$

$\boldsymbol{A}$が$m \times n$行列であるとする。すると、$\boldsymbol{U}$は$m \times m$行列、$\boldsymbol{D}$は$m \times n$行列、$\boldsymbol{V}$は$n \times n$行列として定義される。
これらの行列のそれぞれは特別な構造を持つように定義される。行列$\boldsymbol{U}$および$\boldsymbol{V}$は両方とも直交行列として定義される。行列$\boldsymbol{D}$は対角行列として定義される。$\boldsymbol{D}$は正方である必要はないことに注意する。
$\boldsymbol{D}$の対角に沿った要素は行列$\boldsymbol{A}$の特異値[singular values]として知られる。$\boldsymbol{U}$の列は左特異ベクトル[left-singular vectors]として知られる。$\boldsymbol{V}$の列は右特異ベクトル[right-singular vectors]として知られる。
我々は$\boldsymbol{A}$の関数の固有値分解に関して$\boldsymbol{A}$の特異値分解を読み替えることができる。$\boldsymbol{A}$の左特異ベクトルは$\boldsymbol{A} \boldsymbol{A}^\top$の固有ベクトルである。$\boldsymbol{A}$の右特異ベクトルは$\boldsymbol{A}^\top \boldsymbol{A}$の固有ベクトルである。$\boldsymbol{A}$の非ゼロの特異値は$\boldsymbol{A}^\top \boldsymbol{A}$の固有値の平方根である。$\boldsymbol{A} \boldsymbol{A}^\top$についても同様に真である。
おそらく、SVDの最も有用な特徴は、次の章で見ていくが、正方でない行列への行列の逆を部分的に一般化するのに使えることである。

## Moore-Penroseの擬似逆行列

逆行列は正方出ない行列に対して定義されない。以下の一次方程式を解くことができるような、行列$\boldsymbol{A}$の左逆行列$\boldsymbol{B}$を作りたいとする。

$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{y}
$$

両辺に左から掛けることで、以下を得る。

$$
\boldsymbol{x} = \boldsymbol{B} \boldsymbol{y}
$$

問題の構造に依存して、$\boldsymbol{A}$から$\boldsymbol{B}$へのユニークなマッピングを設計できないかもしれない。
$\boldsymbol{A}$が縦長の場合、この式が解を持たない可能性がある。$\boldsymbol{A}$が横長の場合、取り得る複数の解が存在する。
Moore-Penroseの擬似逆行列はこれらの場合にいくらかの進展をもたらすことを可能とする。$\boldsymbol{A}$の擬似逆行列は以下の
行列として定義される。

$$
\boldsymbol{A}^+ = \lim_{\alpha \searrow 0} (\boldsymbol{A}^\top \boldsymbol{A} + \alpha \boldsymbol{I})^{-1} \boldsymbol{A}^\top
$$

擬似逆行列を計算するための実践的なアルゴリズムはこの定義ではなく以下の式に基づく。

$$
\boldsymbol{A}^+ = \boldsymbol{V} \boldsymbol{D}^+ \boldsymbol{U}^\top
$$

ここで、$\boldsymbol{U}$、$\boldsymbol{D}$、$\boldsymbol{V}$は$\boldsymbol{A}$の特異値分解であり、対角行列$\boldsymbol{D}$の擬似逆行列$\boldsymbol{D}^+$は非ゼロの要素の逆数を取り、その結果の行列の転置を取ることで得られる。
$\boldsymbol{A}$が行より多い列を持つとき、擬似逆行列を用いて一次方程式を解くことは多くの取り得る解のひとつをもたらす。具体的には、すべての取り得る解の中で最小ユークリッドノルム$\|\boldsymbol{x}\|_2$を持つ解$\boldsymbol{x} = \boldsymbol{A}^+ \boldsymbol{y}$をもたらす。
$\boldsymbol{A}$が列より多い行を持つとき、解を持たない可能性がある。この場合、擬似逆行列を用いると、$\boldsymbol{A} \boldsymbol{x}$がユークリッドノルム$\|\boldsymbol{A} \boldsymbol{x} - \boldsymbol{y}\|_2$に関する$\boldsymbol{y}$にできるだけ近いときの$\boldsymbol{x}$をもたらす。

## トレース演算子

トレース演算子は行列のすべての対角成分の総和を求める。

$$
\text{Tr}(\boldsymbol{A}) = \sum_i \boldsymbol{A}_{i,i}
$$

トレース演算子は様々な理由により有用である。総和の表記法に頼らずに記述することが困難ないくつかの操作は行列の積とトレース演算子を用いて記述ことができる。例えば、トレース演算子は行列のFrobeniusノルムを書き表す代替方法をもたらす。

$$
\|A\|_F = \sqrt{\text{Tr}(\boldsymbol{A} \boldsymbol{A}^\top)}
$$

トレース演算子の観点から式を書き表すと、多くの有用な恒等式[identities]を用いて式を操作する機会が開ける。例えば、トレース演算子は転置に対して不変である。

$$
\text{Tr}(\boldsymbol{A}) = \text(\boldsymbol{A}^\top)
$$

対応する行列の形状が結果の積を定義させることを可能にするならば、多くのファクタから構成される正方行列のトレースは最初の位置に最後のファクタを移動しても不変である。

$$
\text{Tr}(\boldsymbol{A} \boldsymbol{B} \boldsymbol{C}) = \text{Tr}(\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}) = \text{Tr}(\boldsymbol{B} \boldsymbol{C} \boldsymbol{A})
$$

または、より一般的に、

$$
\text{Tr} \left( \prod_{i=1}^n F^{(i)} \right) = \text{Tr} \left( F^{(n)} \prod_{i=1}^{n-1} F^{(i)} \right)
$$

この循環置換に対する不変性は結果の積が異なる形状を持っていても成り立つ。例えば、$\boldsymbol{A} \in \mathbb{R}^{m \times n}$と$\boldsymbol{B} \in \mathbb{R}^{n \times m}$に対して、$\boldsymbol{A}\boldsymbol{B} \in \mathbb{R}^{m \times m}$であり$\boldsymbol{B}\boldsymbol{A} \in \mathbb{R}^{n \times n}$であるとしても、いかが成り立つ。

$$
\text{Tr}(\boldsymbol{A}\boldsymbol{B}) = \text{Tr}(\boldsymbol{B}\boldsymbol{A})
$$

もうひとつの覚えておきたい有用な事実として、スカラはそれ自身のトレースであるということである。すなわち、$a = \text{Tr}(a)$である。

## 行列式

$\det(\boldsymbol{A})$と表記される正方行列の行列式は行列を実スカラにマッピングする関数である。行列式はその行列のすべての固有値の積に等しい。行列式の絶対値は行列による乗算が空間をどれだけ拡大または縮小するかの尺度とみなすことができる。行列式が0であれば、そのボリュームのすべてを失わせるように、空間は少なくとも1次元に完全に縮小される。行列式が1であれば、その変換はボリュームを維持する。

## 例：主成分分析

単純な機械学習アルゴリズムのひとつである、主成分分析[principal components analysis; PCA]は基本的な線形代数の知識のみを用いて導出できる。
$\mathbb{R}^n$にある$m$個の点の集合${\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)}}$があるとして、これらの点に非可逆圧縮を適用したい。非可逆圧縮は、メモリをそれほど必要としないがいくらかの精度が失われるかもしれない方法でその点を格納することを意味する。我々はできるだけ失う精度を少なくしたい。
これらの点をエンコードする方法のひとつは、こららの低次元バージョンを表現することである。各点$\boldsymbol{x}^{(i)} \in \mathbb{R}^n$に対して、対応するコードベクトル$\boldsymbol{c}^{(i)} \in \mathbb{R}^l$を求める。$l$が$n$より小さければ、コードの点を格納することは元データを格納するよりも使うメモリが少なくなるだろう。我々は入力に対するコードを生み出すいくつかのエンコード関数$f(\boldsymbol{x}) = \boldsymbol{c}$とそのコードを与えると再構築された入力を生み出すデコード関数$\boldsymbol{x} \approx g(f(\boldsymbol{x}))$を見つけたい。
PCAはデコード関数の選択として定義される。具体的には、デコーダを非常に単純にするために、我々はコードを$\mathbb{R}^n$にマッピングし直そうと行列の乗算を用いることにする。$g(\boldsymbol{c}) = \boldsymbol{D} \boldsymbol{c}$とする。ここで、$\boldsymbol{D} \in \mathbb{R}^{n \times l}$はデコードを定義する行列である。
このデコーダに対する最適なコードを計算することは難しい問題となり得るかもしれない。エンコードの問題を簡単に保つため、PCAは$\boldsymbol{D}$の列が互いに直交することを強いる。（$l = n$でない限り、$\boldsymbol{D}$は厳密に言えば「直交行列」ではないことに注意する。）
これまでに述べたような問題は、すべての点に対して比例して$c_i$を減らす場合、$\boldsymbol{D}_{:,i}$のスケールを増やすことができるので、多くの解を取り得る。その問題に単独の解を与えるため、$\boldsymbol{D}$のすべての列を単位ノルムを持つように制限する。
この基本アイデアを我々が実装できるアルゴリズムへ変えるため、我々がすべき最初のことは入力の点$\boldsymbol{x}$ごとに最適なコード点$\boldsymbol{c}^*$を生成する方法を理解することである。これを行う方法のひとつは入力の点$\boldsymbol{x}$とその再構築されたもの$g(\boldsymbol{c}^*)$の距離を最小化することである。我々はこの距離をノルムを用いて測ることができる。主成分アルゴリズムにおいて、我々は$L^2$ノルムを用いる。

$$
\boldsymbol{c}^* \argmin_{\boldsymbol{c}} \|\boldsymbol{x} - g(\boldsymbol{c})\|_2
$$

我々は、その両方が同じ$\boldsymbol{c}$の値に最小化されうので、$L^2$ノルムそれ自体を用いる代わりに$L^2$ノルムの二乗に切り替えることができる。両方は、$L^2$ノルムが非負であり、二乗の演算が非負の変数に対して単調増加するので、同じ$\boldsymbol{c}$の値によって最小化される。

$$
\boldsymbol{c}^* \argmin_{\boldsymbol{c}} \|\boldsymbol{x} - g(\boldsymbol{c})\|^2_2
$$

最小化する関数は以下のように単純化する。

$$
(\boldsymbol{x} - g(\boldsymbol{c}))^\top (\boldsymbol{x} - g(\boldsymbol{c}))
$$

（[@eq:2.30]の$L^2$ノルムの定義より）

$$
= \boldsymbol{x}^\top \boldsymbol{x} - \boldsymbol{x}^\top g(\boldsymbol{c}) - g(\boldsymbol{c})^\top \boldsymbol{x} + g(\boldsymbol{c})^\top g(\boldsymbol{c})
$$

（分配法則により、$g(\boldsymbol{c})^\top \boldsymbol{x}$がそれ自身の転置と等しいので）

$$
= \boldsymbol{x}^\top \boldsymbol{x} - 2\boldsymbol{x}^\top g(\boldsymbol{c}) + g(\boldsymbol{c})^\top g(\boldsymbol{c})
$$

我々は、$\boldsymbol{c}$に依存しない第一の項を省略するように、再び最小化される関数を変化させることができる。

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top g(\boldsymbol{c}) + g(\boldsymbol{c})^\top g(\boldsymbol{c})
$$

さらに歩みを進めるため、$g(\boldsymbol{c})$の定義で置き換えなければならない。

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{D}^\top  \boldsymbol{D} \boldsymbol{c}
$$

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{I}_l \boldsymbol{c}
$$

（$\boldsymbol{D}$の直交性と単位ノルム制約により）

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{c}
$$

我々はこの最適化問題をベクトル解析[vector calculus]を用いて解くことができる（どうやってこれを行うかを知らないならば[@sec:4.3]を参照）。

$$
$$

## 4.3 {#sec:4.3}

## 9.10 {#sec:9.10}

## 10.7 {#sec:10.7}

### 12.1.2 {#sec:12.1.2}

## 12.3 {#sec:12.3}

# 15 {#sec:15}

## 15.1 {#sec:15.1}
