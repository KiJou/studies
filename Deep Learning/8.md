# 深層モデルの訓練に対する最適化

深層学習アルゴリズムは多くの状況で最適化を伴う。例えば、PCAのようなモデルで推論を行うことは最適化問題を解くことを伴う。我々はしばしば、証明を記述したりアルゴリズムを設計したりするために解析的な最適化を用いる。深層学習に組み込まれる多くの最適化問題の中で、最も困難なものはニューラルネットワークの訓練である。ニューラルネットワークの訓練問題のたった1つのインスタンスを解くために何百のマシンで数日から数ヶ月を投資することは極めて一般的である。この問題はそれほどに重要であり、それほどに高価であるので、最適化技術の特殊化された集合はそれを最適化するために開発された。本性はニューラルネットワークの訓練に対するこれらの最適化技術を提示する。
あなたが勾配ベースの最適化の基本原則に親しみを持たないならば、[@sec:4]を復習することを提案する。この章は一般における数値的最適化の簡潔な概要を含む。
本章は、コスト関数$J(\boldsymbol{\theta})$を大幅に減少させるニューラルネットワークのパラメータ$\boldsymbol{\theta}$を求める、という最適化の1つの特定のケースに焦点を当ている。これは、追加の正則化項と同様に訓練セット全体で評価されるパフォーマンス測度を一般に含む。
我々は機械学習タスクに対する訓練アルゴリズムとして使われる最適化が純粋な最適化と異なる具合の説明から始める。次に、ニューラルネットワークの最適化を難しくする具体的な課題にいくつかを提示する。そして、最適化アルゴリズム自体とパラメータを初期化するための戦略の両方を含めた、いくつかの実践的なアルゴリズムを定義する。より発展的なアルゴリズムは訓練中に学習率に適応したり、コスト関数の二階微分に含まれる情報を活用したりする。最後に、単純な最適化アルゴリズムを高レベルの手順に組み合わせることによって形成されるいくつかの最適化略のレビューで結論とする。

## 学習は純粋な最適化とどれほど異なるか

深層モデルの訓練で使われる最適化アルゴリズムはいくつかの点で伝統的な最適化アルゴリズムと異なる。機械学習は通常では間接的に行われる。ほとんどの機械学習シナリオでは、我々は、テストセットに関して定義され、扱いづらくもあるかもしれない、あるパフォーマンス測度$P$に関心を持っている。故に、我々は間接的にのみ$P$を最適化する。そうすることが$P$を改善するだろうという望みを持って異なるコスト関数$J(\boldsymbol{\theta})$を減少させる。これは、$J$を最小化することがそれ自体の目標である純粋な最適化と対照的である。深層モデルの訓練に対する最適化アルゴリズムもまた一般に機械学習の目的関数の特定の構造に関するある特殊化を含む。
一般的には、コスト関数は訓練セットに対する平均として、以下のように記述できる。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{(\boldsymbol{x}, \mathbf{y}) \sim \hat{p}_{data}} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

ここで、$L$はexampleごとの損失関数であり、$f(\boldsymbol{x}; \boldsymbol{\theta})$は入力が$\boldsymbol{x}$であるときに予測される出力であり、$\hat{p}_{data}$は経験分布である。教師あり学習の場合、$y$はtarget出力である。本章を通して、我々は、$L$への引数が$f(\boldsymbol{x}; \boldsymbol{\theta})$と$y$である、正規化なし教師ありの場合を開発する。様々な形式の正則化や教師なし学習を開発するために、この開発を、例えば、引数として$\boldsymbol{\theta}$か$\boldsymbol{x}$を含むように、または、引数として$y$を除外するように拡張することは自明である。
[@eq:8.1]は訓練セットに関する目的関数を定義する。我々は通常では、期待値が単に有限の訓練セットに対するのではなく、*データ生成分布*$p_{data}$に渡って取られるところの対応する目的関数を最小化する方を好むだろう。

$$
J^*(\boldsymbol{\theta}) = \mathbb{E}_{(\boldsymbol{x}, \mathbf{y}) \sim p_{data}} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

### 経験的リスク最小化

機械学習アルゴリズムの目標は[@eq:8.2]で与えられる汎化誤差の期待値を減らすことである。この量はリスク[risk]として知られる。我々はここで期待値が真の下地の分布$p_{data}$に対して取られることを強調する。真の分布$p_{data}(\boldsymbol{x}, y)$を知っていたならば、リスク最小化は最適化アルゴリズムによって解くことが出来る最適化タスクであっただろう。しかしながら、$p_{data}(\boldsymbol{x}, y)$を知らず、サンプルの訓練セットを持つだけであるとき、我々は機械学習問題を持つ。
機械学習問題を最適化問題に変換し直すための最も単純な方法は訓練セットに関して損失の期待値を最小化することである。これは真の分布$p(\boldsymbol{x}, y)$を訓練セットで定義される経験分布$\hat{p}(\boldsymbol{x}, y)$で置き換えることを意味する。いま、我々は経験的リスク[empirical risk]を最小化する。

$$
\mathbb{E}_{\boldsymbol{x}, \mathbf{y} \sim \hat{p}_{data}(\boldsymbol{x}, y)} [L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)] = \frac{1}{m} \sum_{i=1}^m L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), y^{(i)})
$$

ここで、$m$は訓練examplesの数である。
この平均訓練誤差を最小化することに基づく訓練プロセスは経験的リスク最小化[empirical risk minimization]として知られる。この設定では、機械学習はは依然として素直な最適化に非常に似ている。リスクを直接的に最適化するのではなく、我々は経験的リスクを最適化し、同様にリスクが著しく減少することを望む。多種多様な理論的な結果は真のリスクが様々な量で減少すると期待され得る条件の下で成立している。
にもかかわらず、経験的リスク最小化はオーバーフィットしがちである。高容量を持つモデルは訓練セットを単純に暗記する可能性がある。多くの場合、経験的リスク最小化は本当に実現可能ではない。最も効果的な近代の最適化アルゴリズムは勾配降下法に基づくが、0-1損失のような多くの有用な損失関数は有用な微分を持たない（その微分はいたるところで0か異定義のいずれかである）。これら2つの問題は、深層学習の文脈において、経験的リスク最小化をほとんど使わないことを意味する。代わりに、実際に最適化する量が本当に最適化したい量からさらに異なるような若干異なるアプローチを使わなければならない。

### 代理損失関数と早期終了

時折、我々が実際に関心を持つ損失関数（例えば、分類誤差）は効率的に最適化できるものではない。例えば、0-1損失の期待値を厳密に最適化することは、線形の分類器[@Marcotte1992]に対してであっても、一般に扱いにくい（入力次元において指数関数的）。そのような状況では、代わりに代理損失関数[surrogate loss function]を一般に最適化する。これは、代理人[proxy]として振る舞うが、利点を持つ。例えば、正しいクラスの負の対数尤度は一般に0-1損失に対する代理として用いられれう。負の対数尤度はモデルが、入力を与えられたときにクラスの条件付き確率を推定することを可能にし、モデルがそれをうまく行うことができるならば、期待値における最小の分類誤差を生み出すクラスを選び取ることができる。
いくつかの場合、代理損失関数は実際により学習できるようになる。例えば、テストセットの0-1損失は、対数尤度の代理を用いて訓練するとき、訓練セットの0-1損失がゼロに達した後にしばしば長い間減り続ける。これは、0-1損失の期待値がゼロであるときでさえ、さらに自信と信頼を持った分類機を得ようと、お互いに離してクラスをさらに押し出すことによって、分類器の堅牢性を改善し得るためである。故に、訓練データからこれ以上の情報を抽出することは単純に訓練セットに関して0-1損失の平均を最小化することによって可能となっていたであろう。一般の最適化と訓練アルゴリズムに対して使うものとしての最適化との非常に重要な差異は訓練アルゴリズムが通常では極小で停止しないことである。代わりに、機械学習アルゴリズムは通常では代理損失関数を最小化するが、早期終了[@sec:7.8]に基づく収束基準が満たされるときに停止する。一般に、早期終了の基準は、検証セットで計測される0-1損失のような、真の下地の損失関数に基づかれ、アルゴリズムをオーバーフィッティングが発生し始めたときならいつでも停止させるように設計される。訓練はしばしば代理損失関数が依然として大きな微分を持つ間に停止する。これは、最適化アルゴリズムが勾配が非常に小さくなるときに収束したとみなされる純粋な最適化設定と非常に異なる。

### バッチとミニバッチのアルゴリズム

一般の最適化アルゴリズムとそれらを分ける機械学習アルゴリズムの側面のひとつは目的関数が通常では訓練examplesに対する総和として分解されることである。機械学習に対する最適化アルゴリズムは一般に完全なコスト関数の項の部分集合のみを用いて推定されるコスト関数の期待値に基づくパラメータの各更新を計算する。
例えば、対数区間でみたときの最尤推定問題は各exmapleに対する総和に分解される。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} \sum_{i=1}^m \log p_{model}(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})
$$

この総和を最大化することは訓練セットで定義される経験分布に対する期待値を最大化することと等価である。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \log p_{model}(\boldsymbol{x}, y; \boldsymbol{\theta})
$$

我々の最適化アルゴリズムのほとんどで使われる目的関数$J$の特性のほとんどは訓練セットに対する期待値でもある。例えば、最も一般的に使われる特性は勾配である。

$$
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \nabla_{\boldsymbol{\theta}} \log p_{model}(\boldsymbol{x}, y; \boldsymbol{\theta})
$$

この期待値を厳密に計算することは、データセット全体におけるすべてのexampleでモデルを評価する必要があるので、非常に高価である。実践では、我々は***によってこれらの期待値を計算できる。
