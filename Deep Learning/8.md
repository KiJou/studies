# 深層モデルの訓練に対する最適化

深層学習アルゴリズムは多くの状況で最適化を伴う。例えば、PCAのようなモデルで推論を行うことは最適化問題を解くことを伴う。我々はしばしば、証明を記述したりアルゴリズムを設計したりするために解析的な最適化を用いる。深層学習に組み込まれる多くの最適化問題の中で、最も困難なものはニューラルネットワークの訓練である。ニューラルネットワークの訓練問題のたった1つのインスタンスを解くために何百のマシンで数日から数ヶ月を投資することは極めて一般的である。この問題はそれほどに重要であり、それほどに高価であるので、最適化技術の特殊化された集合はそれを最適化するために開発された。本性はニューラルネットワークの訓練に対するこれらの最適化技術を提示する。
あなたが勾配ベースの最適化の基本原則に親しみを持たないならば、[@sec:4]を復習することを提案する。この章は一般における数値的最適化の簡潔な概要を含む。
本章は、コスト関数$J(\boldsymbol{\theta})$を大幅に減少させるニューラルネットワークのパラメータ$\boldsymbol{\theta}$を求める、という最適化の1つの特定のケースに焦点を当ている。これは、追加の正則化項と同様に訓練セット全体で評価されるパフォーマンス測度を一般に含む。
我々は機械学習タスクに対する訓練アルゴリズムとして使われる最適化が純粋な最適化と異なる具合の説明から始める。次に、ニューラルネットワークの最適化を難しくする具体的な課題にいくつかを提示する。そして、最適化アルゴリズム自体とパラメータを初期化するための戦略の両方を含めた、いくつかの実践的なアルゴリズムを定義する。より発展的なアルゴリズムは訓練中に学習率に適応したり、コスト関数の二階微分に含まれる情報を活用したりする。最後に、単純な最適化アルゴリズムを高レベルの手順に組み合わせることによって形成されるいくつかの最適化略のレビューで結論とする。

## 学習は純粋な最適化とどれほど異なるか

深層モデルの訓練で使われる最適化アルゴリズムはいくつかの点で伝統的な最適化アルゴリズムと異なる。機械学習は通常では間接的に行われる。ほとんどの機械学習シナリオでは、我々は、テストセットに関して定義され、扱いづらくもあるかもしれない、あるパフォーマンス測度$P$に関心を持っている。故に、我々は間接的にのみ$P$を最適化する。そうすることが$P$を改善するだろうという望みを持って異なるコスト関数$J(\boldsymbol{\theta})$を減少させる。これは、$J$を最小化することがそれ自体の目標である純粋な最適化と対照的である。深層モデルの訓練に対する最適化アルゴリズムもまた一般に機械学習の目的関数の特定の構造に関するある特殊化を含む。
一般的には、コスト関数は訓練セットに対する平均として、以下のように記述できる。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{(\boldsymbol{x}, \mathbf{y}) \sim \hat{p}_{data}} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

ここで、$L$はexampleごとの損失関数であり、$f(\boldsymbol{x}; \boldsymbol{\theta})$は入力が$\boldsymbol{x}$であるときに予測される出力であり、$\hat{p}_{data}$は経験分布である。教師あり学習の場合、$y$はtarget出力である。本章を通して、我々は、$L$への引数が$f(\boldsymbol{x}; \boldsymbol{\theta})$と$y$である、正規化なし教師ありの場合を開発する。様々な形式の正則化や教師なし学習を開発するために、この開発を、例えば、引数として$\boldsymbol{\theta}$か$\boldsymbol{x}$を含むように、または、引数として$y$を除外するように拡張することは自明である。
[@eq:8.1]は訓練セットに関する目的関数を定義する。我々は通常では、期待値が単に有限の訓練セットに対するのではなく、*データ生成分布*$p_{data}$に渡って取られるところの対応する目的関数を最小化する方を好むだろう。

$$
J^*(\boldsymbol{\theta}) = \mathbb{E}_{(\boldsymbol{x}, \mathbf{y}) \sim p_{data}} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

### 経験的リスク最小化

機械学習アルゴリズムの目標は[@eq:8.2]で与えられる汎化誤差の期待値を減らすことである。この量はリスク[risk]として知られる。我々はここで期待値が真の下地の分布$p_{data}$に対して取られることを強調する。真の分布$p_{data}(\boldsymbol{x}, y)$を知っていたならば、リスク最小化は最適化アルゴリズムによって解くことが出来る最適化タスクであっただろう。しかしながら、$p_{data}(\boldsymbol{x}, y)$を知らず、サンプルの訓練セットを持つだけであるとき、我々は機械学習問題を持つ。
機械学習問題を最適化問題に変換し直すための最も単純な方法は訓練セットに関して損失の期待値を最小化することである。これは真の分布$p(\boldsymbol{x}, y)$を訓練セットで定義される経験分布$\hat{p}(\boldsymbol{x}, y)$で置き換えることを意味する。いま、我々は経験的リスク[empirical risk]を最小化する。

$$
\mathbb{E}_{\boldsymbol{x}, \mathbf{y} \sim \hat{p}_{data}(\boldsymbol{x}, y)} [L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)] = \frac{1}{m} \sum_{i=1}^m L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), y^{(i)})
$$

ここで、$m$は訓練examplesの数である。
この平均訓練誤差を最小化することに基づく訓練プロセスは経験的リスク最小化[empirical risk minimization]として知られる。この設定では、機械学習はは依然として素直な最適化に非常に似ている。リスクを直接的に最適化するのではなく、我々は経験的リスクを最適化し、同様にリスクが著しく減少することを望む。多種多様な理論的な結果は真のリスクが様々な量で減少すると期待され得る条件の下で成立している。
にもかかわらず、経験的リスク最小化はオーバーフィットしがちである。高容量を持つモデルは訓練セットを単純に暗記する可能性がある。多くの場合、経験的リスク最小化は本当に実現可能ではない。最も効果的な近代の最適化アルゴリズムは勾配降下法に基づくが、0-1損失のような多くの有用な損失関数は有用な微分を持たない（その微分はいたるところで0か異定義のいずれかである）。これら2つの問題は、深層学習の文脈において、経験的リスク最小化をほとんど使わないことを意味する。代わりに、実際に最適化する量が本当に最適化したい量からさらに異なるような若干異なるアプローチを使わなければならない。

### 代理損失関数と早期終了

時折、我々が実際に関心を持つ損失関数（例えば、分類誤差）は効率的に最適化できるものではない。例えば、0-1損失の期待値を厳密に最適化することは、線形の分類器[@Marcotte1992]に対してであっても、一般に扱いにくい（入力次元において指数関数的）。そのような状況では、代わりに代理損失関数[surrogate loss function]を一般に最適化する。これは、代理人[proxy]として振る舞うが、利点を持つ。例えば、正しいクラスの負の対数尤度は一般に0-1損失に対する代理として用いられれう。負の対数尤度はモデルが、入力を与えられたときにクラスの条件付き確率を推定することを可能にし、モデルがそれをうまく行うことができるならば、期待値における最小の分類誤差を生み出すクラスを選び取ることができる。
いくつかの場合、代理損失関数は実際により学習できるようになる。例えば、テストセットの0-1損失は、対数尤度の代理を用いて訓練するとき、訓練セットの0-1損失がゼロに達した後にしばしば長い間減り続ける。これは、0-1損失の期待値がゼロであるときでさえ、さらに自信と信頼を持った分類機を得ようと、お互いに離してクラスをさらに押し出すことによって、分類器の堅牢性を改善し得るためである。故に、訓練データからこれ以上の情報を抽出することは単純に訓練セットに関して0-1損失の平均を最小化することによって可能となっていたであろう。一般の最適化と訓練アルゴリズムに対して使うものとしての最適化との非常に重要な差異は訓練アルゴリズムが通常では極小で停止しないことである。代わりに、機械学習アルゴリズムは通常では代理損失関数を最小化するが、早期終了[@sec:7.8]に基づく収束基準が満たされるときに停止する。一般に、早期終了の基準は、検証セットで計測される0-1損失のような、真の下地の損失関数に基づかれ、アルゴリズムをオーバーフィッティングが発生し始めたときならいつでも停止させるように設計される。訓練はしばしば代理損失関数が依然として大きな微分を持つ間に停止する。これは、最適化アルゴリズムが勾配が非常に小さくなるときに収束したとみなされる純粋な最適化設定と非常に異なる。

### バッチとミニバッチのアルゴリズム

一般の最適化アルゴリズムとそれらを分ける機械学習アルゴリズムの側面のひとつは目的関数が通常では訓練examplesに対する総和として分解されることである。機械学習に対する最適化アルゴリズムは一般に完全なコスト関数の項の部分集合のみを用いて推定されるコスト関数の期待値に基づくパラメータの各更新を計算する。
例えば、対数区間でみたときの最尤推定問題は各exmapleに対する総和に分解される。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} \sum_{i=1}^m \log p_{model}(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})
$$

この総和を最大化することは訓練セットで定義される経験分布に対する期待値を最大化することと等価である。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \log p_{model}(\boldsymbol{x}, y; \boldsymbol{\theta})
$$

我々の最適化アルゴリズムのほとんどで使われる目的関数$J$の特性のほとんどは訓練セットに対する期待値でもある。例えば、最も一般的に使われる特性は勾配である。

$$
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \nabla_{\boldsymbol{\theta}} \log p_{model}(\boldsymbol{x}, y; \boldsymbol{\theta})
$$

この期待値を厳密に計算することは、データセット全体におけるすべてのexampleでモデルを評価する必要があるので、非常に高価である。実践では、我々はデータセットから少数のexamplesを無作為にサンプリングし、これらのexamplesのみに対して平均をとることによってこれらの期待値を計算できる。
$n$個のサンプルから推定される平均の標準誤差[@eq:5.46]は$\sigma / \sqrt{n}$で与えられることを思い出したい。ここで、$\sigma$はサンプルの値の真の標準偏差である。分母の$\sqrt{n}$は勾配を推定するためにより多くのexamplesを用いることへの線形なreturnsがほとんどないことを示す。2つの勾配の仮説の推定値を比較すると、片方は100個のexamplesに基づき、もう片方は10000個のexamplesに基づく。後者は前者より100倍多くの計算を必要とするが、10の倍数でのみ平均の標準偏差を減少させる。ほとんどの最適化アルゴリズムは、厳密な勾配を低速に計算するではなく、勾配の近似的な推定値を高速に計算することができるならば、（更新数の観点ではなく、総計算量の観点で）さらに高速に収束する。
少数のサンプルからの勾配の統計的な推定を動機付けするもうひとつの検討事項は訓練セットにおける冗長性である。最悪の場合、訓練セットにおける$m$個すべてのサンプルはお互いの同一のコピーとなり得るかもしれない。サンプリングベースの勾配の推定は、ナイーブなアプローチより$m$倍少ない計算を用いて、単一のサンプルで正しい勾配を計算できるかもしれない。実践では、このワーストケースの状況に遭遇する可能性は低いが、すべてが勾配への非常に似通った貢献を行う大量のexamplesを見つけるかもしれない。
訓練セット全体を用いる最適化アルゴリズムは、大きなバッチですべての訓練examplesを同時に処理するので、バッチ[batch]とか決定論的勾配法[deterministic gradient methods]と呼ばれる。この用語法は、「バッチ」という言葉がミニバッチ確率的勾配降下法で用いられるミニバッチを述べるのにしばしば使われたりもするので、いくらか混乱を招き得る。一般に、「バッチ勾配降下法」という用語は完全な訓練セットの使用を暗示する一方、examplesのグループを述べるための「バッチ」という用語の使用はそうではない。例えば、ミニバッチの大きさを述べるために「バッチサイズ」という用語を用いるのは一般的である。
一度に1つのexampleのみを用いる最適化アルゴリズムは、あるときでは確率的[stochastic]手法と呼ばれ、またあるときではオンライン[online]手法と呼ばれる。「オンライン」という用語は通常、examplesが、いくつかの経路が作られる固定サイズの訓練セットからではなく、連続的に生成されるexamplesの1つの流れから描かれるときのために予約されている。
深層学習で使われるほとんどのアルゴリズムは、訓練examplesすべてより少ないが1より多い、その中間に位置する。これらは伝統的にミニバッチ[minibatch]とかミニバッチ確率的[minibatch stochastic]手法と呼ばれていて、いまでは単に確率的[stochastic]手法と呼ぶのが一般的である。
確率的手法の標準的な例は、[@sec:8.3.1]で詳細に示される、確率的勾配降下法である。
ミニバッチの大きさは一般に以下の要因によって操作される。

- より大きなバッチはより正確な勾配の推定値をもたらすが、ほとんどlinear returnsを持たない。
- マルチコアアーキテクチャは通常極めて小さなバッチでは十分に活用されない。これはある絶対的な最小のバッチサイズを用いることを動機付けする。それを下回ると、ミニバッチの処理時間は削減されない。
- バッチ内のexamplesすべてが並列に処理されるならば（一般的にあることだが）、メモリ量はバッチサイズでスケールする。多くのハードウェア設定では、これはバッチサイズにおける制因である。
- いくつかの種類のハードウェアは特定のサイズの配列でより良い実行時間を達成する。特に、GPUを用いているとき、2の累乗のバッチサイズがより良い実行時間をもたらすことは一般的である。典型的な2の累乗のバッチサイズは32から256の範囲にあり、大きなモデルでは時折16が試みられる。
- 小さなバッチは、おそらくそれらが学習プロセスに追加するノイズのために、正則化効果[@Wilson2003]をもたらし得る。汎化誤差はしばしば1のバッチサイズに対して最適である。そのような小さなバッチサイズでの訓練は、勾配の推定値における高い分散により、安定性を維持するために小さな学習率を必要とするかもしれないだろう。総実行時間は、減少した学習率によって、および、訓練セット全体を観測するためにより多くのステップを取るので、より多くのステップを作る必要性の結果として非常に高くなる可能性がある。

様々な種類のアルゴリズムは様々な方法でミニバッチからの様々な種類の情報を用いる。いくつかのアルゴリズムは、少数のサンプリングで正確に推定することが難しい情報を用いるか、サンプリング誤差をさらに増幅する方法で情報を用いるか、のいずれかであるので、その他よりもサンプリング誤差に対して敏感である。勾配$\boldsymbol{g}$にのみ基づいて更新を計算する手法は通常、比較的堅牢であり、100のようなより小さなバッチサイズを扱うことができる。ヘッセ行列$\boldsymbol{H}$を用いたり、$\boldsymbol{H}^{-1} \boldsymbol{g}$のように更新を計算したりもするsecond-order手法は一般に、10000のようなさらに大きなバッチサイズを必要とする。これらの大きなバッチサイズは$\boldsymbol{H}^{-1} \boldsymbol{g}$の推定値における変動を最小化するために必要とされる。$\boldsymbol{H}$が完璧に推定されるが、悪条件数を持つとする。$\boldsymbol{H}$による乗算かその逆は前から存在する誤差、この場合、$\boldsymbol{g}$における推定誤差を増幅する。故に、$\boldsymbol{g}$の推定値における非常に小さな変化は、$\boldsymbol{H}$が完璧に推定される場合でさえ、更新$\boldsymbol{H}^{-1} \boldsymbol{g}$における大きな変化を引き起し得る。もちろん、$\boldsymbol{H}$は近似的にのみ推定され、そのために、更新$\boldsymbol{H}^{-1} \boldsymbol{g}$は$\boldsymbol{g}$の推定値に悪条件操作を適用することから予測するであろうよりもっと多くの誤差を含むだろう。
ミニバッチが無作為に選択されることも重要である。サンプルの集合から勾配の期待値のバイアスのない推定値を計算することはこれらのサンプルが独立である必要がある。我々は後続の2つの勾配の推定値に対してもお互いから独立であることを望み、そのために、その次の2つのexamplesのミニバッチもまたお互いに独立であるべきである。多くのデータセットは連続するexamplesが高度に相関している方法で最も自然に配置される。例えば、血液サンプル検査の結果の長いリストを持つ医療データのデータセットがあるかもしれない。このリストは、まず1番目の患者から異なる時間に取られた5つの血液サンプルがあり、その後、2番目の患者から取られた3つの血液サンプルがあり、そして、3番目の患者からの血液サンプル、といったように配置されるかもしれない。このリストから順にexamplesを描こうとした場合、データセットにおける多数の患者の中から主に1人の患者を表すであろうことから、ミニバッチのそれぞれは非常にバイアスされただろう。データセットの順序がある優位性を満たすこれらのような場合、ミニバッチを選ぶ前にexamplesを入れ替えることが必要である。非常に大きなデータセット、例えば、データセンターの数十億個のexamplesを含むデータセットでは、ミニバッチを構築したいと思うたびに無作為で真に一様にexamplesをサンプルすることは非現実的である可能性がある。幸いにも、実践では、データセットの順序を一度だけ入れ替え、入れ替えた状態で格納すると通常は十分である。これは、すべてのモデルがその後に用いるであろう、連続したexamplesの取り得るミニバッチの固定の集合を強いるだろうし、それぞれの個々のモデルは訓練データを通すたびにこの順序を再利用することを強いられるだろう。真の無作為な選択からのこの逸脱は著しい有害な効果があるとは考えない。何らかの方法でexamplesを入れ替えるのさえも失敗すると、アルゴリズムの有効性をひどく減少させ得る。
機械学習における多くの最適化問題は並列に様々なexamplesに対する別個の更新全体を計算できるのに十分うまくexamplesに対して分解する。別の言い方をすれば、他のいくつかのミニバッチに対する更新を計算するのと同時にexamples $\boldsymbol{X}$のミニバッチ1つに対して$J(\boldsymbol{X})$を最小化する更新を計算できる。そのような非同期な並列分散アプローチは[@sec:12.1.3]でさらに考察される。
ミニバッチ確率的勾配降下法に対する興味深い動機は、examplesが繰り返されない限り、真の*汎化誤差*[@eq:8.2]の勾配に従う。ミニバッチ確率的勾配降下法のほとんどの実装はデータセットを一度だけシャッフルし、複数回それを通過する。1回目のパスでは、各ミニバッチは真の汎化誤差のバイアスのない推定値を計算するのに使われる。2回目のパスでは、その推定値は、データ生成分布から新しい公平なサンプルを得るのではなく、すでに使われている再サンプリング値によって形成されるので、バイアスありになる。
確率的勾配降下法が汎化誤差を最小化するという事実は、examplesかミニバッチがデータの流れ[stream]から描かれるオンライン学習で確認するのが最も容易である。言い換えれば、固定サイズの訓練セットを受け取るのではなく、learnerは各インスタンスで新しいexampleを見る生物に似ていて、すべてのexample$(\boldsymbol{x}, y)$がデータ生成分布$p_{data}(\boldsymbol{x}, y)$に由来する。このシナリオでは、examplesは一切繰り返されない。つまり、すべての経験が$p_{data}$からの公平なサンプルである。
その等価性は$\boldsymbol{x}$と$y$の両方が離散的であるときに導出するのが最も容易である。この場合、汎化誤差[@eq:8.2]は

$$
J^*(\boldsymbol{\theta}) = \sum_\boldsymbol{x} \sum_y p_{data}(\boldsymbol{x}, y) L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

として以下のような厳密な勾配を持つ総和として記述できる。

$$
\boldsymbol{g} = \nabla_\boldsymbol{\theta} J^*(\boldsymbol{\theta}) = \sum_\boldsymbol{x} \sum_y p_{data}(\boldsymbol{x}, y) \nabla_\boldsymbol{\theta} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

我々は[@eq:8.5]と[@eq:8.6]における対数尤度で実証される同じ事実をすでに確認してきた。つまり、これが尤度を除く他の関数$L$で成り立つことを確認する。似た結果は、$p_{data}$と$L$に関する緩やかな仮定の下で、$\boldsymbol{x}$と$y$が連続的であるときに導出できる。
したがって、データ生成分布$p_{data}$から対応するターゲット$y^{(i)}$でexamples${\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)}$のミニバッチをサンプリングし、以下のようなそのミニバッチに対するパラメータに関する損失の勾配を計算することによって汎化誤差の厳密な勾配のバイアスなし推定器を得られる。

$$
\hat{\boldsymbol{g}} = \frac{1}{m} \nabla_\boldsymbol{\theta} \sum_i L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), y^{(i)})
$$

$\hat{\boldsymbol{g}}$の方向で$\boldsymbol{\theta}$を更新することは汎化誤差に関してSGDを行う。
もちろん、この解釈はexamplesが再利用されないときのみ適用される。それにも関わらず、訓練セットが極めて大きくない限り、何度か訓練セットを通過するのが通常は最適である。複数のそのようなepochsが使われるとき、最初のepochのみが汎化誤差のバイアスのない勾配に従うが、もちろん、追加のepochsは通常、訓練誤差とテスト誤差の間のギャップを増加することによって引き起こす害を相殺するために減少される訓練誤差による十分な恩恵をもたらす。
いくつかのデータセットが、計算力より速く、サイズにおいて素早く増加すると、機械学習アプリケーションでは各訓練exampleをたった一度だけ用いる、または、訓練セットを通る不完全なパスを作ることがより一般的になっている。極めて大きな訓練セットを用いるとき、オーバーフィッティングは問題ではなく、そのために、アンダーフィティングや計算効率は有力な懸念事項となる。訓練examples数が増えるとともに、汎化誤差に関する計算上のボトルネックの効果の考察は@Bottou2008も参照のこと。

## ニューラルネットワーク最適化における課題

一般における最適化は極めて難しいタスクである。伝統的には、機械学習は最適化問題が凸であることを保証するために目的関数と制約を注意深く設計することによって一般の最適化の難しさを回避してきた。ニューラルネットワークを訓練するとき、我々は一般的な非凸の場合に立ち向かわなければならない。凸最適化でさえも複雑さがまったくないというわけではない。本節では、深層モデルの訓練に対する最適化を伴う最も顕著な課題のいくつかをまとめる。

### 悪条件

いくつかの課題は凸関数を最適化するときでさえ発生する。この内、最も顕著なのはヘッセ行列$\boldsymbol{H}$の悪条件である。これは、凸であれ非凸であれ、ほとんどの数値最適化において非常に一般的な問題であり、[@sec:4.3.1]にてより詳細に述べられる。
悪条件問題はニューラルネットワーク訓練問題に存在していると一般に理解されている。悪条件はSGDに非常に小さなステップでさえコスト関数を増加するという意味において「立ち往生」させることで現れ得る。
コスト関数の2次のテイラー級数展開が$-\epsilon \boldsymbol{g}$の勾配降下法ステップが以下をコストに加えるであろうことを予測することを[@eq:4.9]より思い出したい。

$$
\frac{1}{2} \epsilon^2 \boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g} - \epsilon \boldsymbol{g}^\top \boldsymbol{g}
$$

勾配の悪条件は$\frac{1}{2} \epsilon^2 \boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}$が$\epsilon \boldsymbol{g}^\top \boldsymbol{g}$を超えるときに問題となる。悪条件がニューラルネットワークの訓練タスクにとって有害であるかどうかを決定するため、勾配のノルムの二乗$\boldsymbol{g}^\top \boldsymbol{g}$と$\boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}$項をモニタリングすることができる。多くの場合、勾配のノルムは学習を通してほとんど縮小しないが、$\boldsymbol{g}^\top \boldsymbol{H} \boldsymbol{g}$項は一桁以上の桁数で増加する。その結果として、学習率がさらに強力な曲率を補うために縮小されなければならないので、学習が強力な勾配があるにも関わらず非常に低速になる。[@fig:8.1]はニューラルネットワークの成功した訓練の間に著しく増加する勾配の例を示す。

![勾配降下法はいかなる種類の臨界点にもしばしば到達しない。この例では、勾配のノルムは物体検出で使われる畳み込みネットワークの訓練を通して増加する。（左）個々の勾配のノルムの計算が時間とともに分散する具合を示す散布図。読みやすさを改善するため、epochあたり1つだけの勾配ノルムをプロットする。すべての勾配ノルムの移動平均は実線の曲線としてプロットされる。勾配ノルムは、訓練プロセスが臨界点に収束する場合に期待するであろうように減少するのではなく、時間とともに明らかに増加している。（右）勾配を増加しているにも関わらず、訓練プロセスはほどほどに成功している。検証セット分類誤差は低いレベルに減少している。](fig/8-1.png){#fig:8.1}

悪条件はニューラルネットワークの訓練に加えて他の設定でも現れるにも関わらず、他の状況でそれをなくそうとするのに使われる技術のいくつかはニューラルネットワークにあまり適用できない。例えば、ニュートン法は悪条件のヘッセ行列を持つ凸関数を最小化するための素晴らしいツールであるが、後続の節で論証する通り、ニュートン法はニューラルネットワークに適用できる前に大幅な修正を必要とする。

### 極小

凸最適化問題の最も顕著な特徴のひとつは、極小を求める問題に小さくできるということである。いづれかの極小は極大であることが保証される。いくつかの凸関数は、単一の最大の点ではなく、下部に平坦な領域を持つが、そのような平坦領域内のいづれかの点は許容できる解である。凸関数を最適化するとき、いずれの種類の臨界点を求める場合に良好な解に到達したことを知っている。
ニューラルネットのような非凸関数では、多数の極小を持つ可能性がある。実際に、ほとんどどの深層モデルも極めて多数の極小を持つことを本質的に保証されている。しかし、我々が見るであろう通り、これは必然的に主要な問題ではない。
ニューラルネットワークと複数の等価的にパラメータ化された潜在変数を持ついずれかのモデルのすべては、***により、複数の極小を持つ。***
