# 深層モデルの訓練に対する最適化

深層学習アルゴリズムは多くの状況で最適化を伴う。例えば、PCAのようなモデルで推論を行うことは最適化問題を解くことを伴う。我々はしばしば、証明を記述したりアルゴリズムを設計したりするために解析的な最適化を用いる。深層学習に組み込まれる多くの最適化問題の中で、最も困難なものはニューラルネットワークの訓練である。ニューラルネットワークの訓練問題のたった1つのインスタンスを解くために何百のマシンで数日から数ヶ月を投資することは極めて一般的である。この問題はそれほどに重要であり、それほどに高価であるので、最適化技術の特殊化された集合はそれを最適化するために開発された。本性はニューラルネットワークの訓練に対するこれらの最適化技術を提示する。
あなたが勾配ベースの最適化の基本原則に親しみを持たないならば、[@sec:4]を復習することを提案する。この章は一般における数値的最適化の簡潔な概要を含む。
本章は、コスト関数$J(\boldsymbol{\theta})$を大幅に減少させるニューラルネットワークのパラメータ$\boldsymbol{\theta}$を求める、という最適化の1つの特定のケースに焦点を当ている。これは、追加の正則化項と同様に訓練セット全体で評価されるパフォーマンス測度を一般に含む。
我々は機械学習タスクに対する訓練アルゴリズムとして使われる最適化が純粋な最適化と異なる具合の説明から始める。次に、ニューラルネットワークの最適化を難しくする具体的な課題にいくつかを提示する。そして、最適化アルゴリズム自体とパラメータを初期化するための戦略の両方を含めた、いくつかの実践的なアルゴリズムを定義する。より発展的なアルゴリズムは訓練中に学習率に適応したり、コスト関数の二階微分に含まれる情報を活用したりする。最後に、単純な最適化アルゴリズムを高レベルの手順に組み合わせることによって形成されるいくつかの最適化略のレビューで結論とする。

## 学習は純粋な最適化とどれほど異なるか

深層モデルの訓練で使われる最適化アルゴリズムはいくつかの点で伝統的な最適化アルゴリズムと異なる。機械学習は通常では間接的に行われる。ほとんどの機械学習シナリオでは、我々は、テストセットに関して定義され、扱いづらくもあるかもしれない、あるパフォーマンス測度$P$に関心を持っている。故に、我々は間接的にのみ$P$を最適化する。そうすることが$P$を改善するだろうという望みを持って異なるコスト関数$J(\boldsymbol{\theta})$を減少させる。これは、$J$を最小化することがそれ自体の目標である純粋な最適化と対照的である。深層モデルの訓練に対する最適化アルゴリズムもまた一般に機械学習の目的関数の特定の構造に関するある特殊化を含む。
一般的には、コスト関数は訓練セットに対する平均として、以下のように記述できる。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{(\boldsymbol{x}, \mathbf{y}) \sim \hat{p}_{data}} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

ここで、$L$はexampleごとの損失関数であり、$f(\boldsymbol{x}; \boldsymbol{\theta})$は入力が$\boldsymbol{x}$であるときに予測される出力であり、$\hat{p}_{data}$は経験分布である。教師あり学習の場合、$y$はtarget出力である。本章を通して、我々は、$L$への引数が$f(\boldsymbol{x}; \boldsymbol{\theta})$と$y$である、正規化なし教師ありの場合を開発する。様々な形式の正則化や教師なし学習を開発するために、この開発を、例えば、引数として$\boldsymbol{\theta}$か$\boldsymbol{x}$を含むように、または、引数として$y$を除外するように拡張することは自明である。
[@eq:8.1]は訓練セットに関する目的関数を定義する。我々は通常では、期待値が単に有限の訓練セットに対するのではなく、*データ生成分布*$p_{data}$に渡って取られるところの対応する目的関数を最小化する方を好むだろう。

$$
J^*(\boldsymbol{\theta}) = \mathbb{E}_{(\boldsymbol{x}, \mathbf{y}) \sim p_{data}} L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)
$$

### 経験的リスク最小化

機械学習アルゴリズムの目標は[@eq:8.2]で与えられる汎化誤差の期待値を減らすことである。この量はリスク[risk]として知られる。我々はここで期待値が真の下地の分布$p_{data}$に対して取られることを強調する。真の分布$p_{data}(\boldsymbol{x}, y)$を知っていたならば、リスク最小化は最適化アルゴリズムによって解くことが出来る最適化タスクであっただろう。しかしながら、$p_{data}(\boldsymbol{x}, y)$を知らず、サンプルの訓練セットを持つだけであるとき、我々は機械学習問題を持つ。
機械学習問題を最適化問題に変換し直すための最も単純な方法は訓練セットに関して損失の期待値を最小化することである。これは真の分布$p(\boldsymbol{x}, y)$を訓練セットで定義される経験分布$\hat{p}(\boldsymbol{x}, y)$で置き換えることを意味する。いま、我々は経験的リスク[empirical risk]を最小化する。

$$
\mathbb{E}_{\boldsymbol{x}, \mathbf{y} \sim \hat{p}_{data}(\boldsymbol{x}, y)} [L(f(\boldsymbol{x}; \boldsymbol{\theta}), y)] = \frac{1}{m} \sum_{i=1}^m L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), y^{(i)})
$$

ここで、$m$は訓練examplesの数である。
この平均訓練誤差を最小化することに基づく訓練プロセスは経験的リスク最小化[empirical risk minimization]として知られる。この設定では、機械学習はは依然として素直な最適化に非常に似ている。リスクを直接的に最適化するのではなく、我々は経験的リスクを最適化し、同様にリスクが著しく減少することを望む。多種多様な理論的な結果は真のリスクが様々な量で減少すると期待され得る条件の下で成立している。
にもかかわらず、経験的リスク最小化はオーバーフィットしがちである。高容量を持つモデルは訓練セットを単純に暗記する可能性がある。多くの場合、経験的リスク最小化は本当に実現可能ではない。最も効果的な近代の最適化アルゴリズムは勾配降下法に基づくが、0-1損失のような多くの有用な損失関数は有用な微分を持たない（その微分はいたるところで0か異定義のいずれかである）。これら2つの問題は、深層学習の文脈において、経験的リスク最小化をほとんど使わないことを意味する。代わりに、実際に最適化する量が本当に最適化したい量からさらに異なるような若干異なるアプローチを使わなければならない。

### 代理損失関数と早期終了

時折、我々が実際に関心を持つ損失関数（例えば、分類誤差）は効率的に最適化できるものではない。例えば、0-1損失の期待値を厳密に最適化することは、線形の分類器[@Marcotte1992]に対してであっても、一般に扱いにくい（入力次元において指数関数的）。そのような状況では、代わりに代理損失関数[surrogate loss function]を一般に最適化する。これは、代理人[proxy]として振る舞うが、利点を持つ。例えば、正しいクラスの負の対数尤度は一般に0-1損失に対する代理として用いられれう。負の対数尤度はモデルが、入力を与えられたときにクラスの条件付き確率を推定することを可能にし、モデルがそれをうまく行うことができるならば、期待値における最小の分類誤差を生み出すクラスを選び取ることができる。
いくつかの場合、代理損失関数は実際により学習できるようになる。例えば、テストセットの0-1損失は、対数尤度の代理を用いて訓練するとき、訓練セットの0-1損失がゼロに達した後にしばしば長い間減り続ける。これは、0-1損失の期待値がゼロであるときでさえ、さらに自信と信頼を持った分類機を得ようと、お互いに離してクラスをさらに押し出すことによって、分類器の堅牢性を改善し得るためである。故に、訓練データからこれ以上の情報を抽出することは単純に訓練セットに関して0-1損失の平均を最小化することによって可能となっていたであろう。一般の最適化と訓練アルゴリズムに対して使うものとしての最適化との非常に重要な差異は訓練アルゴリズムが通常では極小で停止しないことである。代わりに、機械学習アルゴリズムは通常では代理損失関数を最小化するが、早期終了[@sec:7.8]に基づく収束基準が満たされるときに停止する。一般に、早期終了の基準は、検証セットで計測される0-1損失のような、真の下地の損失関数に基づかれ、アルゴリズムをオーバーフィッティングが発生し始めたときならいつでも停止させるように設計される。訓練はしばしば代理損失関数が依然として大きな微分を持つ間に停止する。これは、最適化アルゴリズムが勾配が非常に小さくなるときに収束したとみなされる純粋な最適化設定と非常に異なる。

### バッチとミニバッチのアルゴリズム

一般の最適化アルゴリズムとそれらを分ける機械学習アルゴリズムの側面のひとつは目的関数が通常では訓練examplesに対する総和として分解されることである。機械学習に対する最適化アルゴリズムは一般に完全なコスト関数の項の部分集合のみを用いて推定されるコスト関数の期待値に基づくパラメータの各更新を計算する。
例えば、対数区間でみたときの最尤推定問題は各exmapleに対する総和に分解される。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} \sum_{i=1}^m \log p_{model}(\boldsymbol{x}^{(i)}, y^{(i)}; \boldsymbol{\theta})
$$

この総和を最大化することは訓練セットで定義される経験分布に対する期待値を最大化することと等価である。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \log p_{model}(\boldsymbol{x}, y; \boldsymbol{\theta})
$$

我々の最適化アルゴリズムのほとんどで使われる目的関数$J$の特性のほとんどは訓練セットに対する期待値でもある。例えば、最も一般的に使われる特性は勾配である。

$$
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \nabla_{\boldsymbol{\theta}} \log p_{model}(\boldsymbol{x}, y; \boldsymbol{\theta})
$$

この期待値を厳密に計算することは、データセット全体におけるすべてのexampleでモデルを評価する必要があるので、非常に高価である。実践では、我々はデータセットから少数のexamplesを無作為にサンプリングし、これらのexamplesのみに対して平均をとることによってこれらの期待値を計算できる。
$n$個のサンプルから推定される平均の標準誤差[@eq:5.46]は$\sigma / \sqrt{n}$で与えられることを思い出したい。ここで、$\sigma$はサンプルの値の真の標準偏差である。分母の$\sqrt{n}$は勾配を推定するためにより多くのexamplesを用いることへの線形なreturnsがほとんどないことを示す。2つの勾配の仮説の推定値を比較すると、片方は100個のexamplesに基づき、もう片方は10000個のexamplesに基づく。後者は前者より100倍多くの計算を必要とするが、10の倍数でのみ平均の標準偏差を減少させる。ほとんどの最適化アルゴリズムは、厳密な勾配を低速に計算するではなく、勾配の近似的な推定値を高速に計算することができるならば、（更新数の観点ではなく、総計算量の観点で）さらに高速に収束する。
少数のサンプルからの勾配の統計的な推定を動機付けするもうひとつの検討事項は訓練セットにおける冗長性である。最悪の場合、訓練セットにおける$m$個すべてのサンプルはお互いの同一のコピーとなり得るかもしれない。サンプリングベースの勾配の推定は、ナイーブなアプローチより$m$倍少ない計算を用いて、単一のサンプルで正しい勾配を計算できるかもしれない。実践では、このワーストケースの状況に遭遇する可能性は低いが、すべてが勾配への非常に似通った貢献を行う大量のexamplesを見つけるかもしれない。
訓練セット全体を用いる最適化アルゴリズムは、大きなバッチですべての訓練examplesを同時に処理するので、バッチ[batch]とか決定論的勾配法[deterministic gradient methods]と呼ばれる。この用語法は、「バッチ」という言葉がミニバッチ確率的勾配降下法で用いられるミニバッチを述べるのにしばしば使われたりもするので、いくらか混乱を招き得る。一般に、「バッチ勾配降下法」という用語は完全な訓練セットの使用を暗示する一方、examplesのグループを述べるための「バッチ」という用語の使用はそうではない。例えば、ミニバッチの大きさを述べるために「バッチサイズ」という用語を用いるのは一般的である。
一度に1つのexampleのみを用いる最適化アルゴリズムは、あるときでは確率的[stochastic]手法と呼ばれ、またあるときではオンライン[online]手法と呼ばれる。「オンライン」という用語は通常、examplesが、いくつかの経路が作られる固定サイズの訓練セットからではなく、連続的に生成されるexamplesの1つの流れから描かれるときのために予約されている。
深層学習で使われるほとんどのアルゴリズムは、訓練examplesすべてより少ないが1より多い、その中間に位置する。これらは伝統的にミニバッチ[minibatch]とかミニバッチ確率的[minibatch stochastic]手法と呼ばれていて、いまでは単に確率的[stochastic]手法と呼ぶのが一般的である。
確率的手法の標準的な例は、[@sec:8.3.1]で詳細に示される、確率的勾配降下法である。
ミニバッチの大きさは一般に以下の要因によって操作される。

- より大きなバッチはより正確な勾配の推定値をもたらすが、ほとんどlinear returnsを持たない。
- マルチコアアーキテクチャは通常極めて小さなバッチでは十分に活用されない。これはある絶対的な最小のバッチサイズを用いることを動機付けする。それを下回ると、ミニバッチの処理時間は削減されない。
- バッチ内のexamplesすべてが並列に処理されるならば（一般的にあることだが）、メモリ量はバッチサイズでスケールする。多くのハードウェア設定では、これはバッチサイズにおける制因である。
- いくつかの種類のハードウェアは特定のサイズの配列でより良い実行時間を達成する。特に、GPUを用いているとき、2の累乗のバッチサイズがより良い実行時間をもたらすことは一般的である。典型的な2の累乗のバッチサイズは32から256の範囲にあり、大きなモデルでは時折16が試みられる。
- 小さなバッチは、おそらくそれらが学習プロセスに追加するノイズのために、正則化効果[@Wilson2003]をもたらし得る。汎化誤差はしばしば1のバッチサイズに対して最適である。そのような小さなバッチサイズでの訓練は、勾配の推定値における高い分散により、安定性を維持するために小さな学習率を必要とするかもしれないだろう。総実行時間は、減少した学習率によって、および、訓練セット全体を観測するためにより多くのステップを取るので、より多くのステップを作る必要性の結果として非常に高くなる可能性がある。

様々な種類のアルゴリズムは様々な方法でミニバッチからの様々な種類の情報を用いる。いくつかのアルゴリズムは、少数のサンプリングで正確に推定することが難しい情報を用いるか、サンプリング誤差をさらに増幅する方法で情報を用いるか、のいずれかであるので、その他よりもサンプリング誤差に対して敏感である。勾配$\boldsymbol{g}$にのみ基づいて更新を計算する手法は通常、比較的堅牢であり、100のようなより小さなバッチサイズを扱うことができる。ヘッセ行列$\boldsymbol{H}$を用いたり、$\boldsymbol{H}^{-1} \boldsymbol{g}$のように更新を計算したりもするsecond-order手法は一般に、10000のようなさらに大きなバッチサイズを必要とする。これらの大きなバッチサイズは$\boldsymbol{H}^{-1} \boldsymbol{g}$の推定値における変動を最小化するために必要とされる。$\boldsymbol{H}$が完璧に推定されるが、悪条件数を持つとする。$\boldsymbol{H}$による乗算かその逆は前から存在する誤差、この場合、$\boldsymbol{g}$における推定誤差を増幅する。故に、$\boldsymbol{g}$の推定値における非常に小さな変化は、$\boldsymbol{H}$が完璧に推定される場合でさえ、更新$\boldsymbol{H}^{-1} \boldsymbol{g}$における大きな変化を引き起し得る。もちろん、$\boldsymbol{H}$は近似的にのみ推定され、そのために、更新$\boldsymbol{H}^{-1} \boldsymbol{g}$は$\boldsymbol{g}$の推定値に悪条件操作を適用することから予測するであろうよりもっと多くの誤差を含むだろう。
ミニバッチが無作為に選択されることも重要である。サンプルの集合から勾配の期待値のバイアスのない推定値を計算することはこれらのサンプルが独立である必要がある。我々は後続の2つの勾配の推定値に対してもお互いから独立であることを望み、そのために、その次の2つのexamplesのミニバッチもまたお互いに独立であるべきである。多くのデータセットは連続するexamplesが高度に相関している方法で最も自然に配置される。例えば、血液サンプル検査の結果の長いリストを持つ医療データのデータセットがあるかもしれない。このリストは、まず1番目の患者から異なる時間に取られた5つの血液サンプルがあり、その後、2番目の患者から取られた3つの血液サンプルがあり、そして、3番目の患者からの血液サンプル、といったように配置されるかもしれない。このリストから順にexamplesを描こうとした場合、データセットにおける多数の患者の中から主に1人の患者を表すであろうことから、ミニバッチのそれぞれは非常にバイアスされただろう。データセットの順序がある優位性を満たすこれらのような場合、ミニバッチを選ぶ前にexamplesを入れ替えることが必要である。非常に大きなデータセット、例えば、データセンターの数十億個のexamplesを含むデータセットでは、ミニバッチを構築したいと思うたびに無作為で真に一様にexamplesをサンプルすることは非現実的である可能性がある。幸いにも、実践では、データセットの順序を一度だけ入れ替え、入れ替えた状態で格納すると通常は十分である。これは、すべてのモデルがその後に用いるであろう、連続したexamplesの取り得るミニバッチの固定の集合を強いるだろうし、それぞれの個々のモデルは訓練データを通すたびにこの順序を再利用することを強いられるだろう。真の無作為な選択からのこの逸脱は著しい有害な効果があるとは考えない。何らかの方法でexamplesを入れ替えるのさえも失敗すると、アルゴリズムの有効性をひどく減少させ得る。
機会がにおける多くの最適化問題は***分解する。
