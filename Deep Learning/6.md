# 深層フィードフォワードネットワーク

フィードフォワードニューラルネットワーク[feedforward neural networks]とか多層パーセプトロン[multilayer perceptrons]（MLPs）とも呼ばれる、深層フィードフォワードネットワーク[deep feedforward networks]は典型的な深層学習モデルである。フィードフォワードネットワークの目標はある関数$f^*$を近似することである。例えば、分類器に対して、$y = f^*(\boldsymbol{x})$は入力$\boldsymbol{x}$をカテゴリ$y$に対応させる。フィードフォワードネットワークはマッピング$\boldsymbol{y} = f(\boldsymbol{x}; \boldsymbol{\theta})$を定義し、最良の関数近似となるパラメータ$\boldsymbol{\theta}$の値を学習する。
これらのモデルは、$\boldsymbol{x}$から計算される関数を通り、$f$を定義するのに使われる途中計算を通り、最終的に出力$\boldsymbol{y}$へ情報が流れるので、フィードフォワード[feedforward]と呼ばれる。モデルの出力がそれ自体に戻されるフィードバック[feedback]接続は存在しない。フィードフォワードニューラルネットワークがフィードバック接続を含むよう拡張されるとき、これらは、[@sec:10]で示されるように、リカレントニューラルネットワーク[recurrent neural networks]と呼ばれる。
フィードフォワードネットワークは機械学習の専門家にとって極めて重要である。これらは多くの重要な商用アプリケーションの基礎を形成する。例えば、写真からの物体認識で使われる畳み込みネットワークは特殊化された種類のフィードフォワードネットワークである。フィードフォワードネットワークはリカレントネットワークに至るための概念的な足がかり[stepping stone]である。これは多くの自然言語アプリケーションに力を与える。

フィードフォワードニューラルネットワークは、多数の様々な関数とともに構成することで一般に表現されるので、ネットワークと呼ばれる。そのモデルはどの関数がともに構成されるかを述べる有向非巡回グラフ[directed acyclic graph]と関連付けられる。例えば、$f(\boldsymbol{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\boldsymbol{x})))$を形成するために、鎖状に接続される3つの関数$f^{(1)}$、$f^{(2)}$、$f^{(3)}$があるかもしれない。これらの鎖構造は最も一般的に使われるニューラルネットワークの構造である。この場合、$f^{(1)}$はネットワークの第一層[first layer]と呼ばれ、$f^{(2)}$は第二層と呼ばれ、以下同文。その鎖の全体の長さはモデルの深さ[depth]を与える。「深層学習」という名前はこの用語法により生じる。フィードフォワードネットワークの最後の層は出力層[output layer]と呼ばれる。ニューラルネットワークの訓練中では、$f(\boldsymbol{x})$を$f^*(\boldsymbol{x})$と一致させる。訓練データは異なる訓練点で計算される$f^*(\boldsymbol{x})$のノイジーで近似的なexamplesを提供する。各example$\boldsymbol{x}$にはラベル$y \approx f^*(\boldsymbol{x})$が伴う。訓練examplesは出力層が各点$\boldsymbol{x}$でしなければならないことを直接指定する。すなわち、$y$に近い値を生成しなければならないということである。他の層の振る舞いは訓練データによって直接指定されない。学習アルゴリズムは望ましい出力を生成するためにこれらの層の使い方を決定しなければならないが、訓練データはそれぞれ別個の層がすべきことを語らない。代わりに、学習アルゴリズムは$f^*$の近似を最もうまく実装するためにこれらの層の使い方を決めなければならない。訓練データはこれらの層のそれぞれに対して望ましい出力を示さないので、これらは隠れ層[hidden layers]と呼ばれる。
最後に、これらのネットワークは、これらが恐らく神経科学に触発されるので、ニューラルと呼ばれる。ネットワークの各隠れ層は一般にベクトル値的である。これらの隠れ層の次元はモデルの幅[width]を定める。ベクトルの各要素はニューロンに類似した役割を担うと解釈できるだろう。この層を単一のベクトルからベクトルへの関数を表現するとみなすのではなく、並列にそれぞれがベクトルからスカラへの関数を表現するように振る舞う多くのユニット[units]から成るとみなすこともできる。各ユニットは多くの他のユニットから入力を受け取り、自身の活性化の値を計算するという意味でニューロンに似ている。ベクトル値的な表現の多数の層を用いるというアイデアは神経科学から着想を得ている[be drawn from]。これらの表現を計算するのに使われる関数$f^{(i)}(\boldsymbol{x})$の選択もまた生物学的なニューロンが計算する関数についての神経学的な見解によって大まかに導かれる。しかしながら、近代的なニューラルネットワーク研究は多くの数学および工学の分野によって導かれ、ニューラルネットワークの目標は脳を完璧にモデル化することではない。脳の関数のモデルとしてではなく、脳について知っていることからいくつかの洞察を時折得て、統計的な汎化を達成するために設計された関数近似の機械としてフィードフォワードネットワークをみなすことは最も適当である。
フィードフォワードネットワークを理解する1つの方法は、線形モデルから始めて、これらの制限を克服する方法を考えることである。ロジスティック回帰や線形回帰のような線形モデルは、閉形式で、または、凸最適化に、のいずれかで効率的かつ確実にフィットさせることができるので、魅力的である。線形モデルは、モデル容量が一次関数に制限されるので、そのモデルが2つの入力変数の間の相互作用を理解できない、という明らかな欠陥も持つ。
線形モデルを$\boldsymbol{x}$の非線形関数を表現するよう拡張するためには、$\boldsymbol{x}$自体ではなく変換した入力$\phi(\boldsymbol{x})$に線形モデルを適用することができる。ここで、$\phi$は非線形な変換である。等価的に、我々は、$\phi$マッピングを暗黙的に適応することに基づいた非線形な学習アルゴリズムを得るために、[@sec:5.7.2]で述べられるカーネルトリックを適用できる。我々は$\phi$を、$\boldsymbol{x}$を述べる特徴の集合をもたらす、または、$\boldsymbol{x}$に対する新しい表現をもたらすとみなすことができる。
故に、疑問となるのはマッピング$\phi$を選択する方法である。

1. 1つの選択肢は、RBFカーネルに基づくカーネルマシンで暗黙的に用いられる無限次元の$\phi$のような、非常に汎用的な$\phi$を用いることである。$\phi(\boldsymbol{x})$が十分に高次元であるならば、我々は常に訓練セットにフィットするのに十分な容量を持つが、テストセットへの汎化はしばしばpoorなままである。非常に汎用的な特徴マッピングは通常、局所的な平滑さの原則にのみ基づき、発展的な問題を解くための十分な事前情報をエンコードしない。
2. もう1つの選択肢は$\phi$を手作業で設計することである。深層学習の到来までは、これが支配的なアプローチであった。音声認識やコンピュータビジョンのような異なる分野に特化した専門家によって、分野間をほとんど移ったりせずに、別個のタスクごとに何十年の労力を必要とする。
3. 深層学習の戦略は$\phi$を学習することである。このアプローチでは、モデル$y = f(\boldsymbol{x}; \boldsymbol{\theta}, \boldsymbol{w}) = \phi(\boldsymbol{x}; \boldsymbol{\theta})^\top \boldsymbol{w}$を持つ。我々はいま、関数の広範囲のクラスから$\phi$を学習するのに使うパラメータ$\boldsymbol{\theta}$、および、$\phi(\boldsymbol{x})$から望ましい出力へマッピングするパラメータ$\boldsymbol{w}$を持つ。これは隠れ層を定義する$\phi$を持つ深層フィードフォワードネットワークの例である。このアプローチは3つの中で唯一、訓練問題の凸性を断念するが、その益は害を上回る。このアプローチでは、$\phi(\boldsymbol{x}; \boldsymbol{\theta})$として表現をパラメータ化し、良い表現に対応する$\boldsymbol{\theta}$をもとめるために最適化アルゴリズムを用いる。望むならば、このアプローチは高度に汎用的にする、すなわち、非常に広範囲な仲間$\phi(\boldsymbol{x}; \boldsymbol{\theta})$を用いることでそれを行うことによって1番目のアプローチの利点を獲得できる。深層学習は二番目のアプローチの利点を獲得することもできる。人間の専門家はうまく処理するであろうと期待する仲間$\phi(\boldsymbol{x}; \boldsymbol{\theta})$を設計することによって汎化を助けるための知識をエンコードできる。この利点は人間の設計者が、正確に正しい関すを求めるのではなく、正しい一般的な関数の仲間を求める必要があるだけであるということである。

特徴を学習することによってモデルを改良するためのこの一般的な原則はこの章で述べられるフィードフォワードネットワーク以上に拡張される。これは本書を通して述べられるすべての種類のモデルに適用される深層学習の繰り返されるテーマである。フィードフォワードネットワークはフィードバック接続を欠いた$\boldsymbol{x}$から$\boldsymbol{y}$への決定論的なマッピングの学習に対してこの原則の応用である。後に示される他のモデルは確率的なマッピング、フィードバックを伴う関数、および、単一のベクトル上の確率分布の学習に対してこれらの原則を適用する。
我々は本章をフィードフォワードネットワークの単純な例から始める。次に、***
