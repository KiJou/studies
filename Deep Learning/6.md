# 深層フィードフォワードネットワーク

フィードフォワードニューラルネットワーク[feedforward neural networks]とか多層パーセプトロン[multilayer perceptrons]（MLPs）とも呼ばれる、深層フィードフォワードネットワーク[deep feedforward networks]は典型的な深層学習モデルである。フィードフォワードネットワークの目標はある関数$f^*$を近似することである。例えば、分類器に対して、$y = f^*(\boldsymbol{x})$は入力$\boldsymbol{x}$をカテゴリ$y$に対応させる。フィードフォワードネットワークはマッピング$\boldsymbol{y} = f(\boldsymbol{x}; \boldsymbol{\theta})$を定義し、最良の関数近似となるパラメータ$\boldsymbol{\theta}$の値を学習する。
これらのモデルは、$\boldsymbol{x}$から計算される関数を通り、$f$を定義するのに使われる途中計算を通り、最終的に出力$\boldsymbol{y}$へ情報が流れるので、フィードフォワード[feedforward]と呼ばれる。モデルの出力がそれ自体に戻されるフィードバック[feedback]接続は存在しない。フィードフォワードニューラルネットワークがフィードバック接続を含むよう拡張されるとき、これらは、[@sec:10]で示されるように、リカレントニューラルネットワーク[recurrent neural networks]と呼ばれる。
フィードフォワードネットワークは機械学習の専門家にとって極めて重要である。これらは多くの重要な商用アプリケーションの基礎を形成する。例えば、写真からの物体認識で使われる畳み込みネットワークは特殊化された種類のフィードフォワードネットワークである。フィードフォワードネットワークはリカレントネットワークに至るための概念的な足がかり[stepping stone]である。これは多くの自然言語アプリケーションに力を与える。

フィードフォワードニューラルネットワークは、多数の様々な関数とともに構成することで一般に表現されるので、ネットワークと呼ばれる。そのモデルはどの関数がともに構成されるかを述べる有向非巡回グラフ[directed acyclic graph]と関連付けられる。例えば、$f(\boldsymbol{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\boldsymbol{x})))$を形成するために、鎖状に接続される3つの関数$f^{(1)}$、$f^{(2)}$、$f^{(3)}$があるかもしれない。これらの鎖構造は最も一般的に使われるニューラルネットワークの構造である。この場合、$f^{(1)}$はネットワークの第一層[first layer]と呼ばれ、$f^{(2)}$は第二層と呼ばれ、以下同文。その鎖の全体の長さはモデルの深さ[depth]を与える。「深層学習」という名前はこの用語法により生じる。フィードフォワードネットワークの最後の層は出力層[output layer]と呼ばれる。ニューラルネットワークの訓練中では、$f(\boldsymbol{x})$を$f^*(\boldsymbol{x})$と一致させる。訓練データは異なる訓練点で計算される$f^*(\boldsymbol{x})$のノイジーで近似的なexamplesを提供する。各example$\boldsymbol{x}$にはラベル$y \approx f^*(\boldsymbol{x})$が伴う。訓練examplesは出力層が各点$\boldsymbol{x}$でしなければならないことを直接指定する。すなわち、$y$に近い値を生成しなければならないということである。他の層の振る舞いは訓練データによって直接指定されない。学習アルゴリズムは望ましい出力を生成するためにこれらの層の使い方を決定しなければならないが、訓練データはそれぞれ別個の層がすべきことを語らない。代わりに、学習アルゴリズムは$f^*$の近似を最もうまく実装するためにこれらの層の使い方を決めなければならない。訓練データはこれらの層のそれぞれに対して望ましい出力を示さないので、これらは隠れ層[hidden layers]と呼ばれる。
最後に、これらのネットワークは、これらが恐らく神経科学に触発されるので、ニューラルと呼ばれる。ネットワークの各隠れ層は一般にベクトル値的である。これらの隠れ層の次元はモデルの幅[width]を定める。ベクトルの各要素はニューロンに類似した役割を担うと解釈できるだろう。この層を単一のベクトルからベクトルへの関数を表現するとみなすのではなく、並列にそれぞれがベクトルからスカラへの関数を表現するように振る舞う多くのユニット[units]から成るとみなすこともできる。各ユニットは多くの他のユニットから入力を受け取り、自身の活性化の値を計算するという意味でニューロンに似ている。ベクトル値的な表現の多数の層を用いるというアイデアは神経科学から着想を得ている[be drawn from]。これらの表現を計算するのに使われる関数$f^{(i)}(\boldsymbol{x})$の選択もまた生物学的なニューロンが計算する関数についての神経学的な見解によって大まかに導かれる。しかしながら、近代的なニューラルネットワーク研究は多くの数学および工学の分野によって導かれ、ニューラルネットワークの目標は脳を完璧にモデル化することではない。脳の関数のモデルとしてではなく、脳について知っていることからいくつかの洞察を時折得て、統計的な汎化を達成するために設計された関数近似の機械としてフィードフォワードネットワークをみなすことは最も適当である。
フィードフォワードネットワークを理解する1つの方法は、線形モデルから始めて、これらの制限を克服する方法を考えることである。ロジスティック回帰や線形回帰のような線形モデルは、閉形式で、または、凸最適化に、のいずれかで効率的かつ確実にフィットさせることができるので、魅力的である。線形モデルは、モデル容量が一次関数に制限されるので、そのモデルが2つの入力変数の間の相互作用を理解できない、という明らかな欠陥も持つ。
線形モデルを$\boldsymbol{x}$の非線形関数を表現するよう拡張するためには、$\boldsymbol{x}$自体ではなく変換した入力$\phi(\boldsymbol{x})$に線形モデルを適用することができる。ここで、$\phi$は非線形な変換である。等価的に、我々は、$\phi$マッピングを暗黙的に適用することに基づいた非線形な学習アルゴリズムを得るために、[@sec:5.7.2]で述べられるカーネルトリックを適用できる。我々は$\phi$を、$\boldsymbol{x}$を述べる特徴の集合をもたらす、または、$\boldsymbol{x}$に対する新しい表現をもたらすとみなすことができる。
故に、疑問となるのはマッピング$\phi$を選択する方法である。

1. 1つの選択肢は、RBFカーネルに基づくカーネルマシンで暗黙的に用いられる無限次元の$\phi$のような、非常に汎用的な$\phi$を用いることである。$\phi(\boldsymbol{x})$が十分に高次元であるならば、我々は常に訓練セットにフィットするのに十分な容量を持つが、テストセットへの汎化はしばしばpoorなままである。非常に汎用的な特徴マッピングは通常、局所的な平滑さの原則にのみ基づき、発展的な問題を解くための十分な事前情報をエンコードしない。
2. もう1つの選択肢は$\phi$を手作業で設計することである。深層学習の到来までは、これが支配的なアプローチであった。音声認識やコンピュータビジョンのような異なる分野に特化した専門家によって、分野間をほとんど移ったりせずに、別個のタスクごとに何十年の労力を必要とする。
3. 深層学習の戦略は$\phi$を学習することである。このアプローチでは、モデル$y = f(\boldsymbol{x}; \boldsymbol{\theta}, \boldsymbol{w}) = \phi(\boldsymbol{x}; \boldsymbol{\theta})^\top \boldsymbol{w}$を持つ。我々はいま、関数の広範囲のクラスから$\phi$を学習するのに使うパラメータ$\boldsymbol{\theta}$、および、$\phi(\boldsymbol{x})$から望ましい出力へマッピングするパラメータ$\boldsymbol{w}$を持つ。これは隠れ層を定義する$\phi$を持つ深層フィードフォワードネットワークの例である。このアプローチは3つの中で唯一、訓練問題の凸性を断念するが、その益は害を上回る。このアプローチでは、$\phi(\boldsymbol{x}; \boldsymbol{\theta})$として表現をパラメータ化し、良い表現に対応する$\boldsymbol{\theta}$をもとめるために最適化アルゴリズムを用いる。望むならば、このアプローチは高度に汎用的にする、すなわち、非常に広範囲な仲間$\phi(\boldsymbol{x}; \boldsymbol{\theta})$を用いることでそれを行うことによって1番目のアプローチの利点を獲得できる。深層学習は二番目のアプローチの利点を獲得することもできる。人間の専門家はうまく処理するであろうと期待する仲間$\phi(\boldsymbol{x}; \boldsymbol{\theta})$を設計することによって汎化を助けるための知識をエンコードできる。この利点は人間の設計者が、正確に正しい関すを求めるのではなく、正しい一般的な関数の仲間を求める必要があるだけであるということである。

特徴を学習することによってモデルを改良するためのこの一般的な原則はこの章で述べられるフィードフォワードネットワーク以上に拡張される。これは本書を通して述べられるすべての種類のモデルに適用される深層学習の繰り返されるテーマである。フィードフォワードネットワークはフィードバック接続を欠いた$\boldsymbol{x}$から$\boldsymbol{y}$への決定論的なマッピングの学習に対してこの原則の応用である。後に示される他のモデルは確率的なマッピング、フィードバックを伴う関数、および、単一のベクトル上の確率分布の学習に対してこれらの原則を適用する。
我々は本章をフィードフォワードネットワークの単純な例から始める。次に、フィードフォワードネットワークを配備するために必要な設計上の決定のそれぞれを扱う。第一に、フィードフォワードネットワークを訓練することは線形モデルに対して必要なものと同じ設計上の決定の多く、すなわち、オプティマイザ、コスト関数、出力ユニットの形式を選択することを行う必要がある。我々はこれらの勾配ベースの学習の基礎を再調査し、続けて、フィードフォワードネットワークに固有の設計上の決定をのいくつかと突き合わせる。フィードフォワードネットワークが隠れ層の概念を導入したことで、我々は隠れ層の値を計算するのに使われるであろう活性化関数[activation functions]を選ぶ必要がある。また、ネットワークがどれだけの層を含むべきか、それらの層が互いにどのように接続すべきか、各層にどれだけのユニットがあるべきか、を含めて、ネットワークのアーキテクチャも設計しなければならない。深層ニューラルネットワークにおける学習は複雑な関数の勾配を計算する必要がある。我々は、これらの勾配を効率的に計算するために使うことができる、逆伝播[back-propagation]アルゴリズムとその近代的な一般化を示す。最後に、いくつかの歴史的な視点で閉める。

## 例：XORの学習

フィードフォワードネットワークのアイデアをより具体的にするため、我々は非常に単純なタスクに関する完全に機能するフィードフォワードネットワークの例、すなわち、XOR関数の学習から始める。
XOR（排他的論理和）関数は2つのバイナリ値$x_1$と$x_2$に関する演算である。これらのバイナリ値の1つだけが$1$に等しいとき、XOR関数は$1$を返す。そうでなければ、$0$を返す。XOR関数は学習したい目的関数$y = f^*(\boldsymbol{x})$をもたらす。我々のモデルは関数$y = f(\boldsymbol{x}; \boldsymbol{\theta})$をもたらし、我々の学習アルゴリズムは$f^*$にできるだけ似た$f$を作るためにパラメータ$\boldsymbol{\theta}$を適合させる。
この単純な例では、統計的な一般化について考えさせられることはないだろう。我々はネットワークが4つの点$\mathbb{X} = {[0, 0]^\top, [0, 1]^\top, [1, 0]^\top, \text{ and } [1, 1]^\top}$に関して正確に処理することを欲する。我々はこれらの点の4つすべてでネットワークを訓練するだろう。唯一の課題は訓練セットをフィットさせることのみである。
我々はこの問題を回帰問題として扱い、平均二乗誤差の損失関数を用いることができる。我々はこの例に対する計算をできるだけ単純化しようとしてこの損失関数を選んだ。実践的なアプリケーションでは、MSEは通常ではバイナリデータのモデル化に対して適切なコスト関数ではない。より適切なアプローチは[@6.2.2.2]で述べられる。
訓練セット全体で計算を行うとすると、MSEの損失関数は以下となる。

$$
J(\boldsymbol{\theta}) = \frac{1}{4} \sum_{\boldsymbol{x} \in \mathbb{X}} (f^*(\boldsymbol{x}) - f(\boldsymbol{x}; \boldsymbol{\theta}))^2
$$

すると、我々はモデルの形式$f(\boldsymbol{x}; \boldsymbol{\theta})$を選択しなければならない。$\boldsymbol{w}$と$b$から成る$\boldsymbol{\theta}$を持つ線形モデルを選択するとしよう。我々のモデルは以下であると定義される。

$$
f(\boldsymbol{x}; \boldsymbol{w}, b) = \boldsymbol{x}^\top \boldsymbol{w} + b
$$

我々は正規方程式を用いて$\boldsymbol{w}$と$b$に関する閉形式で$J(\boldsymbol{\theta})$を最小化できる。
正規方程式を解くと、$\boldsymbol{w} = \boldsymbol{0}$と$b = \frac{1}{2}$を得る。この線形モデルはいたるところで$0.5$を出力するだけである。なぜこんなことが起こるのだろう？[@fig:6.1]はなぜ線形モデルがXOR関数を表現できないかを示す。この問題を解決する1つの方法は線形モデルが解を表現できるような異なる特徴空間を学習するモデルを用いることである。

![表現を学習することでXOR問題を解く。プロット上に表示される太字の数字は学習した関数が各点で出力しなければならない値を示す。（左）元の値に直接適用した線形モデルはXOR関数を実装できない。$x_1 = 0$のとき、モデルの出力は$x_2$が増加するに従って増加しなければならない。$x_1 = 1$のとき、モデルの出力は$x_2$が増加するに従って減少しなければならない。線形モデルは固定の係数$w_2$を$x_2$に適用しなければならない。従って、その線形モデルは$x_2$に関する係数を変化させるために$x_1$の値を用いることができず、この問題を解くことができない。（右）ニューラルネットワークによって抽出された特徴で表現される変換した空間では、線形モデルはその問題を解くことができる。我々の解法の例では、$1$を出力しなければならない2つの点は特徴空間において単一の点に潰されてしまっている。別の言い方をすれば、その非線形な特徴は$\boldsymbol{x} = [1, 0]^\top$と$\boldsymbol{x} = [0, 1]^\top$の両方を特徴空間における単一の点$\boldsymbol{h} = [1, 0]^\top$にマッピングさせた。その線形モデルは$h_1$における増加と$h_2$における減少として関数を記述できる。この例では、特徴空間を学習することに対する動機は訓練セットにフィットできるようにモデル容量をより大きくすることのみである。より現実的なアプリケーションでは、学習した表現もまたモデルが汎化するのに役立ち得る。](fig/6-1.png){#fig:6.1}

具体的に言うと、我々は2つの隠れユニットを含む1つの隠れ層を持つ単純なフィードフォワードネットワークを導入するだろう。このモデルの図は[@fig:6.2]を参照のこと。このフィードフォワードネットワークは関数$f^{(1)}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c})$によって計算される隠れユニットのベクトル$\boldsymbol{h}$を持つ。そして、これらの隠れユニットの値は第二層に対する入力として使われる。第二層はネットワークの出力層である。出力層は依然として単なる線形回帰モデルであるが、いまや$\boldsymbol{x}$ではなく$\boldsymbol{h}$に適用されている。ネットワークはともに連鎖する2つの関数$\boldsymbol{h} = f^{(1)}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c})$と$f^{(2)}(\boldsymbol{h}; \boldsymbol{w}, b)$を含み、完全なモデルは$f(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c}, \boldsymbol{w}, b) = f^{(2)}(f^{(1)}(\boldsymbol{x}))$となる。

![2つの異なるスタイルで描かれるフィードフォワードネットワークの例。具体的に、これはXORの例を解くのに使われるフィードフォワードネットワークである。これは2つのユニットを含む単一の隠れ層を持つ。（左）このスタイルでは、すべてのユニットをグラフにおけるノードとして描く。このスタイルは明示的で曖昧さがないが、この例より大きなネットワークに対しては、場所を取りすぎる可能性がある。（右）このスタイルでは、層の活性化を表現するベクトル全体ごとにグラフにおけるノードを描く。このスタイルは更にコンパクトである。時折、我々はこのグラフの辺に2つの層の間の関係を記述するパラメータの名前で注釈を付ける。ここでは、行列$\boldsymbol{W}$が$\boldsymbol{x}$から$\boldsymbol{h}$へのマッピングを記述し、ベクトル$\boldsymbol{w}$が$\boldsymbol{h}$から$y$へのマッピングを記述することを示す。我々は一般に、この種の図をラベル付けするとき、各層に関連付けられる切片パラメータを省略する。](fig/6-2.png){#fig:6.2}

$f^{(1)}$が計算すべき関数とは何だろうか？線形モデルはこれまで我々の役に立ってきたので、$f^{(1)}$を同様に線形にしたくなるかもしれない。残念ながら、$f^{(1)}$が線形であったならば、全体としてのフィードフォワードネットワークはその入力の一次関数をそのままにしただろう。今のところは切片項を無視して、$f^{(1)}(\boldsymbol{x}) = \boldsymbol{W}^\top \boldsymbol{x}$および$f^{(2)}(\boldsymbol{h}) = \boldsymbol{h}^\top \boldsymbol{w}$であるとする。故に、$f(\boldsymbol{x}) = \boldsymbol{x}^\top \boldsymbol{W} \boldsymbol{w}$である。我々は$\boldsymbol{w}' = \boldsymbol{W} \boldsymbol{w}$であるところの$f(\boldsymbol{x}) = \boldsymbol{x}^\top \boldsymbol{w}'$としてこの関数を表現できるだろう。
明らかに、我々は特徴を記述するための非線形な関数を用いなければならない。ほとんどのニューラルネットワークは学習したパラメータによって制御され、活性化関数と呼ばれる固定の非線形関数へと続くアフィン変換を用いてこれを行う。$\boldsymbol{h} = g(\boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{c})$を定義することによって、ここではその戦略を用いる。ここで、$\boldsymbol{W}$は線形変換の重みをもたらし、$\boldsymbol{c}$はバイアスをもたらす。以前に、線形回帰モデルを記述するため、入力ベクトルから出力スカラへのアフィン変換を記述するための重みのベクトルとスカラのバイアスのパラメータを用いた。いま、我々はベクトル$\boldsymbol{x}$からベクトル$\boldsymbol{h}$へのアフィン変換を記述するので、バイアスパラメータのベクトル全体が必要である。活性化関数$g$は一般に$h_i = g(\boldsymbol{x}^\top \boldsymbol{W}_{:,i} + c_i)$を用いて要素ごとに適用される関数となるように選ばれる。近代的なニューラルネットワークでは、既定のオススメは、[@fig:6.3]に図示される、$g(z) = \max{0, z}$によって定義される正規化線形ユニット[rectified linear unit]またはReLU[@Jarrett2009; @Nair2010; @Glorot2011a]を用いることである。

![正規化線形[rectified linear]活性化関数。この活性化関数はほとんどのフィードフォワードニューラルネットワークで使用を推奨される既定の活性化関数である。線形変換の出力にこの関数を適用することは非線形変換を生み出す。しかし、その関数は2つの線形な区分を持つ線形区分関数であると言う意味において非常に線形に近い状態を維持する。正規化線形ユニットはほぼ線形であるので、線形モデルが勾配ベースの手法で最適化されることを容易にする特性の多くを保持する。線形モデルをうまく汎化させる特性の多くも保持する。計算機科学全体に共通の原則とは最小限の構成要素から複雑なシステムを作ることができるということである。0か1を格納できることだけが必要なチューリングマシンのメモリと同じように、正規化線形関数から普遍的関数近似器を作ることができる。](fig/6-3.png){#fig:6.3}

我々は以下として完全なネットワークを指定できる。

$$
f(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c}, \boldsymbol{w}, b) = \boldsymbol{w}^\top \max{0, \boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{c}} + b
$$

故に、我々はXOR問題への解を指定できる。以下であるとする。

$$
\boldsymbol{W} = \left[ \begin{matrix}
1 & 1 \\
1 & 1
\end{matrix} \right]
$$

$$
\boldsymbol{c} = \left[ \begin{matrix}
0 \\
1
\end{matrix} \right]
$$

$$
\boldsymbol{w} = \left[ \begin{matrix}
1 \\
-2
\end{matrix} \right]
$$

そして、$b = 0$である。
すると、モデルが入力のバッチをどのように処理するかを見て回ることができる。$\boldsymbol{X}$を、各行に1つのexampleを持ち、バイナリの入力空間における4つすべての点を含む計画行列であるとする。

$$
\boldsymbol{X} = \left[ \begin{matrix}
0 & 0 \\
0 & 1 \\
1 & 0 \\
1 & 1
\end{matrix} \right]
$$

ニューラルネットワークにおける第1ステップは第一層の重み行列を入力行列に乗算することである。

$$
\boldsymbol{XW} = \left[ \begin{matrix}
0 & 0 \\
1 & 1 \\
1 & 1 \\
2 & 2
\end{matrix} \right]
$$

次に、以下をえるためにバイアスベクトル$\boldsymbol{c}$を加算する。

$$
\left[ \begin{matrix}
0 & -1 \\
1 & 0 \\
1 & 0 \\
2 & 1
\end{matrix} \right]
$$

この空間では、examplesすべては傾斜$1$の線に沿って存在する。我々はこの線にそって移動するので、出力は$0$で始まり、$1$に上がり、そして、$0$に下がる必要がある。線形モデルはこのような関数を実装できない。exampleごとの$\boldsymbol{h}$の値を計算し終えるため、我々は正規化された[rectified]線形変換を適用する。

$$
\left[ \begin{matrix}
0 & 0 \\
1 & 0 \\
1 & 0 \\
2 & 1
\end{matrix} \right]
$$

この変換はexamples間の関係を変化させてしまう。これらはもはや単一の直線上に存在しない。[@fig:6.1]に示される通り、これらは、線形モデルがこの問題を解くことができるところの空間に置かれる。
重みベクトル$\boldsymbol{w}$を乗算して終いとする。

$$
\left[ \begin{matrix}
0 \\
1 \\
1 \\
0
\end{matrix} \right]
$$

ニューラルネットワークはバッチにおけるすべてのexampleに対する正しい答えを得た。
この例では、我々は単純に解を指定し、そして、誤差ゼロを得ることを示した。現実の状況では、無数のモデルパラメータと無数の訓練examplesがあるかもしれず、そのため、ここで行ったように解を推測できない。代わりに、勾配ベースの最適化アルゴリズムは非常に小さな誤差を生成するパラメータを求めることができる。XOR問題に対して示した解は損失関数の最小におけるものであり、そのため、勾配降下法はこの点に収束できるだろう。勾配降下法が求めることもできるであろうXOR問題に対する他の等価的な解が存在する。勾配降下法の収束点はパラメータの初期値に依存する。実践では、勾配降下法は、ここで示したもののような、明確で容易に理解できる整数値の解を求めることは通常ではないだろう。

## 勾配ベース学習

ニューラルネットワークを設計および訓練することは勾配降下法を伴う他のいずれかの機械学習モデルを訓練するのとそこまで違いがない。[@sec:5.10]では、最適化の手順、コスト関数、モデルの族を指定することで機械学習アルゴリズムを作る方法を述べた。
これまでに見てきた線形モデルとニューラルネットワークとの最も大きな違いはニューラルネットワークの非線形性が最も関心のある損失関数を非凸にしてしまうことである。これはニューラルネットワークが通常では、ロジスティック回帰またはSVMsを訓練するのに使われる大域収束性の保証を持つ線形回帰モデルまたは凸最適化アルゴリズムを訓練するのに使われる一次方程式ソルバではなく、単にコスト関数を非常に低い値にするだけの反復的で勾配ベースのオプティマイザを用いることで訓練されることを意味する。凸最適化は任意の食パラメータから開始して収束する（理論的にはそうだが、実践ではロバストだが数値的な問題に遭遇する可能性がある）。非凸な損失関数に適用される確率的勾配降下法はそのような収束保証を持たず、初期パラメータの値に敏感である。フィードフォワードニューラルネットワークにとって、すべての重みを小さなランダム値に初期化することは重要である。フィードフォワードネットワークや他のほとんどすべての深層モデルを訓練するのに使われる反復的な勾配ベースの最適化アルゴリズムは[@sec:8]にてその詳細を述べ、それとともに、特にパラメータの初期化を[@sec:8.4]で述べる。今のところは、訓練アルゴリズムがほとんど常にあれやこれやでコスト関数を降下するために勾配を用いることに依存していると理解するので十分である。[@sec:4.3]で導入される特定のアルゴリズムは勾配降下法のアイデアの改良および洗練したものであり、より具体的に言えば、[@sec:5.9]で導入されるものはほとんどの場合の確率的勾配降下法の改良である。
もちろん、我々は線形回帰、勾配降下法を伴うサポートベクトルマシンのようなモデルも訓練することができる。実際に、これは訓練セットが極めて大きいときに一般的である。この視点から、ニューラルネットワークを訓練することは他のいずれのモデルを訓練することともほとんど違いがない。勾配を計算することはニューラルネットワークに対する方が若干複雑であるが、依然として効率的かつ厳密に行うことができる。[@sec:6.5]では、逆伝播アルゴリズムおよび逆伝播アルゴリズムを一般化した近代的なものを用いて勾配を得る方法を述べる。
他の機械学習モデルと同様に、勾配ベースの学習を適用するため、我々はコスト関数を選択しなければならず、モデルの出力を表現する方法を選択しなければならない。我々はニューラルネットワークのシナリオに特に重点をおいてこれらの設計上の懸念事項を再訪する。

### コスト関数

深層ニューラルネットワークの設計の重要な側面はコスト関数の選択である。ニューラルネットワークに対するコスト関数は、線形モデルのような、他のパラメトリックモデルに対するコスト関数と多かれ少なかれ同じである。
ほとんどの場合、我々のパラメトリックモデルは分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$を定義し、我々は単純に最大尤度の原則を用いる。これは我々が訓練データとコスト関数としてのモデルの予測値の間の交差エントロピーを用いることを意味する。
時折、我々はより単純なアプローチを取る。そこでは、$\boldsymbol{y}$上の完全な確率分布を予測するのではなく、$\boldsymbol{x}$で条件付けられた$\boldsymbol{y}$のある統計量を単に予測するだけである。特殊化された損失関数はこれらの推定値の予測器を訓練することを可能にする。
ニューラルネットワークを訓練するのに使われる総コスト関数はしばしば、正則化項を持つここで述べたprimary cost functionsのひとつを組み合わせるだろう。我々は[@sec:5.2.2]で線形モデルに適用される正規化の単純な例のいくつかをすでに見てきている。線形モデルに使われるweight decayアプローチもまた深層ニューラルネットワークに直接適用可能であり、最も人気のある正則化戦略に入る。ニューラルネットワークに対するより発展的な正則化戦略は[@sec:7]で述べられる。

#### 最大尤度を伴う条件付き分布の学習

最も近代的なニューラルネットワークは最大尤度を用いて訓練される。これはコスト関数が単純に負の対数尤度であり、訓練データとモデル分布の間の交差エントロピーとして等価的に述べられることを意味する。

$$
J(\boldsymbol{\theta}) = -\mathbb{E}_{\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}} \sim \hat{p}_{data}} \log p_{model}(\boldsymbol{y} | \boldsymbol{x})
$$

コスト関数の特定の形式は、$\log p_{model}$の特定の形式に依存して、モデルからモデルへと変化する。上記の式の展開は一般にモデルパラメータに依存せず、破棄できるだろういくつかの項を生み出す。例えば、[@sec:5.5.1]で見た通り、$p_{model}(\boldsymbol{y} | \boldsymbol{x}) = \mathcal{N}(\boldsymbol{y}; f(\boldsymbol{x}; \boldsymbol{\theta}), \boldsymbol{I})$であるならば、$\frac{1}{2}$のスケーリングファクタと$\boldsymbol{\theta}$に依存しない項を除いて[up to]、以下の平均二乗誤差のコストを復元する。

$$
J(\theta) = \frac{1}{2} \mathbb{E}_{\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}} \sim \hat{p}_{data}} \| \boldsymbol{y} - f(\boldsymbol{x}; \boldsymbol{\theta}) \|^2 + \text{const}
$$

破棄される定数は、この場合ではパラメータ化しないことを選択した、ガウス分布の分散に基づいている。以前に、我々は出力分布を伴う最尤推定と平均二乗誤差の最小化の間の等価性が線形モデルに対して成り立つことを確認したが、実際には、その等価性はガウシアンの平均を予測するのに使われる$f(\boldsymbol{x}; \boldsymbol{\theta})$にも関わらず成り立つ。
最大尤度からコスト関数を導出するというこのアプローチの利点はモデルごとにコスト関数を設計することの負荷を取り除くことである。モデル$p(\boldsymbol{y} | \boldsymbol{x})$を指定することはコスト関数$\log p(\boldsymbol{y} | \boldsymbol{x})$を自動的に決定する。
ニューラルネットワーク全体で繰り返されるひとつのテーマは、コスト関数の勾配が学習アルゴリズムに対する良き導き手として役立つのに十分大きくかつ予測可能でなければならないということである。飽和する（非常に平坦になる）関数は、勾配を非常に小さくするので、この目的を損なう。多くの場合、これは隠れユニットや出力ユニットの出力を生成するのに使われる活性化関数が飽和するために発生する。負の対数尤度は多くのモデルに対してこの問題を回避するのに役立つ。いくつかの出力ユニットは引数が非常に大きな負であるときに飽和し得る$\exp$関数を伴う。負の対数尤度のコスト関数における$\log$関数はいくつかの出力ユニットの$\exp$をもとに戻す。我々は[@sec:6.2.2]でコスト関数と出力ユニットの選択との相互作用を考察するだろう。
最尤推定を行うのに使われる交差エントロピーのコストの普通でない特性のひとつは実践で一般に使われるモデルへ適用されるときに最小値を通常では持たないことである。離散的な出力変数に対して、ほとんどのモデルはそのような0または1の確率を表現できないが、任意に近いところを表現できる方法でパラメータ化される。ロジスティック回帰はそのようなモデルの例である。実数値の出力変数に対して、（例えば、出力のガウス分布の分散パラメーtあを学習することで）モデルが出力分布の密度を制御できるならば、正しい訓練セットの出力に極めて高密度を割り当てることができるようになり、結果として交差エントロピーが負の無限大に近づく。[@sec:7]で述べられる正則化技術は学習問題を修正するいくつかの異なる方法をもたらす。すなわち、モデルはこの方法においてreap unlimited rewardことができない。

#### 条件付き統計量の学習

完全な確率分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$を学習する代わりに、我々はしばしば$\boldsymbol{x}$を与えられたときの$\boldsymbol{y}$のただ1つの条件付き統計量を学習したい。
例えば、$\boldsymbol{y}$の平均を予測するために採用することを望む予測器$f(\boldsymbol{x}; \boldsymbol{\theta})$を持つかもしれない。
我々が十分に強力なニューラルネットワークを用いるならば、ニューラルネットワークを関数の幅広いクラスから任意の関数$f$を表現できるとみなすことができる。それとともに、このクラスは、特定のパラメトリックな形式持つことによってではなく、連続性や有界性のような特徴によってのみ制限される。この視点から、我々はコスト関数を単なる関数ではなく汎関数[functional]であるとみなすことができる。汎関数は関数から実数値へのマッピングである。故に、我々は学習を、パラメータの集合を選ぶだけではなく、関数を選択することと考えることができる。我々はその最小値をある望ましい特定の関数で発生させるためにコスト汎関数を設計できる。例えば、その最小値が$\boldsymbol{x}$を$\boldsymbol{x}$を与えられたときの$\boldsymbol{y}$の期待値にマッピングする関数にあるようにするためのコスト汎関数を設計できる。関数に関して最適化問題を解くことは、[@sec:19.4.2]で述べられる、変分法[calculus of variations]と呼ばれる数学上の道具を必要とする。この章の内容を理解するのに変分法を理解する必要はない。今のところは、変分法が以下の2つの結果を導くのに使割れるであろうことを理解するひつようがあるだけである。
変分法を用いて導出される1つ目の結果は、以下の最適化問題を解決することが、

$$
f^* = \argmin_f \mathbb{E}_{\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}} \sim p_{data}} \| \boldsymbol{y} - f(\boldsymbol{x}) \|^2
$$

最適化したいクラスの内にこの関数がある限り、以下を生み出すことである。

$$
f^*(\boldsymbol{x}) = \mathbb{E}_{\boldsymbol{\mathbf{y}} \sim p_{data}(\boldsymbol{y} | \boldsymbol{x})} [\boldsymbol{y}]
$$

言い換えれば、我々が真のデータ生成分布からの無限に多くのサンプルで訓練できたならば、平均二乗誤差のコスト関数を最小化することは$\boldsymbol{x}$の値ごとの$\boldsymbol{y}$の平均を予測する関数を与えただろう。
異なるコスト関数は異なる統計量を与える。変分法を用いて導出される2つ目の結果は、以下の関数は$\boldsymbol{x}$ごとの$\boldsymbol{y}$の中間値を予測する関数を生み出す。ただし、そのような関数が最適化したい関数の族によって記述され得るであろう場合に限る。

$$
f^* = \argmin_f \mathbb{E}_{\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}} \sim p_{data}} \| \boldsymbol{y} - f(\boldsymbol{x}) \|_1
$$

このコスト関数は平均絶対誤差[mean absolute error]と一般に呼ばれる。
残念ながら、平均二乗誤差と平均絶対誤差はしばしば、勾配ベースの最適化で用いられるときにまずい結果を引き起こす。いくつかの飽和する出力ユニットは、これらのコスト関数と組み合わさると非常に小さな勾配を生成する。これは、交差エントロピーのコスト関数の方が、分布$p(\boldsymbol{y} | \boldsymbol{x})$の全体を推定する必要がないときでさえも、平均二乗誤差や平均絶対誤差より人気である理由のひとつである。

### 出力ユニット

コスト関数の選択は出力ユニットの選択と強く結びついている。大抵の場合、我々は単純にデータ分布とモデル分布の間の交差エントロピーを用いる。従って、出力を表現する方法の選択は交差エントロピー関数の形式を決定する。
出力として使っても良いいずれの種類のニューラルネットワークユニットも隠れ層として用いることができる。ここでは、モデルの出力としてこれらのユニットを用いることに焦点を当てるが、原則として、これらは同様に内部的に用いることができる。我々は[@sec:6.3]で隠れユニットとしてこれらを用いることに関する追加の詳細とともにこれらのユニットを再訪する。
本節全体を通して、我々はフィードフォワードネットワークが$\boldsymbol{h} = f(\boldsymbol{x}; \boldsymbol{\theta})$で定義される隠れ特徴の集合をもたらすとする。故に、出力層の役割はネットワークが処理しなければならないタスクを完了するための特徴からいくつかの追加の変換を提供することである。

#### ガウシアン出力分布に対する線形ユニット

単純な種類の出力ユニットのひとつは非線形性を持たないアフィン変換に基づいている。これらはしばしば単に線形ユニットと呼ばれる。
特徴$\boldsymbol{h}$を与えられた場合、線形な出力ユニットの層はベクトル$\hat{\boldsymbol{y}} = \boldsymbol{W}^\top \boldsymbol{h} + \boldsymbol{b}$を生成する。
線形な出力層はしばしば条件付きガウス分布の平均を生成するのに使われる。

$$
p(\boldsymbol{y} | \boldsymbol{x}) = \mathcal{N}(\boldsymbol{y}; \hat{\boldsymbol{y}}, \boldsymbol{I})
$$

故に、対数尤度を最大化することは平均二乗誤差を最小化することと等価である。
最大尤度のフレームワークはガウシアンの共分散を学習すること、または、ガウシアンの共分散を入力の関数とすることを簡単にする。しかし、共分散はすべての入力に対して正定値行列であるように制約しなければならない。線形な出力層ではそのような制約を満足させるのは難しく、そのために、一般的には他の出力ユニットが共分散をパラメータ化するために使われる。共分散のモデル化へのアプローチは、[@sec:6.2.2.4]にて、手短に述べられる。
線形ユニットは飽和しないので、これらは勾配ベースの最適化アルゴリズムにそこまでの困難をもたらさず、多種多様な最適化アルゴリズムで使われるだろう。

#### ベルヌーイ出力分布に対するシグモイドユニット

多くのタスクは二値変数$y$の値を予測する必要がある。2つのクラスを伴う分類問題はこの形式に当てはめることができる。最大尤度のアプローチは$\boldsymbol{x}$で条件付けられた$\boldsymbol{y}$上のベルヌーイ分布を定義することである。
ベルヌーイ分布は単なる単一の数値によって定義される。ニューラルネットは$P(y = 1 | \boldsymbol{x})$のみを予測する必要がある。この数値が有効な確率であるためには、区間$[0, 1]$になければならない。
この制約を満たすことはある慎重な設計上の努力を必要とする。有効な確率を得るために線形ユニットを用いてその値をしきい値としたとする。

$$
P(y = 1 | \boldsymbol{x}) = \max{0, \min{1, \boldsymbol{w}^\top \boldsymbol{h} + b}}
$$

もちろん、これは有効な条件付き分布を定義するだろうが、勾配降下法を用いて非常に効率的に訓練することはできないだろう。$\boldsymbol{w}^\top \boldsymbol{h} + b$が単位区間の外側にいるときならいつでも、そのパラメータに関するモデルの出力の勾配は$\boldsymbol{0}$となるだろう。$\boldsymbol{0}$の勾配は、学習アルゴリズムが対応するパラメータを改善する方法に対するガイドをもはや持たないので、普通は問題となる。
代わりに、モデルが間違った答えを持っているときでも常に強い勾配が存在することを保証する異なるアプローチを用いることがより良い。このアプローチは最大尤度と組み合わされたシグモイド出力ユニットを用いることに基づく。
シグモイド出力ユニットは以下によって定義される。

$$
\hat{y} = \sigma \left( \boldsymbol{w}^\top \boldsymbol{h} + b \right)
$$

ここで、$\sigma$は[@sec:3.10]で述べられるロジスティックシグモイド関数である。
我々は2つの構成要素を持つとシグモイド出力ユニットを考えることができる。1つ目は、$\boldsymbol{w}^\top \boldsymbol{h} + b$を計算するために線形な層を用いる。2つ目は、$z$を確率に変換するためにシグモイド活性化関数を用いる。
値$z$を用いて$y$上の確率分布を定義する方法を考察するために今のところは$x$への依存性を省略する。シグモイドは合計が$1$にならない非正規化確率分布$\tilde{P}(y)$を構築するとこによって動機付けできる。故に、我々は有効な確率分布を得るために適切な定数で除算できる。非正規化対数確率が$y$および$z$において線形であるという仮定からはじめるならば、非正規化確率を得るためにべき乗することができる。故に、我々はこれが$z$のsigmoidal変換によって制御されるベルヌーイ分布を生み出すことを確認するために正規化する。

$$
\log \tilde{P}(y) = yz
$$

$$
\tilde{P}(y) = \exp(yz)
$$

$$
P(y) = \frac{\exp(yz)}{\sum_{y' = 0}^1 \exp(y'z)}
$$

$$
P(y) = \sigma((2y - 1)z)
$$

べき乗と正規化に基づく確率分布は統計モデリング界隈全体で一般的である。そのような二値変数上の分布を定義する$z$変数はロジット[logit]と呼ばれる。
対数空間において確率を予測することへのこのアプローチは最尤学習で用いることは自然である。最大尤度で使われるコスト関数が$-\log P(y | \boldsymbol{x})$であるので、コスト関数中の$\log$はシグモイドの$\exp$をもとに戻す。この効果が無いと、シグモイドの飽和は勾配ベースの学習が順調に進むことを妨げる。シグモイドによってパラメータ化されたベルヌーイの最尤学習に対する損失関数は以下である。

$$
J(\boldsymbol{\theta}) = -\log P(y | \boldsymbol{x})
$$

$$
= -\log \sigma((2y - 1)z)
$$

$$
= \zeta((1 - 2y)z)
$$

この導出は[@sec:3.10]からのいくつかの特性を使用する。softplus関数の観点で損失を書き換えることで、$(1 - 2y)z$が非常に大きな負であるときにのみ飽和することを確認できる。故に、飽和はモデルがすでに正しい答えを持っているとき、すなわち、$y = 1$かつ$z$が非常に大きな正である、または、$y = 0$かつ$z$が非常に大きな負であるときにのみ発生する。$z$が間違った符号を持つとき、softplus関数への引数$(1 - 2y)z$は$|z|$に単純化されても良い。$z$が間違った符号を持ちながら、$|z|$が
大きくなるに従って、softplus関数は、単純にその引数$|z|$を返す方に向かって漸近する。$z$に関する微分は$\text{sign}(z)$に漸近し、そのため、極めて不正確な$z$の極限において、softmax関数はまったく勾配を縮小しない。この特性は、勾配ベースの学習が間違えた$z$をすぐさまに正しくすることができることを意味するので、有用である。
解析的に言うと、シグモイドが有効な確率の閉区間$[0, 1]$全体を用いるのではなく、開区間$(0, 1)$に制限された値を返すので、シグモイドの対数は常に定義され、有限である。ソフトウェア実装において、数値的な問題を避けるため、$\hat{y} = \sigma(z)$の関数としてではなく、$z$の関数として負の対数尤度を書くことが最良である。シグモイド関数がゼロにアンダーフローするならば、$\hat{y}$の対数を取ることは負の無限大を生み出す。

#### Multinoulli出力分布のためのsoftmaxユニット

$n$個の取り得る値を持つ離散変数上の確率分布を表現することを望むならいつでも、softmax関数を用いても良い。これは二値変数上の確率分布を表現するのに使われたシグモイド関数の一般化と見ることができる。
softmax関数は殆どの場合で、$n$個の異なるクラス上の確率分布を表現するため、分類器の出力として用いられる。より稀なことだが、モデルがある内部変数に対する$n$個の異なる選択肢の中から1つを選ぶことを望むならば、softmax関数はモデル自体の内部で用いることができる。
二値変数の場合、単一の数値を生成することを望んだ。

$$
\hat{y} = P(y = 1 | \boldsymbol{x})
$$

この数値は0と1の間にある必要があり、対数尤度の勾配ベース最適化に対してうまく振る舞わせるためのその数値の対数を欲したので、我々は代わりに数値$z = \log \tilde{P}(y = 1 | \boldsymbol{x})$を予測することを選んだ。べき乗と正規化はシグモイド関数によって制御されるベルヌーイ分布をもたらした。
$n$個の値を持つ離散変数の場合に一般化するため、$\hat{y}_i = P(y = i | \boldsymbol{x})$を持つベクトル$\hat{\boldsymbol{y}}$を生成する必要がある。我々は$\hat{y}_i$の各要素が$0$と$1$の間にあるだけでなく、ベクトル全体の総和が$1$になる、すなわち、有効な確率分布を表現することも必要とする。ベルヌーイ分布で機能するのと同じアプローチはmultinoulli分布に一般化される。まず、線形な層は非正規化対数確率を予測する。

$$
\boldsymbol{z} = \boldsymbol{W}^\top \boldsymbol{h} + \boldsymbol{b}
$$

ここで、$z_i = \log \tilde{P}(y = i | \boldsymbol{x})$である。故に、softmax関数は望ましい$\hat{\boldsymbol{y}}$を得るために$\boldsymbol{z}$をべき乗し、正規化できる。正式には、softmax関数は以下によって求められる。

$$
\text{softmax}(\boldsymbol{z})_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
$$

ロジスティックシグモイドと同様に、$\exp$関数の使用は最大対数尤度を用いて目的値$y$を出力するようsoftmaxを訓練するときにうまく機能する。この場合、我々は$\log P(y = i; \boldsymbol{z}) = \log \text{softmax}(\boldsymbol{z})$を最大化することを望む。$\exp$の観点でsoftmaxを定義することは、対数尤度における$\log$がsoftmaxの$\exp$をもとに戻せるので、自然である。

$$
\log \text{softmax}(\boldsymbol{z})_i = z_i - \log \sum_j \exp(z_j)
$$

[@eq:6.30]の第一項は、入力$z_i$が常にコスト関数への直接の寄与を持つことを示す。この項は飽和できないので、[@eq:6.30]における第二項への$z_i$の寄与が非常に小さくなる場合でさえも学習が進む、ということが分かる。対数尤度を最大化するとき、第一項は$z_i$を押し上げるように促す一方で、第二項は$\boldsymbol{z}$のすべてを押し下げるように促す。第二項$\log \sum_j \exp(z_j)$に対するある直観を得るため、この項が$\max_j z_j$によって大まかに近似できることを調査する。この近似は$\exp(z_k)$が$\max_j z_j$より著しく小さいいずれかの$z_k$に対して有意でないというアイデアに基づく。この近似から獲得できる直観は負の対数尤度のコスト関数が常に最も活発な不正確な予測に強くペナルティーを科すことである。正しい答えがすでにsoftmaxへ最も大きな入力を持つならば、$-z_i$の項および$\log \sum_j \exp(z_j) \approx \max_j z_j = z_i$の項はおおよそ打ち消されるだろう。故に、この例は全体の訓練コストにほとんど寄与しないだろう。これは、未だに正しく分類されない他の例によって多数を占められるだろう。
ここまで、我々は単一の例のみを考察してきた。全体として、正則化されていない最大尤度はモデルが訓練セットに見られるそれぞれの結果の数の分数をsoftmaxに予測させるようなパラメータを学ぶようにするだろう。

$$
\text{softmax}(\boldsymbol{z}(\boldsymbol{x}; \boldsymbol{\theta})) \approx \frac{\sum_{j=1}^m \boldsymbol{1}_{y^{(j)} = i, \boldsymbol{x}^{(j)} = \boldsymbol{x}}}{\sum_{j=1}^m \boldsymbol{1}_{\boldsymbol{x}^{(j)} = \boldsymbol{x}}}
$$

最大尤度は一貫性のある推定器であるので、これは、モデルの族が訓練分布を表現する能力を持つ限り、発生することを保証される。実践では、制限されたモデル容量や不完全な最適化はモデルがこれらの分数を近似できるだけであることを意味するだろう。
対数尤度以外の多くの目的関数はsoftmax関数と同じくらいうまく機能しない。具体的に、softmaxの$\exp$をもとに戻すために$\log$を用いない目的関数は$\exp$への引数が非常に小さくなるとき、勾配の消失を引き起こしてしまい、学習することができない。特に、二乗誤差はsoftmaxユニットに対する損失関数としては貧弱で、モデルが非常に信頼できる不正確な予測を行うときでさえ、その出力を変化させるようモデルを訓練できない可能性がある[@Bridle1990]。これらの他の損失関数が失敗し得る理由を理解するため、我々はsoftmaxそれ自体を調べる必要がある。シグモイドと同様に、softmax活性化は飽和し得る。シグモイド関数はその入力が極めて負または極めて正であるときに飽和する単一の出力を持つ。softmaxは複数の出力値を持つ。これらの出力値は入力値間の差異が極めて大きくなるときに飽和し得る。softmaxが飽和するとき、飽和する活性化関数の逆を取ることができない限り、softmaxに基づく多くのコスト関数もまた飽和する。softmax関数がその入力間の差異に反応することを理解するため、softmaxの出力がその入力のすべてへ同じスカラを加算することに不変であることを観察する。

$$
\text{softmax}(\boldsymbol{z}) = \text{softmax}(\boldsymbol{z} + c)
$$

この特性を用いて、我々は数値的に安定なsoftmaxの変形を導出できる。

$$
\text{softmax}(\boldsymbol{z}) = \text{softmax}(\boldsymbol{z} - \max_i z_i)
$$

再定式化バージョンは、$z$が極めて大きい、または、極めて負の値を含むときでさえも、小さな数値誤差のみでsoftmaxを計算することを可能にする。数値的に安定な変形を調べると、softmax関数はその引数が$\max_i z_i$から逸脱する量によって操作される、といことが分かる。
出力$\text{softmax}(\boldsymbol{z})_i$は対応する入力が極大値（$z_i = \max_i z_i$）であり、$z_i$が他のすべての入力より大きいときに$1$に飽和する。出力$\text{softmax}(\boldsymbol{z})_i$もまた$z_i$が極大値でなく、最大値がより大きいときに$0$に飽和し得る。これはシグモイドユニットが飽和する方法の一般化であり、損失関数がそれを補うために設計されていない場合に学習に対して似た困難を引き起こし得る。
softmax関数への引数$\boldsymbol{z}$は2つの異なる方法で生成できる。最も一般的なものは、線形層$\boldsymbol{z} = \boldsymbol{W}^\top \boldsymbol{h} + \boldsymbol{b}$を用いて上記のように、単純に$\boldsymbol{z}$のすべての要素をニューラルネットワークの以前の層に出力させることである。簡単である一方、このアプローチは実際に分布を過剰にパラメータ化する。$n$個の出力の合計が$1$にならなければならないという制約は$n - 1$個のパラメータが必要であることのみを意味する。つまり、$n$番目の値の確率は$1$から最初の$n - 1$個の確率を引くことで得ても良い。故に、我々は$\boldsymbol{z}$の1つの要素が固定されるという要件を暗に示すことができる。例えば、$z_n = 0$であることを必要とすることができる。もちろん、これはまさにシグモイドユニットが行うことをである。$P(y = 1 | \boldsymbol{x}) = \sigma(z)$を定義することは2次元の$\boldsymbol{z}$と$z_1 = 0$を持つ$P(y = 1 | \boldsymbol{x}) = \text{softmax}(z)_1$を定義することと等価である。softmaxへの$n - 1$引数および$n$引数のアプローチの両方は同じ確率分布の集合を述べることができるが、異なる学習ダイナミクスを持つ。実践では、過剰にパラメータ化したバージョンと制限したバージョンを用いることの間の差異は滅多に存在せず、過剰にパラメータ化したバージョンを実装するほうがより単純である。
神経科学の視点から、それに関与するユニット間で競争の形式を創り出す方法としてsoftmaxを考えることは興味深い。つまり、softmaxの出力は常に合計が$1$となるので、1つのユニットの値における増加はその他の値における減少と必然的に対応する。これは皮膚における近隣のニューロンの間に存在すると考えられている側方抑制と類似する。一方（極大値$a_i$とその他の間の差異が絶対値で大きいとき）では、 勝者総取り（出力のひとつがほぼ$1$となり、その他がほぼ$0$となる）の形式となる。
"softmax"という名前は幾分か混乱し得る。その関数は$\max$関数より$\argmax$関数に密接に関係する。"soft"という語はsoftmax関数が連続で微分可能であるという事実から導かれる。one-hotベクトルとして表現される結果を持つ$\argmax$関数は連続でも微分可能でもない。故に、softmax関数は$\argmax$の「ソフトにした」バージョンをもたらす。対応する最大値関数のソフトなバージョンは$\text{softmax}(\boldsymbol{z})^\top \boldsymbol{z}$である。おそらくsoftmax関数を"softargmax"と呼ぶほうが良いのかもしれないが、現在の名前は慣例として定着している。

#### 他の出力タイプ

上述される線形、シグモイド、softmaxの出力ユニットは最も一般的である。ニューラルネットワークは我々が望むほぼすべての種類の出力層に一般化できる。最大尤度の原則はほぼすべての種類の出力層に対して良好なコスト関数を設計する方法に対するガイドを提供する。
一般に、条件付き分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$を定義するならば、最大尤度の原則はコスト関数として$-\log p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$を使うことを提案する。
一般に、我々は関数$f(\boldsymbol{x}; \boldsymbol{\theta})$を表現するとしてニューラルネットワークを考えることができる。この関数の出力は値$\boldsymbol{y}$の直接的な予測値ではない。代わりに、$f(\boldsymbol{x}; \boldsymbol{\theta}) = \boldsymbol{\omega}$は$y$上の分布に対するパラメータをもたらす。故に我々の損失関数は$-\log p(\boldsymbol{\mathbf{y}}; \boldsymbol{\omega}(\boldsymbol{x}))$と解釈できる。
例えば、$\boldsymbol{\mathbf{x}}$を与えられたときの$\boldsymbol{\mathbf{y}}$に対する条件付きガウシアンの分散を学習したいかもしれない。分散$\sigma^2$が定数である単純な場合では、分散の最尤推定が単純に観測値$\boldsymbol{\mathbf{y}}$とこれらの期待値の間の差の二乗の経験的な平均であるので、閉形式の式が存在する。特別なケースのコードを書く必要がない、計算的により高価なアプローチは$\boldsymbol{\omega} = f(\boldsymbol{x}; \boldsymbol{\theta})$によって制御される分布$p(\boldsymbol{\mathbf{y}} | \boldsymbol{x})$の特性のひとつとして分散を含めることである。故に、負の対数尤度$-\log p(\boldsymbol{y}; \boldsymbol{\omega}(\boldsymbol{x}))$は最適化プロシージャに分散をインクリメンタルに学習させるために必要である適切な項をを持つコスト関数をもたらすだろう。標準偏差が入力に依存しない単純な場合では、$\boldsymbol{\omega}$に直接コピーされるネットワークにおける新しいパラメータを作ることができる。この新しいパラメータは$\sigma$それ自体であるかもしれないか、$\sigma^2$を表現するパラメータ$v$にできるかもしれない。または、分布をパラメータ化するためにどのように線タックするかに依存して、$\frac{1}{\sigma^2}$を表現するパラメータ$\beta$にできるかもしれない。我々は我々のモデルが$\boldsymbol{\mathbf{x}}$の異なる値に対して$\boldsymbol{\mathbf{y}}$における分散の異なる量を予測するようにしたいかもしれない。これは不等分散[heteroscedastic]モデルと呼ばれる。不等分散の場合、我々は単純に分散の指定を$f(\boldsymbol{\mathbf{x}}; \boldsymbol{\theta})$によって出力される値のひとつとする。これを行う一般的な方法は、[@eq:3.22]に述べられる通り、分散ではなく精度を用いてガウス分布を定式化することである。多変量の場合、対角精度行列を用いることが最も一般的である。

$$
\text{diag}(\boldsymbol{\beta})
$$

この定式化は、$\boldsymbol{\beta}$によってパラメータ化されるガウス分布の対数尤度に対する定式が$\beta_i$による乗算と$\log \boldsymbol{\beta}_i$の加算のみを伴うので、勾配降下法でうまく機能する。乗算、加算、対数演算の勾配はうまく振る舞う。比較して、分散の観点で出力をパラメータ化したならば、除算を用いるふつようがあっただろう。除算の関数はゼロ近くで任意に傾斜が急となる。大きな勾配は学習を助け得る一方で、任意に多くな勾配は通常では不安定になる。標準偏差の観点で出力をパラメータ化したならば、対数尤度は依然として二乗のときと同様に除算を伴っただろう。二乗演算を解する勾配はゼロ近くで消失する可能性があり、二乗されたパラメータを学習しづらくする。標準偏差、分布、精度のどれを使うかに関わらず、我々はガウシアンの共分散行列が正定値であることを保証しなければならない。精度行列の固有値は共分散行列の固有値の逆数であるので、これは精度行列が正定値であることを保証することと等価である。対角行列またはスカラ倍した対角行列を用いるならば、モデルの出力に強制する必要がある条件は正値性のみである。$\boldsymbol{a}$が対角精度を決定するのに使われるモデルのそのままの活性化であるとするならば、我々は正の精度ベクトル$\boldsymbol{\beta} = \zeta(\boldsymbol{a})$を得るためにsoftplus関数を用いることができる。この同じ戦略は、精度ではなく分散か標準偏差を用いる場合、または、対角行列ではなくスカラ倍した単位行列を用いる場合に同様に適用される。対角よりリッチな構造を持つ共分散または精度行列を学習することは滅多にない。共分散がfullかつcoditionalであるならば、パラメータ化は予測される共分散行列の正定値性を保証するように選ばれなければならない。これは$\Sigma(\boldsymbol{x}) = \boldsymbol{B}(\boldsymbol{x}) \boldsymbol{B}^T(\boldsymbol{x})$をかくことで達成され得る。ここで、$\boldsymbol{B}$は制約のない正方行列である。行列が最大階数を持つ場合の実践的な問題のひとつは、$d \times d$行列では行列式に対して$O(d^3)$の計算と$\Sigma(\boldsymbol{x})$の逆行列（または、等価的に、かつ、より一般的に行われるものとして、その固有値分解または$\boldsymbol{B}(\boldsymbol{x})$のそれ）を必要とし、尤度を計算することが高価であることである。
我々は、しばしばマルチモーダル回帰、すなわち、$\boldsymbol{x}$の同じ値に対して$\boldsymbol{y}$空間におけるいくつかの異なるピークを持ち得る条件付き確率$p(\boldsymbol{y} | \boldsymbol{x})$から実数値を予測することを行いたい。この場合、混合ガウシアンはその出力に対する自然な表現である[@Jacobs1991; @Bishop1994]。これらの出力として混合ガウシアンを持つニューラルネットワークはしばしば$n$個の構成要素を持つ混合密度ネットワーク[mixture density networks]と呼ばれる。混合ガウシアンの出力は条件付き確率分布によって定義される。

$$
p(\boldsymbol{y} | \boldsymbol{x}) = sum_{i=1}^n p(c = i | \boldsymbol{x}) \mathcal{N}(\boldsymbol{y}; \boldsymbol{\mu}^{(i)}(\boldsymbol{x}), \boldsymbol{\Sigma}^{(i)}(\boldsymbol{x}))
$$

そのニューラルネットワークは、$p(c = i | \boldsymbol{x})$を定義するベクトル、すべての$i$に対して$\boldsymbol{\mu}^{(i)}(\boldsymbol{x})$をもたらす行列、すべての$i$に対して$\boldsymbol{\Sigma}^{(i)}(\boldsymbol{x})$をもたらすテンソル、の3つの出力を持たなければならない。これらの出力は様々な制約を満たさなければならない。

1. 混合の成分$p(c = i | \boldsymbol{x})$：これらは潜在変数[^1]$c$と関連した$n$個の異なる成分の上のmultinoulli分布を形成し、これらの出力が正であり合計して$1$になることを保証するため、一般に$n$次元ベクトル上のsoftmaxによって得ることができる。
2. $\boldsymbol{\mu}^{(i)}(\boldsymbol{x})$：これらは$i$番目のガウシアン成分に関連した中心または平均を示し、制約を持たない（一般にこれらの出力ユニットに対してまったく非線形性を持たない）。$\boldsymbol{\mathbf{y}}$が$d$-ベクトルであれば、ネットワークは$d$次元ベクトルのすべての$n$を含む$n \times d$行列を出力しなければならない。最大尤度でこれらの平均を学習することはだた1つの出力モードのみを持つ分布の平均を学習するよりいささか複雑である。我々は観察結果を実際に生成した構成要素に対する平均を更新したいだけである。実践では、我々はどの成分が各観察結果を生成したかを知らない。負の対数尤度のための式は成分がexampleを生成した確率によって各要素ごとの損失への各exampleの寄与を自然と重み付けする。
3. 共分散$\boldsymbol{\Sigma}^{(i)}(\boldsymbol{x})$：これらは成分$i$ごとの共分散行列を指定する。単一のガウシアン成分を学習するときと同様に、我々は一般に行列式を計算する必要性を回避するために対角行列を用いる。混合の平均を学習することと同様に、最大尤度はそれぞれの混合の成分への点ごとに部分的責任を割り当てる必要性によって複雑になる。勾配降下法は、混合モデルの下で負の対数尤度の正しい指定を与えられたならば、自動的に正しいプロセスに従うだろう。

[^1]: 我々は$c$を、そのデータにおいてそれを観測しないので、潜在的であるとみなす。入力$\boldsymbol{\mathbf{x}}$と目的$\boldsymbol{\mathbf{y}}$を与えられたとき、どのガウシアン成分が$\boldsymbol{\mathbf{y}}$を担当していたかを確実に知ることはできないが、我々は$\boldsymbol{\mathbf{y}}$がこれらのひとつを選んで生成されたと想像でき、その観測されない選択を確率変数とすることができる。

理由のひとつとして、（ある分散が特定のexampleに対して小さくなり、非常に大きな勾配を生み出すときに）数値的に不安定となり得る（分散による）除算を得るので、（ニューラルネットワークの出力に関する）条件付きガウシアン混合の勾配ベース最適化は不確実になり得ると報告されてきた。ひとつの解法は勾配をクリップすることであり（[@sec:10.11.1]を参照）、もうひとつはヒューリスティックに勾配をスケールすることである[@Uria2014]。
ガウシアン混合の出力は音声の生成モデル[@Schuster1999]や物理的物体の移動[@Graves2013]において特に効果的である。混合密度戦略はネットワークに複数の出力モデルを表現したり、その出力の分散を制御する方法をもたらす。これは、これらの実数値の領域において高い品質の度合いを得るために重要である。混合密度ネットワークの例は[@fig:6.4]に示される。

![混合密度の出力層を持つニューラルネットワークから描かれるサンプル。入力$x$は一様分布からサンプリングされ、出力$y$は$p_{model}(y | x)$からサンプリングされる。ニューラルネットワークは入力から出力分布のパラメータへの非線形マッピングを学習できる。これらのパラメータは混合成分ごとのパラメータと同様に3つの混合成分のどれが出力を生成するだろうかを統治する確率を含む。各混合成分は予測した平均と分散を持つガウシアンである。出力分布のこれらすべての側面は入力$x$に関して変化させ、非線形な方法でそれを行うことができる。](fig/6-4.png){#fig:6.4}

一般に、より多くの変数を含むより大きなベクトル$\boldsymbol{y}$をモデル化し、これらの出力変数に関してますますリッチな構造を課すことを継続することを望むかもしれない。例えば、ニューラルネットワークに文を形成する文字列を出力させたいならば、我々のモデル$p(\boldsymbol{y}; \boldsymbol{\omega}(\boldsymbol{x}))$に適用される最大尤度の原則を使い続けるかもしれない。この場合、$\boldsymbol{y}$を記述するために用いるモデルはこの章の範疇を超えるのに十分なほど複雑になるだろう。[@sec:10]では、文全体に渡るそのようなモデルを定義するためのリカレントニューラルネットワークを用いる方法を述べ、第三部では、任意の確率分布をモデル化するための発展的技術を述べる。

## 隠れユニット

これまで、我々は勾配ベース最適化で訓練されるほとんどのパラメトリックな機械学習モデルに一般的であるニューラルネットワークに対する設計上の選択に考察の焦点を当ててきた。では、我々は、どのようにしてモデルの隠れ層で使う隠れユニットの種類を選ぶか、というフィードフォワードニューラルネットワークに固有の問題に目を向ける。
隠れユニットの設計は極めて活発な研究分野であり、未だそこまで多くの決定的なガイドの理論的原則を持たない。
正規化線形ユニットは隠れユニットの素晴らしい既定の選択である。他の多くの種類の隠れユニットは利用可能である。（正規化線形ユニットが通常では許容できる選択であるにもかかわらず）どんなときにどの種類を使うかを決めることは難しい可能性がある。我々はここで隠れユニットの各種を動機付けして基本的な直観のいくつかを述べる。どれが最もうまく機能するだろうかを前もって予測することは通常では不可能である。設計プロセスは試行錯誤から成り、ある種類の隠れユニットがうまく機能するかもしれないという直観を得て、そして、その種類の隠れユニットでネットワークを訓練したり、検証セットでパフォーマンスを評価したりする。
このリストに含まれる隠れユニットのいくつかは実際にはすべての入力点で微分可能ではない。例えば、正規化線形関数$g(z) = \max{0, z}$は$z = 0$で微分できない。これは$g$が勾配ベースの学習アルゴリズムで用いられることを無効にするように見えるかもしれない。実践では、勾配降下法は依然としてこれらのモデルが機械学習タスクに対して使われるのに十分なほどうまく処理する。これの理由のひとつは、[@fig:4.3]に示される通り、ニューラルネットワークの訓練アルゴリズムが通常ではコスト関数の極小に達しないが、その代わりに単にその値を大幅に減らすだけであるためである（これらのアイデアは[@sec:8]でさらに述べられる）。我々は訓練が実際に勾配を$\boldsymbol{0}$とする点に達することを期待しないので、コスト関数の極大値が未定義の勾配を持つ点に対応することを許容する。微分可能でない隠れユニットは通常、少数の点のみで微分不可能である。一般に、関数$g(z)$は$z$のすぐ左への関数の傾斜によって定義される左微分と$z$のすぐ右への関数の傾斜によって定義される右微分を持つ。関数は左微分と右微分の療法が定義され、互いに等しい場合にのみ$z$において微分可能である。ニューラルネットワークの文脈において用いられる関数は通常では左微分を定義し、かつ、右微分を定義している。$g(z) = \max{0, z}$の場合、$z = 0$での左微分は$0$であり、右微分は$1$である。ニューラルネットワークの訓練のソフトウェア実装は通常では、微分が未定義であるとかエラーを起こしていると報告するのではなく、片側の微分の1つを返す。これはディジタルコンピュータ上の勾配ベース最適化が兎にも角にも数値誤差に左右されることを観察することによってヒューリスティックに正当化されるだろう。関数が$g(0)$を計算するよう求められるとき、その大本の値が本当に$0$であったという可能性は非常に低い。代わりに、$0$に丸められたある小さな値$\epsilon$であった可能性が高い。いくつかの状況では、より理論的に満足する正当化が利用できるが、これらは通常ではニューラルネットワークの訓練に適用されない。重要な点は、実践において以下で述べられる隠れユニットの活性化関数の非微分可能性は安全に無視できることである。
特に示されない限り、ほとんどの隠れユニットは入力のベクトル$\boldsymbol{x}$を受け入れ、アフィン変換$\boldsymbol{z} = \boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{b}$を計算して、要素ごとの非線形関数$g(z)$を適用するとして記述できる。ほとんどの隠れユニットは活性化関数$g(z)$の形式の選択によってのみ互いから区別される。

### 正規化線形ユニットとそれらの一般化

正規化線形ユニットは活性化関数$g(z) = \max{0, z}$を用いる。
これらのユニットは、線形ユニットととても良く似ているので、最適化しやすい。線形ユニットと正規化線形ユニットの唯一の違いは、正規化線形ユニットが半分の領域にわたってゼロを出力することである。これはユニットがアクティブであるときならいつでも正規化線形ユニットを介した微分を大きい状態に維持させる。その勾配は大きいだけでなく一貫性がある。正規化演算の二階微分はほとんどいたるところで$0$であり、正規化演算の微分はユニットがアクティブであるあらゆるところで$1$である。これは勾配の方向が副次的効果をもたらす活性化関数を用いるであろうよりも学習に対してかなり有用であることを意味する。
正規化線形ユニットは一般にアフィン変換の上で用いられる。

$$
\boldsymbol{h} = g(\boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{b})
$$

アフィン変換のパラメータを初期化するとき、$0.1$のような小さな正の値に$\boldsymbol{b}$のすべての要素を設定することは良いプラクティスであり得る。そうすることは正規化線形ユニットが訓練セットにおけるほとんどの入力に対してはじめからアクティブとなり、微分が通過できるであろう可能性を非常に高くする。
正規化線形ユニットの一般化がいくつか存在する。これらの一般化のほとんどは正規化線形ユニットに匹敵して処理し、ときにはより良く処理する。
正規化線形ユニットのひとつの欠点はこれらの活性化がゼロとなるexamplesに関する勾配ベース手法を介して学習できないことである。正規化線形ユニットの様々な一般化はこれらがいたるところで勾配を受け取ることを保証する。
正規化線形ユニットの3つの一般化は$z_i < 0$のときに非ゼロの傾斜$\alpha_i$を用いることに基づく。すなわち、$h_i = g(\boldsymbol{z}, \boldsymbol{\alpha})_i = \max(0, z_i) + \alpha_i \min(0, z_i)$である。absolute value rectificationは$g(z) = |z|$を得るために$\alpha_i = -1$に固定する。これは画像からの物体認識[@Jarrett2009]に使われる。ここでは、入力照明の極性反転の下で不変である特徴を探すことが理にかなっている。他の正規化線形ユニットの一般化はより広く適用可能である。leaky ReLU[@Maas2013]は$\alpha_i$を$0.01$のような小さな値に固定する一方で、parametric ReLUまたはPReLUは学習できるパラメータとして$\alpha_i$を扱う[@He2015]。
maxoutユニット[@Goodfellow2013a]はさらに正規化線形ユニットを一般化する。要素ごとの関数$g(z)$を適用する代わりに、maxoutユニットは$k$個の値のグループに$\boldsymbol{z}$を分割する。そして、各maxoutユニットはこれらのグループのひとつの最大要素を出力する。

$$
g(\boldsymbol{z})_i = \max_{j \in \mathbb{G}^{(i)}} z_j
$$

ここで、$\mathbb{G}^{(i)}$はグループ$i$に対する入力へのインデックスの集合${(i - 1)k + 1, \dots, ik}$である。これは入力の$\boldsymbol{x}$空間における複数方向に対応する区間線形関数を学習する方法をもたらす。
maxoutユニットは最大$k$個の区間を持つ区間線形な凸関数を学習できる。故に、maxoutユニットは単なるユニット間の関係ではなく活性化関数それ自体を学習すると見ることができる。十分に大きい$k$を用いて、maxoutユニットは任意の忠実度を持ついずれの凸関数を近似することを学習できる。特に、2つの区分を持つmaxout層は、正規化線形活性化関数、absolute value rectification関数、leakyまたはparametric ReLUを用いる伝統的な層と同じ入力$\boldsymbol{x}$の関数を実装することを学習できる。または、すべてがまったく異なる関数を実装することを学習できる。もちろん、maxout層はこれらの他の層タイプのいずれかから別々にパラメータ化されるだろう。そのため、学習ダイナミクスはmaxoutが他の層タイプのひとつとして$\boldsymbol{x}$の同じ関数を実装することを学習する場合でさえも異なるだろう。
各maxoutユニットは1つだけでなく$k$個の重みベクトルによってパラメータ化され、そのために、maxoutユニットは一般に正規化線形ユニットより多くの正則化をを必要とする。これらは訓練セットが大きくユニットあたりの区分数が低く維持されるならば、正則化なしでうまく機能する可能性がある[@Cai2013]。
maxoutユニットはいくつかの他の恩恵を持つ。いくつかの場合では、より少ないパラメータを必要とすることによっていくつかの統計的かつ計算的な利点を得られる。具体的には、$n$個の異なる線形フィルタによって捕捉される特徴が$k$個の特徴の各グループ上の$\max$を取ることで情報を損失せずにまとめることができるならば、次の層を$k$倍少ない重みだけで済ませることができる。
各ユニットは複数のフィルタで操作されるので、maxoutユニットは、ニューラルネットワークが過去に訓練されたタスクを処理する方法を忘れるという破滅的忘却[catastrophic forgetting]と呼ばれる現象に抵抗するのに役立ついくつかの冗長性を持つ[@Goodfellow2014a]。
正規化線形ユニットとその一般化のすべては、モデルの振る舞いが線形に近ければモデルを最適化しやすくなるという原則に基づいている。このより容易な最適化を得るための線形な振る舞いを用いる同様の一般的な原則は深層線形ネットワークに加えて他の状況でも適用される。リカレントネットワークは列から学習し、状態や出力の列を生成できる。これらを訓練する時、数回のステップに渡って情報を伝播させる必要がある。これは、（ほぼ$1$の大きさを持ついくつかの方向微分を伴う）いくつかの線形な計算を伴うときにさらに容易になる。最もうまく処理するリカレントネットワークアーキテクチャのひとつであるLSTMは総和、すなわち、特定の素直な種類の線形な活性化を介して時間を経て情報を伝播する。これは[@sec:10.10]でさらに考察される。

### ロジスティックシグモイドと双曲線正接

正規化線形ユニットの導入以前では、ほとんどのニューラルネットワークはロジスティックシグモイド活性化関数

$$
g(z) = \sigma(z)
$$

または双曲線正接活性化関数

$$
g(z) = \tanh(z)
$$

を用いた。これらの活性化関数は、$\tanh(z) = 2 \sigma(2z) - 1$であるので、密接に関係している。
我々はすでに二値変数が$1$となる確率を予測するのに使われる出力ユニットとしてシグモイドユニットを見てきた。区間線形ユニットと異なり、sigmoidalなユニットはその領域のほとんどに渡って飽和する、すなわち、$z$が非常に大きい正のときに高値に飽和し、$z$が非常に大きな負のときに低値に飽和し、$z$が$0$近くのときにのみ入力に激しく敏感になる。sigmoidalなユニットの広範囲に渡る飽和は勾配ベースの学習を非常に難しくし得る。この理由のため、フィードフォワードネットワークにおける隠れユニットとしてのこれらの使用はいまでは推奨されない。出力ユニットとしてのこれらの使用は、適切なコスト関数が出力層におけるシグモイドの飽和をもとに戻すことができるとき、勾配ベースの学習の使用に匹敵する。
sigmoidalな活性化関数が使われなければならないとき、双曲線正接活性化関数は一般にロジスティックシグモイドよりうまく処理する。これは、$\tanh(0) = 0$である一方で$\sigma(0) = \frac{1}{2}$であると言う意味において、恒等関数によく似ている。$\tanh$が$0$近くで恒等関数に似ているので、深層ニューラルネットワーク$\hat{y} = \boldsymbol{w}^\top \tanh(\boldsymbol{U}^\top \tanh(\boldsymbol{V}^\top \boldsymbol{x}))$を訓練することは、ネットワークの活性化が小さく維持されることができる限り、線形モデル$\hat{y} = \boldsymbol{w}^\top \boldsymbol{U}^\top \boldsymbol{V}^\top \boldsymbol{x}$を訓練することと似ている。これは$\tanh$ネットワークの訓練をより容易にする。
sigmoidalな活性化関数はフィードフォワードネットワーク以外での設定においてより一般的である。リカレントネットワーク、多くの確率的モデル、いくつかのautoencodersは区分線形活性化関数の使用を排し、飽和の欠点にも関わらずsigmoidalなユニットをより魅力的にする追加の要件を持つ。

### 他の隠れユニット

他の多くの種類の隠れユニットは使うことは可能だが、そこまで頻繁に使われない。
一般に多種多様な微分可能な関数は完璧にうまく処理する。多くの未公開の活性化関数は単に人気のものと同じくらいうまく処理する。具体例を提供するため、我々はMNISTデータセットに関して$\boldsymbol{h} = \cos(\boldsymbol{W} \boldsymbol{x} + \boldsymbol{b})$を用いてフィードフォワードネットワークをテストし、1%より小さい誤差率を得た。これは、より慣例的な活性化関数を用いて得られる結果に匹敵する。新技術の研究開発の最中、多くの様々な活性化関数をテストして、標準的な実践の上でいくつかのバリエーションが同程度に処理することを発見することは一般的である。これは、通常は新しい隠れユニットのタイプが大幅な改善をもたらすと明確に実証される場合にのみ公表されることを意味している。既知のタイプとほぼ同程度に処理する新しい隠れユニットのタイプはつまらないほどに一般的である。
文献に登場しているすべての隠れユニットのタイプをリスト化することは非現実的であるだろう。我々はいくつかの特に有用で独特なものを強調する。
ひとつの可能性はまったく活性化$g(z)$を持たないことである。これを活性化関数として恒等関数を用いていると考えることもできる。
我々は線形ユニットがニューラルネットワークの出力として有用となり得ることをすでに見てきた。これは隠れユニットとして使っても良い。ニューラルネットワークのすべての層が線形変換のみから成るならば、全体としてのネットワークは線形であるだろう。しかし、ニューラルネットワークのいくつかの層が純粋に線形であることは許容される。$n$個の入力と$p$個の出力を持つニューラルネットワーク層$\boldsymbol{h} = g(\boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{b})$を考えよう。我々はこれを、一方の層が重み行列$\boldsymbol{U}$を用い、もう一方の層が重み行列$\boldsymbol{V}$を用いるような、2つの層で置き換えても良い。第一層が活性化関数を持たないならば、我々は$\boldsymbol{W}$に基づく元々の層の重み行列を実質的に因数分解したことになる。因数分解的アプローチは$\boldsymbol{h} = g(\boldsymbol{V}^\top \boldsymbol{U}^\top \boldsymbol{x} + \boldsymbol{b})$を計算することである。$\boldsymbol{U}$が$q$個の出力を生み出すならば、$\boldsymbol{U}$および$\boldsymbol{V}$は共に$(n + p)q$個のパラメータのみを含み、その一方で、$\boldsymbol{W}$は$np$個のパラメータを含む。小さな$p$に対して、これはパラメータにおける相当な節約となり得る。これは線形変換を低ランクに制限するというコストが伴う[comes at the cost]が、これらの低ランクの関係はしばしば十分である。故に、線形な隠れユニットはネットワークにおいてパラメータ数を削減する効果的な方法をもたらす。
softmaxユニットは（[@sec:6.2.2.3]で述べられる通り）出力として通常では用いられるもうひとつの種類のユニットであるが、時折隠れ層として用いられるかもしれない。softmaxユニットは$k$この取り得る値を持つ離散変数上の確率分布を自然とひょうげんし、そのため、一種のスイッチとして用いられるかもしれない。これらの種類の隠れ層は通常、[@sec:10.12]で述べられるように、メモリを操ることを明示的に学習するより発展的なアーキテクチャでのみ用いられる。
いくつかの他のかなり一般的な隠れユニットのタイプは以下を含む。

- 放射基底関数（RBF）ユニット：$h_i = \exp \left( -\frac{1}{\sigma_i^2} \| \boldsymbol{W}_{:,i} - \boldsymbol{x} \|^2 \right)$。この関数は$\boldsymbol{x}$がテンプレート$\boldsymbol{W}_{:,i}$に近づくに従ってよりアクティブになる。これはほとんどの$\boldsymbol{x}$に対して$0$に飽和するので、最適化するのが難しい可能性がある。
- softplus：$g(a) = \zeta(a) = \log(1 + e^a)$。これは、関数近似のために @Dugas2001 によって、そして、無向な確率モデルの条件付き分布に対して @Glorot2011a によって導入された、rectifierの滑らかなバージョンである。@Glorot2011aはsoftplusとrectifierを比較し、後者による結果の方が良好であることを発見した。softplusの使用は一般に推奨されない。softplusは隠れユニットのタイプのパフォーマンスが非常に非直観的である、つまり、いたるところで微分可能であるため、または、あまり完全には飽和しないために、おそらくrectifier以上の利点を持つことが期待されるであろうが、経験的にはそうならないことを実証している。
- hard tanh：これは$\tanh$とrectifierに似た形状をしているが、後者とは異なり、$g(a) = \max(-1, \min(1, a))$に限定される。これは@Collobert2004によって導入された。

隠れユニットの設計は活発な研究領域のままであり、多くの有用な隠れユニットタイプが発見され続けている。

## アーキテクチャ設計

ニューラルネットワークに対するもうひとつの重要な設計上の検討事項はアーキテクチャを決定することである。アーキテクチャ[architecture]という言葉はネットワークの全体の構造を指している。つまり、いくつのユニットを持つべきかということやどのようにユニットが互いに接続されるべきかということである。
ほとんどのニューラルネットワークは層と呼ばれるユニットのグループに組織される。ほとんどのニューラルネットワークアーキテクチャはこれらの層を各層がその手前にある層の関数である鎖状構造に配置する。この構造では、第一層は以下によって求められる。

$$
\boldsymbol{h}^{(1)} = g^{(1)} \left( \boldsymbol{W}^{(1) \top} \boldsymbol{x} + \boldsymbol{b}^{(1)} \right)
$$

第二層は以下によって求められる。

$$
\boldsymbol{h}^{(2)} = g^{(2)} \left( \boldsymbol{W}^{(2) \top} \boldsymbol{h}^{(1)} + \boldsymbol{b}^{(2)} \right)
$$

これらの鎖ベースのアーキテクチャでは、主なアーキテクチャ上の検討事項はネットワークの深さと各層の幅を選択することである。のちに分かることだが、1つの隠れ層を持つネットワークでも訓練セットにフィットするには十分である。より深いネットワークはしばしば1層あたりにかなり少ないユニットとかなり少ないパラメータを用いることができ、同様にテストセットへ頻繁に汎化することができるが、最適化するのがより難しくなる傾向もある。タスクに対する理想的なネットワークアーキテクチャは検証セット誤差をモニタリングすることによって導かれる実験を介して見つけなければならない。

### 普遍的な近似の特性と深度

行列の乗算を介して特徴から出力へマッピングする線形モデルは定義により線形関数のみを表現できる。これは、線形モデルに適用されるときに多くの損失関数が凸最適化問題となるので、訓練しやすいという利点を持つ。残念ながら、我々はしばしば非線形関数を学習するシステムを欲する。
一見して、我々はおそらく非線形関数を学習することが学習したい種類の非線形性に対する特殊化されたモデルの族を設計することを必要とすると思うだろう。幸いにも、隠れ層を持つフィードフォワードネットワークは普遍的な近似フレームワークをもたらす。具体的には、universal approximation theorem[@Hornik1989; @Cybenko1989]は、1つの線形出力層と（ロジスティックシグモイド活性化関数のような）いずれかの「押し潰す」活性化関数を伴う少なくとも1つの隠れ層を持つフィードフォワードネットワークがいずれかの望ましい非ゼロの誤差量を持つ一方の有限次元空間から他方の空間への任意のボレル可測関数を近似できることを述べる。ただし、そのネットワークが十分な隠れユニットを与えられることを条件とする。フィードフォワードネットワークの微分もまた任意にうまくその関数の微分を近似できる[@Hornik1990]。ボレル可測性の概念は本書の範疇を超える。我々の目的には、$\mathbb{R}^n$の閉じた有界の部分集合に関する任意の連続関数がボレル可測であり、それ故に、ニューラルネットワークによって近似されても良いと言えば十分である。ニューラルネットワークは任意の有限次元の離散空間から別の空間へマッピングする任意の関数を近似することもして良い。オリジナルの定理がまず非常に大きな負と非常に大きな正の引数の両方に対して飽和する活性化関数を持つユニットの観点で述べられた一方で、universal approximation theoremsはより広いクラスの活性化関数に対しても証明された。現在ではこれに一般的に使われる正規化線形ユニットを含める[@Leshno1993]。
universal approximation theoremは、どんな関数を学習しようとしているかに関わらず、大きなMLPがこの関数を表現できるであろうということを我々が知っているということを意味している。しかし、我々は訓練アルゴリズムがその関数を学習できるであろうことを保証しない。MLPが関数を表現できる場合でさえ、学習は2つの異なる理由により失敗し得る。1つ目に、訓練に使われる最適化アルゴリズムは望ましい関数に対応するパラメータの値を求めることができないかもしれない。2つ目に、訓練アルゴリズムは結果がオーバーフィットするに従って誤った関数を選択するかもしれない。ノーフリーランチ定理が普遍的に優れた機械学習アルゴリズムが存在しないということを示すということを[@sec:5.2.1]より思い出したい。フィードフォワードネットワークは、関数を与えられたときにその関数を近似するフィードフォワードネットワークが存在する、という意味において関数を表現することに対する普遍的なシステムをもたらす。特定のexamplesの訓練セットを試験し、訓練セットにない点に汎化するであろう関数を選択することに対する普遍的なプロシージャは存在しない。
universal approximation theoremに従うと、我々が望む正確さの任意の度合いを達成するのに十分大きいネットワークは存在するが、その定理はこのネットワークがどれだけ大きいだろうかを語らない。@Barron1993は広いクラスの関数を近似するのに必要とされる単一層ネットワークの大きさのいくつかの境界をもたらす。残念ながら、最悪の場合では、隠れ層の数のべき乗（おそらく区別される必要がある各入力構成に対応して1つの隠れ層を持つ）が必要とされるだろう。これは二値の場合だと理解するのが最も簡単である。ベクトル$\boldsymbol{v} \in {0, 1}^n$に関する取り得る二値関数の数は$2^{2^n}$であり、そのような関数を選ぶことは$2^n$ビットを必要とする。これは一般に$O(2^n)$の自由度を必要とするだろう。
まとめると、単一の層を持つフィードフォワードネットワークは任意の関数を表現するのに十分であるが、その層は実現不可能なほどに大きくなるかもしれず、正確に学習および汎化できないかもしれない。多くの状況では、より深いモデルを用いると、望ましい関数を表現するのに必要なユニット数を減らすことができ、汎化誤差の量を減らすことができる。
関数の様々な族はある値$d$より大きい深さを持つアーキテクチャによって効率的に近似できるが、これらは深さが$d$以下に制限される場合に更に大きなモデルを必要とする。多くの場合、浅いモデルで必要な隠れユニットの数は$n$のべき乗である。このような結果は、機械学習に対して使われる連続で微分可能なニューラルネットワークに似ていないモデルに対してまず証明されたが、その後、これらのモデルに拡張された。最初の結果は論理ゲートの回路用であった[@Hastad1986]。後の研究はこれらの結果を非負の重みを持つ線形しきい値ユニットへ[@Hastad1991; @Hajnal1993]、そして、連続値の活性化を持つネットワークへ[@Maass1992; @Maass1994]と拡張した。多くの近代的なニューラルネットワークは正規化線形ユニットを用いる。@Leshno1993は正規化線形ユニットを含む非多項式の活性化関数の広範な族を伴う浅いネットワークが普遍的な近似特性を持つことを実証したが、これらの結果は深さや効率の問題に対処しない。つまり、彼らは十分に広いrectifierのネットワークが任意の関数を表現できるであろうことのみを明示している。@Montufar2014は深いrectifierのネットワークで表現可能な関数が浅い（隠れ層1つ）ネットワークでの隠れユニットのべき乗を必要とし得ることを示した。より正確に言えば、彼らは（recifierの非線形性かmaxoutユニットから得られる）区間線形ネットワークがネットワークの深さのべき乗である領域数を持つ関数を表現できることを示した。[@fig:6.5]はabsolute value rectificationを持つネットワークがある隠れユニットの上部で計算される関数の鏡像を、その隠れユニットの入力に関して、どのようにして創り出すかを図示する。各隠れユニットは（絶対値の非線形性の両側に関する）鏡写しの応答を生成するために入力空間を折りたたむところを指定する。これらの折りたたみ演算を構成することによって、我々はすべての種類の規則的な（例えば、繰り返し）パターンを捕捉できる指数関数的にたくさんの区分線形領域を得る。

![@Montufar2014による公式的により深いrectifierネットワークの指数関数的な利点の直観的で幾何的な説明。（左）absolute value rectificationユニットは入力におけるすべての鏡写しの点の組に対して同じ出力を持つ。鏡写しの対称軸はユニットの重みとバイアスによって定義される超平面によって求められる。そのユニットの上部で計算される関数（緑のdecision surface）はその対称軸を横切るより単純なパターンの鏡像となるだろう。（中）その関数は対称軸の周りの空間を折りたたむことで得られる。（右）もうひとつの繰り返しパターンは別の対称性を得るために（別の下流のユニットによって）最初のものの上で折りたたむことができる（これは2つの隠れ層で4回繰り返される）。図は@Montufar2014の許可を得て再現された。](fig/6-5.png){#fig:6.5}

@Montufar2014における主要な定理は、$d$個の入力、深さ$l$、隠れ層あたり$n$個のユニットを持つ深層rectifierネットワークによって切り出された線形領域の数は以下となる。

$$
O \left( \left( \begin{matrix}
n \\
d
\end{matrix} \right)^{d (l - 1)} n^d \right)
$$

すなわち、これは深さ$l$のべき乗である。ユニットあたり$k$個のフィルタを持つmaxoutネットワークの場合、線形領域の数は以下である。

$$
O \left( k^{(l - 1) + d} \right)
$$

もちろん、我々が機械学習の（そして、特にAIに対する）アプリケーションにおいて学習したい種類の関数がそのような特性を共有している保証は存在しない。
我々は統計的な理由のために深層モデルを選びたくなることもあるかもしれない。我々が特定の機械学習アルゴリズムを選ぶときはいつでも、そのアルゴリズムがどんな種類の関数を学習すべきかについて持っている事前の信念のある集合を暗黙的に述べている。深層モデルを選択することは学習したい関数がいくつかのより単純な関数の合成を含むべきであるという非常に一般的な信念をエンコードする。これは学習問題が他のより単純なバラツキの基礎を成す要因の観点で次々に説明できるバラツキの基礎を成す要因の集合を発見することから成ると理解することであると言っていると表現学習の視点から解釈できる。代わりに、学習したい関数が、各ステップが前のステップの出力を使う複数のステップから成るコンピュータプログラムであるというbeliefを表現するとして深層アーキテクチャの使用を解釈できる。これらの中間的な出力はバラツキの要因である必要はないが、代わりに、ネットワークがその内部処理を組織化するのに使うカウンタやポインタに類似する可能性がある。経験的には、より大きな深さは多種多様なタスクに対してより良い汎化となるように見える[@Bengio2007; @Erhan2009; @Bengio2009; @Mesnil2011; @Ciresan2012; @Krizhevsky2012; @Sermanet2013; @Farabet2013; @Couprie2013; @Kahou2013; @Goodfellow2014d; @Szegedy2014a]。これらの経験的な結果のいくつかの例は[@fig:6.6]と[@fig:6.7]を参照のこと。これらの結果は、深層アーキテクチャを用いることが、モデルが学習する関数の空間上の有用な事前確率を実際に説明するということを示唆している。

![深さの効果。より深いネットワークが住所の写真から複数桁の数値を書き起こすのに用いられるときにより良く汎化することを示す経験的な結果。データは@Goodfellow2014dから。テストセットの正確さは深さが増加するとともに一貫して増加する。モデルサイズへのその他の増加が同じ効果を生まないことを実証する対照実験は[@fig:6.7を参照のこと。]](fig/6-6.png){#fig:6.6}

![パラメータ数の効果。より深いモデルはよりうまく行う傾向がある。これは単にモデルがより大きいことによるためだけではない。@Goodfellow2014dに由来するこの実験は、この図に示される通り、これらの深さを増加させずに畳み込みネットワークの層におけるパラメータ数を増加させることがテストセットのパフォーマンスの増加という点でほとんど効果的でない、ということを示している。その凡例は各曲線を作るのに使われるネットワークの深さ、および、その曲線が畳み込み層または全結合層の大きさにおけるバラツキを表現するかどうかを示す。我々は、この文脈における狭いモデルが約2000万個のパラメータでオーバーフィットする一方で、深いモデルが6000万個以上を持っていることから恩恵を受けられることを確認している。これは深層モデルを用いることがそのモデルが学習できる関数の空間[expresses a useful preference]ということを示唆している。具体的に言えば、これは関数がともに構成されるたくさんのより単純な関数から成るべきであるというbeliefを表現する。これは結果として、より単純な表現から次々と構成される表現を学習すること（例えば、エッジの観点で定義されるカド）、または、順番に依存するステップを伴うプログラムを学習すること（例えば、最初に物体の集まりを見つけて、個々に区分けして、識別する）、のどちらかとなる。](fig/6-7.png){#fig:6.7}

### 他のアーキテクチャ上の検討事項

これまでに我々は、ネットワークの深さと各層の幅が主な検討事項であると共に、層の単純な鎖としてニューラルネットワークを述べてきた。実践では、ニューラルネットワークはかなり多くの多様性を示す。
多くのニューラルネットワークのアーキテクチャは特定のタスクのために開発されてきた。畳み込みネットワークと呼ばれるコンピュータビジョンに対して特殊化されたアーキテクチャは[@sec:9]で述べられる。フィードフォワードネットワークも、[@sec:10]で述べられるが、シーケンス処理に対するリカレントニューラルネットワークに一般化できるだろう。これは、それ自身のアーキテクチャ上の検討事項を持つ。
一般に、たとえこれが最も一般的なプラクティスであるとしても、層は鎖に接続される必要はない。多くのアーキテクチャは主鎖を作るが、層$i$から層$i + 2$以上へ行くskip connectionsのように、追加のアーキテクチャ上の特徴を追加する。これらのskip connectionsは勾配が出力層から入力により近い層に流れやすくする。
アーキテクチャ上の設計のもうひとつ重要な検討事項はまさに層の組がお互いに接続する方法である。行列$\boldsymbol{W}$を介する線形変換によって述べられるデフォルトのニューラルネットワークの層では、すべての入力ユニットはすべての出力ユニットに接続される。先の章にある多くの特殊化されたネットワークはより少ない接続を持ち、そのために、入力層における各ゆにっとは出力層におけるユニットの小さな部分集合のみと接続される。接続数を減らすというこれらの戦略はパラメータ数とネットワークを計算するために必要な計算量を減少させるが、しばしばかなり問題依存である。例えば、[@sec:9]で述べられる畳み込みネットワークはコンピュータビジョンの問題に対して非常に効果的である疎な接続の特殊化されたパターンを用いる。この章では、汎用的なニューラルネットワークのアーキテクチャに関するより具体的なアドバイスを与えることは難しい。続く章では、異なるアプリケーション領域に対してうまく機能すると分かっている特定のアーキテクチャ上の戦略を開発する。

## 逆伝播と他の差別化アルゴリズム

入力$\boldsymbol{x}$を受け入れ、出力$\hat{\boldsymbol{y}}$を生み出すためにフィードフォワードニューラルネットワークを用いるとき、情報はネットワークを通して順方向に流れる。入力$\boldsymbol{x}$は後に各層で隠れユニットに伝播し、最終的に$\hat{\boldsymbol{y}}$を生み出す初期情報を提供する。これはforward propagationと呼ばれる。訓練中に、forward propagationはスカラのコスト$J(\boldsymbol{\theta})$を生み出すまで続けることができる。逆伝播[back-propagation]アルゴリズム[@Rumelhart1986a]、しばしば単にbackpropと呼ばれるが、はコストからの情報が勾配を計算するためにネットワークを通して逆方向に流れることを許可する。勾配に対する解析的な数式を計算することはstraightforwardであるが、そのような数式を数値的に計算することは計算上高価である可能性がある。逆伝播アルゴリズムは単純で高価でない手順を用いてこれを行う。
逆伝播という用語はしばしば多層ニューラルネットワークに対する全体の学習アルゴリズムとして誤解される。実際には、逆伝播法は勾配を計算するための手法のみを指し、一方で、確率的勾配降下法のような別のアルゴリズムはこの勾配を用いて学習を行うのに使われる。さらに、逆伝播法はしばしば多層ニューラルネットワークに特有であると誤解されるが、原則では、任意の関数の微分を計算できる（いくつかの関数に対して、関数の微分が未定義であると報告されることは正しい応答である）。具体的に言うと、我々は任意の関数$f$に対する勾配$\nabla_{\boldsymbol{x}} f(\boldsymbol{x}, \boldsymbol{y})$を計算する方法を述べるだろう。ここで、$\boldsymbol{x}$は望ましい微分を持つ変数の集合であり、$\boldsymbol{y}$は関数への入力だが微分を持つことが必須ではない変数の追加の集合である。学習アルゴリズムでは、我々がよく必要とする勾配はパラメータに関するコスト関数の勾配$\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta})$である。多くの機械学習タスクは、学習プロセスの一部として、または、学習したモデルを解析するために他の微分を計算することを伴う。逆伝播アルゴリズムは同様にこれらのタスクに適用でき、パラメータに関するコスト関数の勾配を計算することに制限されない。ネットワークと通して情報を伝播することで微分を計算するというアイデアは非常に一般的であり、複数の出力を持つ関数$f$のヤコビアンのような値を計算するのに使うことができる。我々はここでは$f$が単一の出力を持つ最も一般的に使われるケースに説明を制限する。

### Computational Graphs


これまで我々は比較的に非公式なグラフ言語でニューラルネットワークを考察してきた。逆伝播アルゴリズムをより精密に述べるため、より精密なcomputational graph言語を持つことは役に立つ。
グラフとして計算を公式化する方法はたくさん考えられる。
ここでは、我々は変数を示すためにグラフの各ノードを用いる。変数はスカラ、ベクトル、行列、テンソル、または、さらに別のタイプの変数であるかもしれない。
グラフを公式化するため、我々は演算のアイデアを導入する必要もある。演算は1つ以上の変数の単純な関数である。我々のグラフ言語は許容できる演算の集合を伴う。この集合の演算より複雑な関数はともに多くの演算を構成することで述べても良い。
一般性を失うことなく、我々は単一の出力変数のみを返すと演算を定義する。これは、出力変数がベクトルのように複数の成分を持つことができるので、一般性を損なわない。逆伝播法のソフトウェア実装は通常では複数の出力を伴う演算をサポートするが、概念上の理解に重要でない多くの追加の詳細を導入するので、我々の説明ではこの場合を回避する。
変数$y$が変数$x$に演算を適用することで計算されるならば、我々は$x$から$y$に有向な辺を描く。我々は時折、適用した演算の名前を持つ出力ノードに注釈を付け、それ以外の、演算が文脈から明らかであるときにはこのラベルを省略する。
computational graphsの例は[@fig:6.8]に示される。

![computational graphsの例。（a）$z = xy$を計算するための$\times$演算を用いるグラフ。（b）ロジスティック回帰の予測値$\hat{y} = \sigma(\boldsymbol{x}^\top \boldsymbol{w} + b)$に対するグラフ。中間式のいくつかは代数式における名前を持たないが、グラフにおける名前を必要とする。我々は単純に$i$番目のそのような変数を$\boldsymbol{u}^{(i)}$と名前を付ける。（c）入力$\boldsymbol{X}$のミニバッチを含む計画行列を与えられるときに正規化線形ユニットの活性化$\boldsymbol{H}$の計画行列を計算する、式$\boldsymbol{H} = \max{0, \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b}$に対するcomputational graph。（d）各変数に多くとも1つの演算を適用した$a$から$c$の例だが、1つより多い演算を適用することができる。ここで、我々は線形回帰モデルの重み$\boldsymbol{w}$に1つより多い演算を適用するcomputation graphを示す。その重みは予測値$\hat{y}$とweight decayのペナルティ$\lambda \sum_i w_i^2$の両方を作るのに使われる。](fig/6-8.png){#fig:6.8}

### 微積分の連鎖律

微積分の連鎖律（確率の連鎖律と混同しないこと）は既知の微分を持つ他の関数を構成することによって形成される関数の微分を計算するのに使われる。逆伝播法は非常に効率的である演算の特定の順序を持つ連鎖律を計算するアルゴリズムである。
$x$を実数であるとし、$f$と$g$の両方を実数から実数へマッピングする関数であるとする。$y = g(x)$かつ$z = f(g(x)) = f(y)$であるとする。そして、連鎖律は以下であると述べる。

$$
\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}
$$

我々はスカラの場合を超えてこれを一般化できる。$\boldsymbol{x} \in \mathbb{R}^m$であり、$\boldsymbol{y} \in \mathbb{R}^n$であり、$g$が$\mathbb{R}^m$から$\mathbb{R}^n$へマッピングし、$f$が$\mathbb{R}^n$から$\mathbb{R}$へマッピングするとする。$\boldsymbol{y} = g(\boldsymbol{x})$かつ$z = f(\boldsymbol{y})$であるならば、以下となる。

$$
\frac{\partial z}{\partial x_i} \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}
$$

ベクトルの表記法では、これは等価なものとして以下のように書けるだろう。

$$
\nabla_{\boldsymbol{x}} z = \left( \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \right)^\top \nabla_{\boldsymbol{y}} z
$$

ここで、$\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$は$g$の$n \times m$のヤコビ行列である。
これにより、我々は変数$\boldsymbol{x}$の勾配がヤコビ行列$\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$に勾配$\nabla_{\boldsymbol{y}} z$を乗算することで得られることを理解する。逆伝播アルゴリズムはグラフにおける演算ごとにそのようなヤコビアンと勾配の積を行うことから成る。
通常、我々は逆伝播アルゴリズムを、単にベクトルに適用するのではなく、任意次元のテンソルに適用する。概念的には、これはベクトルによる逆伝播法とまったく同じである。唯一異なるのは、テンソルを形成するために数値がグリッド内にどのように配置されるか、という所である。我々は逆伝播法を実行する前に書くテンソルをベクトルに平坦化し、ベクトル値の勾配を計算し、その勾配をテンソルに成形し直すことを想像できるかもしれない。この再配置の視点では、逆伝播法は依然として単にヤコビアンに勾配を乗算することである。
テンソル$\boldsymbol{\mathsf{X}}$に関する値$z$の勾配を表記するため、我々は、$\boldsymbol{\mathsf{X}}$がベクトルであるかのようにみなしたならば、$\nabla_{\boldsymbol{\mathsf{X}}} z$と書く。$\boldsymbol{\mathsf{X}}$に付くインデックスは複数の座標を持つ。例えば、3Dテンソルは3つの座標によってインデックスが付けられる。我々はインデックスの完全なタプルを表現するrために単一の変数$i$を用いることでこれを抽象化できる。すべての取り得るインデックスのタプル$i$に対して、$(\nabla_{\boldsymbol{\mathsf{X}}} z)_i$は$\frac{\partia z}{\partial \mathsf{X}_i}$を与える。これは、ベクトルへの取り得るすべての整数インデックス$i$に対して、$$(\nabla_{\boldsymbol{x}} z)_i$が$\frac{\partia z}{\partial x_i}を与えることとまったく同じである。この表記法を用いると、我々は連鎖律をテンソルを適用することとして記述できる。$\boldsymbol{\mathsf{Y}} = g(\boldsymbol{\mathsf{X}})$かつ$z = f(\boldsymbol{\mathsf{Y}})$であれば、以下となる。

$$
\nabla_{\boldsymbol{\mathsf{X}}} z = \sum_j (\nabla_{\boldsymbol{\mathsf{X}}} \mathsf{Y}_j) \frac{\partial z}{\partial \mathsf{Y}_j}
$$

### 逆伝播を得るための連鎖律の再帰的な適用

連鎖律を用いると、そのスカラを生み出したcomputational graphにおけるいずれかのノードに関するスカラの勾配のための代数式を書き下すことは簡単である。しかし、計算機でその式を実際に計算することはいくつかの追加の検討事項をもたらす。
具体的には、多くの部分式は勾配に対して全体の式の内部で数回繰り返されるかもしれない。勾配を計算するいずれかの手続は数回これらの部分式を格納するかこれらを再計算するかのいずれであるかを選択する必要がある。これらの部分式がどのように発生するかの例は[@fig:6.9]で与えられる。いくつかの場合、同じ部分式を二度計算することは単純に無駄だろう。複雑なグラフに対して、連鎖律のナイーブな実装を実現不可能にする、指数関数的に多数のこれらの無駄な計算が存在し得る。他の場合、同じ部分式を二度計算することはより高いランタイムコストでメモリ消費量を減らすための有効な方法となり得るだろう。

![勾配を計算するときに繰り返される部分式となるcomputational graph。$w \in \mathbb{R}$がグラフへの入力であるとする。我々は同じ関数$f : \mathbb{R} \rightarrow \mathbb{R}$を連鎖のすべてのステップで適用する演算として用いる。すなわち、$x = f(w), y = f(x), z = f(y)$である。$\frac{\partial z}{\partial w}$を計算するため、我々は[@eq:6.44]を適用し、以下を得る。

$$
\frac{\partial z}{\partial w}
$$

$$
= \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \frac{\partial x}{\partial w}
$$

$$
= f'(y) f'(x) f'(w)
$$

$$
= f'(f(f(w))) f'(f(w)) f'(w)
$$

[@eq:6.51]は$f(w)$の値を一度だけ計算し、変数$x$に格納するという実装を示唆する。これは逆伝播アルゴリズムによって取られるアプローチである。代替のアプローチは[@eq:6.52]によって取られる示唆される。ここでは、部分式$f(w)$が1回より多く現れる。代替アプローチでは、$f(w)$は必要とされるたびに再計算される。これらの式の値を格納するための必要なめもりが少ないとき、[@eq:6.51]の逆伝播アプローチは、実行時間が短くなるので、明らかに好ましい。しかし、[@eq:6.52]もまた連鎖律の有効なじっそうであり、メモリが限定的であるときに有用である。](fig/6-9.png){#fig:6.9}

我々は、実際に行われる、連鎖律の再帰的な適用に従った順番で、実際の勾配計算を直接指定する逆伝播アルゴリズムのバージョンから始める（関連する順方向の計算に対する[@lst:6.1]と[@lst:6.2]）。これは、これらの計算を直接処理できるか、アルゴリズムの説明を逆伝播法を計算するためのcomputational graphの記号的な仕様と見なせるか、のいずれかであるだろう。しかし、この形式化は勾配の計算を行う記号的なグラフの操作[manipulation]と構築を明示しない。そのような形式化は[@lst:6.5]によって[@sec:6.5.6]で示される。ここでは、任意のテンソルを含むノードを一般化することも行う。
まず、単一のスカラ$u^{(n)}$を計算する方法を述べるcomputational graphを考えよう（例えば、訓練exampleに関する損失）。このスカラは、$u^{(1)}$から$u^{(n_i)}$の$n_i$個の入力ノードに関する、我々が欲しい勾配を持つ量である。言い換えれば、我々はすべての$i \in {1, 2, \dots, n_i}$に対して$\frac{\partial u^{(n)}}{\partial u^{(i)}}$を計算したい。パラメータ上の勾配降下法に対する勾配の計算への逆伝播法のアプリケーションでは、$u^{(u)}$はexampleかミニバッチに関連するコストとなるだろう。一方、$u^{(1)}$から$u^{(n_i)}$はモデルのパラメータに対応する。
我々はグラフのノードが、$u^{(n_i + 1)}$から始まり、$u^{(n)}$に向かって、次々にこれらの出力を計算できるような方法で順序付けされていると仮定するだろう。[@lst:6.1]で定義される通り、各ノード$u^{(i)}$は演算$f^{(i)}$に関連し、以下の関数を評価することで計算される。

$$
u^{(i)} = f(\mathbb{A}^{(i)})
$$

ここで、$\mathbb{A}^{(i)}$は$u^{(i)}$の親であるすべてのノードの集合である。

```algorithm
\For{$i = 1, \dots, n_i$}
  \State{$u^{(i)} \leftarrow x_i$}
\EndFor
\For{$i = n_i + 1, \dots, n$}
  \State{$\mathbb{A}^{(i)} \leftarrow {u^{(j)} | j \in Pa(u^{(i)})}$}
  \State{$u^{(i)} \leftarrow f^{(i)}(\mathbb{A}^{(i)})$}
\EndFor
\Return $u^{(n)}$
```
:$n_i$個の入力$u^{(1)}$から$u^{(n_i)}$から出力$u^{(n)}$にマッピングする計算を行う手続。これは各ノードが以前のノードの値$u^{(j)}, j < i, \text{ with } j \in Pa(u^{(i)})$を含む引数の集合$\mathbb{A}^{(i)}$に関数$f^{(i)}$を適用することで数値的な値$u^{(i)}$を計算するcomputational graphを定義する。computational graphへの入力はベクトル$\boldsymbol{x}$であり、最初の$n_i$個のノード$u^{(1)}$から$u^{(n_i)}$にセットさせる。computational graphの出力は最後の（出力）ノード$u^{(n)}$から読み出される。 {#lst:6.1}

このアルゴリズムは、グラフ$\mathcal{G}$に付け加えることができるであろう、forward propagationの計算を明示する。逆伝播法を行うため、我々は$\mathcal{G}$に依存するcomputational graphを構築し、追加のノードの集合を加えることができる。これらは$\mathcal{G}$のノードあたり1つのノードを持つ部分グラフ$\mathcal{B}$を形成する。$\mathcal{B}$における計算は$\mathcal{G}$における計算の順序のちょうど逆順であり、$\mathcal{B}$の各ノードはforward graphのノード$u^{(i)}$に関連した微分$\frac{\partial u^{(n)}}{\partial u^{(i)}}$を計算する。これは、[@lst:6.2]に明示されるように、スカラの出力$u^{(n)}$に関する連鎖律を用いて行われる。

$$
\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i:j \in Pa(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}}
$$

部分グラフ$\mathcal{B}$は$\mathcal{G}$のノード$u^{(j)}$からノード$u^{(i)}$への辺ごとに厳密に1つの辺を含む。$u^{(j)}$から$u^{(i)}$への辺は$\frac{\partial u^{(i)}}{\partial u^{(j)}}$の計算に関連する。加えて、ドット積は、$u^{(j)}$の子供であるノード$u^{(i)}$に関してすでに計算される勾配と同じ子ノード$u^{(i)}$に対する偏微分$\frac{\partial u^{(i)}}{\partial u^{(j)}}$を含むベクトルとの間で、ノードごとに行われる。まとめると、逆伝播法を行うために必要な計算量は$\mathcal{G}$における辺の数に対して線形にスケールする。ここで、辺ごとの計算は（その親のひとつに関するノード1つの）偏微分を計算すること、そして、乗算1回および加算1回を行うことに対応する。以下では、我々はこの分析をテンソル値的なノードに一般化する。これは、同じノードで複数のスカラ値をグループ化し、より効率的な実装を可能にする方法である。

```algorithm
\State{ネットワークの活性化を得るためにforward propagation（この例に対して[@lst:6.1]）を実行する。}
\State{計算された微分を格納するデータ構造である`grad_table`を初期化する。その成分$\mathtt{grad\_table}[u^{(i)}]$は$\frac{\partial u^{(n)}}{\partial u^{(i)}}$の計算された値を格納するだろう。}
\State{$\mathtt{grad\_table}[u^{(i)}] \leftarrow 1$}
\For{$j = n - 1$から$1$まで}
  \State{次の行は格納された値を用いて$\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i:j \in Pa(u^{(i)}) \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}}$を計算する：}
  \State{$\mathtt{grad\_table}[u^{(j)}] \leftarrow \sum_{i:j \in Pa(u^{(i)})} \mathtt{grad\_table}[u^{(i)}] \frac{\partial u^{(i)}}{\partial u^{(j)}}$}
\EndFor
\Return ${\mathtt{grad\_table}[u^{(i)}] | i = 1, \dots, n_i}$
```
:グラフにおける変数に関して$u^{(n)}$の微分を計算するための逆伝播アルゴリズムの単純化したバージョン。この例はすべての変数がスカラである単純化したケースを示すことで理解を深めることが目的であり、我々は$u^{(1)}, \dots, u^{(n_i)}$に関する微分を計算したい。この単純化したバージョンはグラフ中のすべてのノードの微分を計算する。このアルゴリズムの計算コストは、各辺に関連した偏微分が定数時間を必要とすると仮定すると、グラフ中の辺の数に比例する。これはforward propagationに対する計算量と同じオーダーである。それぞれの$\frac{\partial u^{(i)}}{\partial u^{(j)}}$は$u^{(i)}$の親$u^{(j)}$の関数であり、それ故に、forward graphのノードを逆伝播グラフに対して追加されるこれらと接続する。 {#lst:6.2}

逆伝播アルゴリズムはメモリに関係なく共通の部分式の数を減らすために設計される。具体的には、グラフ中のノードあたりヤコビアンの積を1回程度行う。これはbackprop（[@lst:6.2]）が関連する偏微分$\frac{\partial u^{(i)}}{\partial u^{(j)}}$を得るためにきっかり一度だけグラフのノード$u^{(j)}$からノード$u^{(i)}$への各辺に訪れるという事実から理解できる。故に、逆伝播法は繰り返される部分式における指数関数的な爆発を回避する。他のアルゴリズムは、computational graphに関して単純化を行うことでより多くの部分式を回避できるかもしれないし、いくつかの部分式をを格納するのではなく再計算することによってメモリを節約できるかもしれない。我々は逆伝播アルゴリズム自体を述べた後にこれらのアイデアを再訪する。

### 全結合MLPにおける逆伝播法の計算

逆伝播法の計算の上記の定義を明確にするため、全結合の多層のMLPに関連した特定のグラフを考えるとする。
まず、[@lst:6.3]は、$\boldsymbol{x}$が入力に提供されるときにニューラルネットワークの出力となる$\hat{\boldsymbol{y}}$を伴う、パラメータを単一の$(\text{入力}, \text{目的})$の訓練example$(\boldsymbol{x}, \boldsymbol{y})$と関連する教師あり損失$L{\hat{\boldsymbol{y}}, \boldsymbol{y})$にマッピングする、forward propagationを示す。
次に、[@lst:6.4]はこのグラフに逆伝播アルゴリズムを適用するために行われるための対応する計算を示す。
[@lst:6.3]と[@lst:6.4]は単純で分かりやすいよう選ばれたデモンストレーションである。しかし、これらはある特定の問題に特殊化されている。
近代的なソフトウェア実装は下の[@sec:6.5.6]で述べられる逆伝播法の一般化された形式に基づく。これは、記号的な計算に対してデータ構造を明示的に操作することでいずれかのcomputational graphに適応できる。

```algorithm
\Require{ネットワークの深さ$l$}
\Require{モデルの重み行列$\boldsymbol{W}^{(i)}, i \in {1, \dots, l}$}
\Require{モデルのバイアスパラメータ$\boldsymbol{b}^{(i)}, i \in {1, \dots, l}$}
\Require{プロセスへの入力$\boldsymbol{x}$}
\Require{目的の出力$\boldsymbol{y}$}
  \State{$\boldsymbol{h}^{(0)} = \boldsymbol{x}$}
  \For{$k = 1, \dots, l$}
    \State{$\boldsymbol{a}^{(k)} = \boldsymbol{b}^{(k)} + \boldsymbol{W}^{(k)} \boldsymbol{h}^{(k - 1)}$}
    \State{$\boldsymbol{h}^{(k)} = f(\boldsymbol{a}^{(k)})$}
  \EndFor
  \State{$\hat{\boldsymbol{y}} = \boldsymbol{h}^{(l)}$}
  \State{$J = L(\hat{\boldsymbol{y}}, \boldsymbol{y}) + \lambda \Omega(\theta)$}
```
:典型的な深層ニューラルネットワークおよび損失関数の計算を通したforward propagation。損失$L(\hat{\boldsymbol{y}}, \boldsymbol{y})$は出力$\hat{\boldsymbol{y}}$と目的$\boldsymbol{y}$に依存する（損失 関数の例は[@sec:6.2.1.1]を参照）。総コスト$J$を得るため、損失はregularizer$\Omega(\theta)$に追加されるかもしれない。ここで、$\theta$はすべてのパラメータ（重みとバイアス）を含む。[@lst:6.4]はパラメータ$\boldsymbol{W}$と$\boldsymbol{b}$に関する$J$の勾配を計算する方法を示す。単純さのため、このデモンストレーションは単一の入力example$\boldsymbol{x}$のみを用いる。実践的なアプリケーションはミニバッチを使うべきである。より現実的なデモンストレーションは[@sec:6.5.7]を参照のこと。 {#lst:6.3}

```algorithm
\State{順方向の計算の後、出力層に関する勾配を計算する：}
\State{$\boldsymbol{g} \leftarrow \nabla_{\hat{\boldsymbol{y}}} J = \nabla_{\hat{\boldsymbol{y}}} L(\hat{\boldsymbol{y}}, \boldsymbol{y})$}
\For{$k = l, l + 1, \dots, 1$}
  \State{層の出力に関する勾配をpre-nonlinearity活性化に関する勾配に変換する（$f$が要素ごとであれば、要素ごとの乗算である）：}
  \Statte{$\boldsymbol{g} \leftarrow \nabla_{\boldsymbol{a}^{(k)}} J = \boldsymbol{g} \odot f'(\boldsymbol{a}^{(k)})$}
  \State{重みとバイアスに関する勾配を計算する（必要なところでは正則化項を含む）：}
  \State{$\nabla_{\boldsymbol{b}^{(k)}} J = \boldsymbol{g} + \lambda \nabla_{\boldsymbol{b}^{(k)}} \Omega(\theta)$}
  \State{$\nabla_{\boldsymbol{W}^{(k)}} J = \boldsymbol{g} \boldsymbol{h}^{(k - 1) \top} + \lambda \nabla_{\boldsymbol{W}^{(k)}} \Omega(\theta)$}
  \State{次の低レベルの隠れ層の活性化に関するこう奪いを伝播する：}
  \State{$\boldsymbol{g} \leftarrow \nabla_{\boldsymbol{h}^{(k - 1)}} J = \boldsymbol{W}^{(k) \top} \boldsymbol{g}$}
\EndFor
```
:入力$\boldsymbol{x}$に加えて、目的$\boldsymbol{y}$を用いる、[@lst:6.3]の深層ニューラルネットワークに対する逆伝播法。この計算は、出力層から始まり、第一隠れ層に戻って、層$k$ごとに活性化$\boldsymbol{a}^{(k)}$に関する勾配を生成する。各層の出力が誤差を減らすためにどれだけ変化すべきかを示すものと解釈できる、これらの勾配により、各層のパラメータに関する勾配を得られる。重みとバイアスに関する勾配は、（勾配が計算された後すぐに更新を行って）確率的な勾配の更新の一部として即座に使ったり、他の勾配ベースの最適化手法で使ったりすることができる。 {#lst:6.4}

### Symbol-to-Symbol微分

代数式とcomputational graphsの両方は記号[symbols]、または、特定の値を持たない変数で操作する。これらの代数およびグラフベースの表現は記号表現[symbolic representations]と呼ばれる。実際に使われる、または、ニューラルネットワークを訓練するとき、我々はこれらの記号に特定の値を割り当てなければならない。我々はネットワークへの記号的な入力$\boldsymbol{x}$を、$[1.2, 3.765, -1.8]^\top$のような、特定の数値的[numeric]な値で置き換える。
逆伝播法へのいくつかのアプローチはグラフへの入力に対する数値的な値の集合とcomputational graphを取り、これらの入力の値での勾配を述べる数値的な値の集合を返す。我々はこのアプローチをsymbol-to-number differentiationと呼ぶ。これはTorch[@Collobert2011b]やCaffe[@Jia2013]のうようなライブラリによって用いられるアプローチである。
もうひとつのアプローチはcomputational graphを取り、望ましい微分の記号的な記述を提供するグラフに追加のノードを加える。これはTheano[@Bergstra2010; @Bastien2012]やTensorFlow[@Abadi2015]によって取られるアプローチである。これがどうやって機能するかの例は[@fig:6.10]に図示される。このアプローチの主な利点は微分がオリジナルの式と同じ言語で記述されることである。微分は単なるもうひとつのcomputational graphであるので、より高階の微分をえるために微分を微分して、再び逆伝播法を実行することが可能である（より高階の微分の計算は[@sec:6.5.10]で述べられる）。

![微分の計算へのsymbol-to-symbolのアプローチの例。このアプローチでは、逆伝播アルゴリズムはいずれの実際の特定の数値的な値にアクセスすることすら必要ない。代わりに、これらの微分を計算する方法を記述するcomputational graphにノードを追加する。汎用グラフ評価エンジンはいずれの特定の数値的な値に対する微分をのちに計算できる。（左）この例では、我々は$z = f(f(f(w)))$を表現するグラフから始める。（右）我々は、$\frac{dz}{dw}$に対応する式のためのグラフを構築するよう指示して、逆伝播アルゴリズムを実行する。この例では、逆伝播アルゴリズムがどのように機能するかを説明しない。その目的は望ましい結果が何であるかを図示することのみである。つまり、微分の記号的な記述を伴うcomputational graphである。](fig/6-10.png){#fig:6.10}

我々は後者のアプローチを用い、微分に対するcomputational graphの構築に関して逆伝播アルゴリズムを記述するだろう。故に、グラフのいずれかの部分集合はあとで特定の数値的な値を用いて計算されるかもしれない。これは各演算が計算されるべきときに厳密に指定することを回避することを可能にする。代わりに、汎用グラフ評価エンジンはその親の値が分かり次第すべてのノードを計算できる。
symbol-to-symbolベースのアプローチの記述はsymbol-to-numberアプローチを包摂する。symbol-to-numberアプローチはsymbol-to-symbolアプローチで作られるグラフで行われるものとまったく同じ計算を行うこととして理解できる。主な違いはsymbol-to-numberアプローチがグラフを表に出さないことである。

### 一般的な逆伝播法

逆伝播アルゴリズムはいたって単純である。グラフにおいて祖先$\boldsymbol{x}$のひとつに関するあるスカラ$z$の勾配を計算するため、我々は$z$に関する勾配が$\frac{dz}{dz} = 1$によって与えられるという観察によって始める。故に、我々は、現在の勾配に$z$を生み出した演算のヤコビアンを乗算することによって、グラフにおける$z$の各親に関する勾配を計算できる。我々はヤコビアンを乗算し、$\boldsymbol{x}$に達するまでこの方法でグラフを通って後方に進む[travel]ことを続ける。$z$から2つ以上の経路を通って後に戻ることで達するかもしれないいずれかのノードに対して、我々は単に異なる経路からそのノードに到達する勾配の総和を取る。
より公式的に言えば、グラフ$\mathcal{G}$における各ノードは変数に対応する。最大の一般性を達成するため、我々はこの変数をテンソル$\boldsymbol{\mathsf{V}}$であると記述する。一般にテンソルは任意の次元数を持てる。これらはスカラ、ベクトル、行列を包摂する。
我々は各変数$\boldsymbol{\mathsf{V}}$が以下のサブルーチンに関連すると仮定する。

- $\mathtt{get\_operation}(\boldsymbol{\mathsf{V}})$：これは、computational graphにおける$\boldsymbol{\mathsf{V}}$に入る辺によって表現する、$\boldsymbol{\mathsf{V}}$を計算する演算を返す。例えば、行列の乗算を表現するPythonやC++のクラスや`get_operation`関数としても良い。行列の乗算によって作られる変数$\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}$があるとする。すると、$\mathtt{get\_operation}(\boldsymbol{\mathsf{V}})$は対応するC++クラスのインスタンスへのポインタを返す。
- $\mathtt{get\_consumers}(\boldsymbol{\mathsf{V}}, \mathcal{G})$：これはcomputational graph $\mathcal{G}$において$\boldsymbol{\mathsf{V}}$の子である変数のリストを返す。
- $\mathtt{get\_inputs}(\boldsymbol{\mathsf{V}}, \mathcal{G})$：これはcomputational graph $\mathcal{G}$において$\boldsymbol{\mathsf{V}}$の親である変数のリストを返す。

各演算`op`もまた`bprop`演算に関連する。この`bprop`演算は[@eq:6.47]で述べられるように、ヤコビアンとベクトルの積を計算できる。これが逆伝播アルゴリズムが素晴らしい一般性を達成できる方法である。各演算はそれに参加するグラフ中の辺を通して逆伝播を行う方法を知っている責任がある。例えば、我々は変数$\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}$を生成するために行列の乗算演算を用いるかもしれない。$\boldsymbol{C}$に関するスカラ$z$の勾配が$\boldsymbol{G}$によって与えられるとする。行列乗算の演算は、その入力引数のそれぞれに1つの、2つの逆伝播の規則を定義する責任がある。出力に関する勾配が$\boldsymbol{G}$であるとしたときの$\boldsymbol{A}$に関するを要求するために勾配`bprop`メソッドを呼び出すならば、行列乗算演算の`bprop`メソッドは$\boldsymbol{A}$に関する勾配が$\boldsymbol{G} \boldsymbol{B}^\top$によって求められることを示さなければならない。同様に、$\boldsymbol{B}$に関する勾配を要求するために`bprop`メソッドを呼び出すならば、行列演算は`bprop`メソッドを実装し、望ましい勾配が$\boldsymbol{A}^\top \boldsymbol{G}$によって与えられることを明示する責任がある。逆伝播アルゴリズム自体はいずれの微分規則も知る必要はない。正しい引数で各演算の`bprop`の規則を呼出す必要があるだけである。公式的には、$\mathtt{op.bprop}(\mathtt{inputs}, \boldsymbol{\mathsf{X}}, \boldsymbol{\mathsf{G}})$は以下を返さなければならない。

$$
\sum_i (\nabla_{\boldsymbol{\mathsf{X}}} \mathtt{op.f}(\mathtt{inputs})_i) \mathsf{G}_i
$$

これは、[@eq:6.47]で表されるように、単なる連鎖律の実装である。ここでは、$\mathtt{inputs}$は演算に供給される入力のリストであり、$\mathtt{op.f}$は演算を実装する数学的な関数であり、$\boldsymbol{\mathsf{X}}$は計算したい勾配を持つ入力であり、$\boldsymbol{\mathsf{G}}$は演算の出力に関する勾配である。
`op.bprop`メソッドは、たとえそうでないとしても、そのすべての入力が互いに異なると装うべきである。例えば`mul`演算は$x^2$を計算するために$x$のコピーを2つ渡され、`op.bprop`メソッドは依然として両方の入力に関する微分として$x$を返すべきである。逆伝播アルゴリズムはのちに、$x$の正しい全微分である$2x$を得るためにこれらの入力の両方をともに加算するだろう。
逆伝播法のソフトウェア実装は通常では演算とそれらの`bprop`メソッドの両方を提供する。つまり、深層学習のソフトウェアライブラリのユーザは行列演算、指数、対数などのような一般的な演算を用いて作られるグラフを通して逆伝播を行うことができる。逆伝播法の新しい実装を作るソフトウェアエンジニアや既存のライブラリにオレオレ演算を追加する必要がある上級者は通常ではいずれかの新しい演算に対して手動で`op.bprop`メソッドを導出しなければならない。
逆伝播アルゴリズムは公式的に[@lst:6.5]で述べられる。

```algorithm
\Require{計算しなけれならない勾配を持つ目的の変数の集合$\mathbb{T}$}
\Require{computational graph $\mathcal{G}$}
\Require{微分される変数$z$}
  \State{$z$の祖先であり、$\mathbb{T}$におけるノードの子孫であるノードのみを含むよう剪定された$\mathcal{G}$を$\mathcal{G}'$とする。}
  \State{テンソルをそれらの高愛に関連付けるデータ構造である$\mathtt{grad\_table}$を初期化する。}
  \State{$\mathtt{grad\_table}[z] \leftarrow 1$}
  \For{$\boldsymbol{\mathsf{V}}$ in $\mathbb{T}$}
    \State{$\mathtt{build\_grad}(\boldsymbol{\mathsf{V}}, \mathcal{G}, \mathcal{G}', \mathtt{grad\_table})$}
  \EndFor
  \Return{$\mathbb{T}$に制限された$\mathtt{grad\_table}$}
```
:逆伝播アルゴルズムの最も外側の骨格。この部分は単純なセットアップおよびクリナップ処理を行う。重要な処理のほとんどは[@lst:6.6]の`build_grad`サブルーチンで発生する。 {#lst:6.5}

```algorithm
\Require{$\mathcal{G}$と$\mathtt{grad\_table$に追加されるべき勾配を持つ変数$\boldsymbol{\mathsf{V}}$}
\Require{修正するグラフ$\mathcal{G}$}
\Require{勾配に関わるノードに$\mathcal{G}$を制限したもの$\mathcal{G}'$}
\Require{これらの勾配にノードをマッピングするデータ構造$\mathtt{grad\_table$}
  \If{$\boldsymbol{\mathsf{V}}$が$\mathtt{grad\_table$の中にある}
    \Return{$\mathtt{grad\_table}[\boldsymbol{\mathsf{V}}]$}
  \EndIf
  \State{$i \leftarrow 1$}
  \For{$\boldsymbol{\mathsf{C}}$ in $\mathtt{get\_consumers}(\boldsymbol{\mathsf{V}}, \mathcal{G}'$}
    \State{$\mathtt{op} \leftarrow \mathtt{get\_operation}(\boldsymbol{\mathsf{C}})$}
    \State{$\boldsymbol{\mathsf{D}} \leftarrow \mathtt{build\_grad}(\boldsymbol{\mathsf{C}}, \mathcal{G}, \mathcal{G}', \mathtt{grad\_table})$}
    \State{$\boldsymbol{\mathsf{G}}^{(i)} \leftarrow \mathtt{op.bprop}(\mathtt{get\_inputs}(\boldsymbol{\mathsf{C}}, \mathcal{G}'), \boldsymbol{\mathsf{V}}, \boldsymbol{\mathsf{D}})$}
    \State{$i \leftarrow i + 1$}
  \EndFor
  \State{$\boldsymbol{\mathsf{G}} \leftarrow \sum_i \boldsymbol{\mathsf{G}}^{(i)}$}
  \State{$\mathtt{grad\_table}[\boldsymbol{\mathsf{V}}] = \boldsymbol{\mathsf{G}}$}
  \State{$\boldsymbol{\mathsf{G}}$と$\mathcal{G}$に生成する演算を挿入する}
  \Return{$\boldsymbol{\mathsf{G}}$}

```
:逆伝播アルゴルズムの内部ループサブルーチン$\mathtt{build\_grad}(\boldsymbol{\mathsf{V}}, \mathcal{G}, \mathcal{G}', \mathtt{grad\_table})$は[@lst:6.5]で定義される逆伝播アルゴルズムで呼び出される。 {#lst:6.6}

[@sec:6.5.2]では、我々は逆伝播法が連鎖律において同じ部分式を複数回計算するのを避けるために開発されたことを説明した。ナイーブなアルゴリズムはこの繰り返される部分式により指数の実行時間を持ち得るだろう。今や我々は逆伝播アルゴリズムを示したのだから、その計算コストを理解できる。各演算の計算が大まかに同じコストを持つと仮定するならば、実行される演算数の観点から計算コストを解析しても良い。我々がcomputational graphの基本単位として演算を参照することをここでは覚えておいてほしい。これは実際にはいくつかの算術演算から成るかもしれない（例えば、我々は行列の乗算を単一の演算として扱うグラフを持つかもしれない）。$n$個のノードを持つグラフにおける勾配を計算することは$O(n^2)$個より多い演算を実行することも、$O(n^2)$個より多い演算の出力を格納することもない。ここでは、我々は、基礎を成すハードウェアによって実行される個別の演算ではなく、computational graph中で演算を数えていて、そのために、各演算の実行時間が大きくばらつくかもしれないことを覚えておくことは重要である。例えば、それぞれが何百万の成分を含む2つの行列の乗算はグラフ中では単一の演算に対応するかもしれない。我々は、最悪の場合にforward propagationステージが元のグラフの$n$個すべてのノードを実行するであろうことから、勾配を計算することが多くとも$O(n^2)$個の演算を必要とすることを理解できる（どんな値を計算したいかに依存して、グラフ全体を実行する必要がないかもしれない）。逆伝播アルゴリズムは$O(1)$個のノードで表現されるはずであるヤコビアンとベクトルの積を元のグラフの辺につき1つ追加する。computational graphは有向非巡回グラフであるので、多くとも$O(n^2)$個の辺を持つ。実践で一般的に使われる種類のグラフに対して、この状況はさらに良くなる。ほとんどのニューラルネットワークのコスト関数はおおまかに鎖構造であり、逆伝播法が$O(n)$のコストをもつようにする。これは指数関数的に多いノードを実行する必要があるかもしれないナイーブなアプローチよりかなり良くなる。この潜在的な指数コストはさあ域的な連鎖律[@eq:6.53]を非再帰的に展開して書き直すことで理解できる。

$$
\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{\text{path}(u^{(\pi_1)}, u^{(\pi_2)}, \dots, u^{(\pi_t)}), \text{ from } \pi_1 = j \text{ to } \pi_t = n} \prod_{k = 2}^t \frac{\partial u^{(\pi_k)}}{\partial u^{(\pi_{k - 1})}}
$$

ノード$j$からノード$n$までの経路の数がこれらの経路の長さにおいて指数関数的に増加するので、そのような経路の数である、上記の総和における項の数はforward propagationグラフの深さで指数関数的に増加する。この大きなコストは、$\frac{\partial u^{(i)}}{\partial u^{(j)}}$に対する同じ計算が何回もやり直されるであろうことから、課せられるだろう。そのような再計算を回避するため、我々は逆伝播法を中間結果$\frac{\partial u^{(n)}}{\partial u^{(i)}}$を格納する利点を活かすtable-fillingアルゴリズムとして考えることができる。グラフの各ノードはそのノードに対する勾配を格納するためのテーブルにおける対応するスロットを持つ。順番にこれらのテーブルのエントリを埋めることで、逆伝播法は繰り返す多くの共通の部分式を回避する。このtable-filling戦略は時折、動的計画法[dynamic programming]と呼ばれる。

### 例：MLPの訓練のための逆伝播法

例として、我々は多層パーセプトロンを訓練するのに使われる逆伝播アルゴルズムを順を追って説明する。
ここでは、我々は単一の隠れ層を持つ非常に単純な多層パーセプトロンを開発する。このモデルを訓練するため、我々はミニバッチの確率的勾配降下法を用いるだろう。逆伝播アルゴルズムは単一のミニバッチに関してコストの勾配を計算するのに使われる。具体的には、我々は計画行列$\boldsymbol{X}$と関連するクラスのラベルのベクトル$\boldsymbol{y}$として整形される訓練セットからのexamplesのミニバッチを用いる。ネットワークは隠れ層の特徴$\boldsymbol{H} = \max{0, \boldsymbol{X} \boldsymbol{W}^{(1)}$を計算する。その見せ方を単純化するため、このモデルではバイアスを用いない。我々のグラフ言語が要素ごとの$\max{0, \boldsymbol{Z}}$を計算できる`relu`演算を含むと仮定する。すると、クラス全体での正規化されていない対数確率の予測は$\boldsymbol{H} \boldsymbol{W}^{(2)}$で与えられる。我々のグラフ言語が目的$\boldsymbol{y}$とこれらの非正規化対数確率で定義される確率分布の間の交差エントロピーを計算する`cross_entropy`演算を含むと仮定する。結果の交差エントロピーはコスト$J_{MLE}$を定義する。この交差エントロピーを最小化することは分類器の最尤推定を行う。しかし、この例をより現実的にするため、正則化項も含める。その総コストは以下のように交差エントロピーと係数$\lambda$を伴うweight decay項からなる。

$$
J = J_{MLE} + \lambda \left( \sum_{i,j} \left( W_{i,j}^{(1)} \right)^2 + \sum_{i,j} \left( W_{i,j}^{(2)} \right)^2 \right)
$$

そのcomputational graphは[@fig:6.11]に図示される。

![交差エントロピーとweight decayを用いる単層MLPのexampleを訓練するためのコストを計算するのに使われるcomputational graph。]](fig/6-11.png){#fig:6.11}

このexampleの勾配に対するcomputational graphは描いたり読んだりするにはうんざりしそうなほどに大きい。これは、素直だがソフトウェアエンジニアが手動で導出するには退屈な勾配を自動的に生成できる、逆伝播アルゴルズムの恩恵のひとつを実証する。
我々は[@fig:6.11]におけるforward propagationのグラフを調べることで逆伝播アルゴルズムの振る舞いを大まかに描き出すことができる。訓練するために、我々は$\nabla_{\boldsymbol{W}^{(1)}} J$と$\nabla_{\boldsymbol{W}^{(2)}} J$の両方を計算したい。$J$からその重みへと逆方向に導く2つの異なる経路が存在する。ひとつは交差エントロピーのコストを通り、もうひとつはweight decayのコストを通る。weight decayのコストは比較的単純である。すなわち、$\boldsymbol{W}^{(i)}$の勾配に$2 \lambda \boldsymbol{W}^{(i)}$を常に寄与するだろう。
交差エントロピーのコストを通る別の経路はわずかに複雑である。$\boldsymbol{G}$を`cross_entropy`演算によってもたらされる非正規化対数確率$\boldsymbol{U}^{(2)}$の勾配であるとする。すると、逆伝播アルゴルズムは2つの異なる分岐を探索する必要がある。短い方の分岐では、行列乗算の演算の第二引数に逆伝播の規則を用いて、$\boldsymbol{W}^{(2)}$の勾配に$\boldsymbol{H}^\top \boldsymbol{G}$を加える。もうひとつの分岐はネットワークに沿ってさらに降下するより長い鎖に対応する。まず、逆伝播アルゴルズムは、行列乗算の演算の第一引数に対して逆伝播の規則を用いて、$\nabla_{\boldsymbol{H}} J = \boldsymbol{G} \boldsymbol{W}^{(2) \top}$を計算する。次に`relu`演算は$0$より小さい$\boldsymbol{U}^{(1)}$の成分に対応する勾配の要素をゼロにするためにその逆伝播の規則を用いる。その結果を$\boldsymbol{G}'$と呼ぶことにする。逆伝播アルゴルズムの最後のステップは$\boldsymbol{W}^{(1)}$の勾配に$\boldsymbol{X}^\top \boldsymbol{G}''$を追加するために`matmul`演算の第２引数に対して逆伝播の規則を用いることである。
これらの勾配が計算された後、勾配降下アルゴリズム、または、別の最適化アルゴリズムはパラメータを更新するためにこれらの勾配を用いる。
MLPでは、その計算コストは行列乗算のコストによってその多くを占められる。forward propagationステージの間に、我々はそれぞれの重み行列を乗算する。これは結果として$O(w)$個の積和となる。ここで、$w$は重みの数である。逆伝播ステージの間に、我々はそれぞれの重み行列の転置を乗算する。これは、同様の計算コストを持つ。そのアルゴリズムの主なメモリコストは、我々が入力を隠れ層の非線形性に格納する必要がある、ということである。この値は計算されてから逆方向パスが同じ点に戻るまで格納される。故に、そのメモリコストは$O(m n_h)$である。ここで、$m$はミニバッチのexample数であり、$n_h$は隠れユニットの数である。

### 複雑化

ここでの逆伝播アルゴリズムの我々の記述は実践で実際に使われる実装より単純である。
上で言及した通り、我々は単一のテンソルを返す関数である演算の定義を制限していた。ほとんどのソフトウェア実装は1つより多いテンソルを返すことができる演算をサポートする必要がある。例えばテンソルでの最大値とその値のインデックスの両方を計算したいならば、メモリを通して単一パスで両方を計算することが最良であり、そのために、このプロシージャを2つの出力を伴う単一の演算として実装することが最も効率的である。
我々は逆伝播法のメモリ消費を制御する方法を述べてこなかった。逆伝播法はしばしば多数のテンソルの総和を伴う。ナイーブなアプローチでは、これらのテンソルのそれぞれは別個に計算され、これらすべては第二ステップで加算されるだろう。ナイーブなアプローチは、単一バッファを整備して計算されるたびに各値をそのバッファに加えることで回避できる、過度に高いメモリのボトルネックを持つ。
逆伝播法の現実世界の実装もまた、32ビット浮動小数点、64ビット浮動小数点、整数値のような、様々なデータタイプを扱う必要がある。これらのタイプのそれぞれを扱うためのポリシーは設計するのに特別な注意を払う。
いくつかの演算は未定義の勾配を持ち、これらのケースを追跡してユーザによって要求される勾配が未定義であるかどうかを決定することは重要である。
他の様々な細かい部分[technicalities]は現実世界の微分をより複雑にする。これらの細かい部分は克服不可能ではなく、この章は微分を計算するのに必要となる重要な知的ツールを述べてきたが、はるかにずっと多くの微妙な点が存在することに気づくことは重要である。

### 深層学習コミュニティの外側での微分

深層学習コミュニティは幾分、より広い計算機科学コミュニティから孤立していて、微分を計算する方法についてのそれ自身の文化的態度を大きく発展させてきた。より噛み砕いて言うと、自動微分[automatic differentiation]の分野はアルゴリズム的に微分を計算する方法と関係している。ここで述べられる逆伝播アルゴリズムは自動微分への1つのアプローチでしかない。これはreverse mode accumulationと呼ばれるより広範囲のクラスの技術の特殊なケースである。他のアプローチは異なる順番で連鎖律の部分式を計算する。一般に、最も低い計算コストとなる計算順を定めることは困難な問題である。勾配を計算するための演算の最適な列を求めることは、代数式を最も安価な形式に単純化する必要があるかもしれないという意味で、NP完全[@Naumann2008]である。
例えば、確率を表す変数$p_1, p_2, \dots, p_n$、非正規化対数確率を表す変数$z_1, z_2, \dots, z_n$があるとする。我々は以下を定義するとする。

$$
q_i = \frac{\exp(z_i)}{\sum_i \exp(z_i)}
$$

ここで、我々は指数、総和、除算の演算からsoftmax関数を作り、交差エントロピーの損失$J = -\sum_i p_i \log q_i$を構築する。人間の数学者は$z_i$に関する$J$の微分が非常に単純な形式$q_i - p_i$を取ることを観察できる。逆伝播アルゴリズムはこの方法で勾配を単純化することができず、代わりに、オリジナルのグラフですべての対数および指数の演算を通して勾配を明示的に伝播するだろう。Theano[@Bergstra2010; @Bastien2012]のようないくつかのソフトウェアライブラリは真の逆伝播アルゴリズムによって提案されるグラフより改善するためにいくつかの種類の代数的な置換を行う。
forwardグラフ$\mathcal{G}$が単一の出力ノードを持ち、各偏微分$\frac{\partial u^{(i)}}{\partial u^{(j)}}$が一定の計算コストで計算できるとき、それぞれの局所的な偏微分$\frac{\partial u^{(i)}}{\partial u^{(j)}}$は再帰的な連鎖律の定式[@eq:6.53]に関連する積和に沿って一度だけ計算される必要があるので、逆伝播法は勾配計算の計算量がforward計算の計算量と同じオーダーであることを保証する。これは[@lst:6.2]で確認できる。それ故に、全体の計算は$O(\text{辺の数})$である。しかし、これは逆伝播法によって構築されるcomputational graphを単純化することで減らすことができる可能性を持ち、これはNP完全なタスクである。TheanoやTensorFlowのような実装はグラフを単純化することを反復的に試みるために既知の単純化パターンをマッチングすることに基づくヒューリスティクスを用いる。我々はスカラの出力の勾配を計算するためだけに逆伝播法を定義したが、逆伝播法はヤコビアン（グラフ中の$k$個の異なるスカラのノード、$k$個の値を含むテンソル値的なノード、のどちらか）を計算するよう拡張できる。故に、ナイーブな実装はさらに$k$倍の計算を必要であるかもしれない。つまり、元のforwardグラフにおけるスカラの内部ノードごとに、ナイーブな実装は単一の勾配ではなく$k$個の勾配を計算する。グラフの出力の数が入力の数より多いとき、forward mode accumulationと呼ばれる自動微分のもうひとつの形式を用いる方が時折好ましい。forward mode accumulationはリカレントネットワークにおいて勾配のリアルタイム計算を得るために提案された。例としては[@Williams1989]がある。このアプローチもまた、計算効率とメモリのトレードオフにより、グラフ全体で値および勾配を格納する必要性を回避する。forward modeとbackward modeの関係は、以下のような、行列のシーケンスの左乗算と右乗算の関係に類似している。

$$
\boldsymbol{A} \boldsymbol{B} \boldsymbol{C} \boldsymbol{D}
$$

ここで、行列はヤコビアンと考えることができる。例えば、$\boldsymbol{D}$が列ベクトルであるが、$\boldsymbol{A}$が多数の行を持つ場合、グラフは単一の出力と多数の入力を持ち、終わりから乗算を開始して逆方向に向かうことは行列とベクトルの積のみを必要とする。この順序はbackward modeに対応する。そうではなく、左から乗算し始めることは一連の行列と行列の積を伴うだろう。これは、全体の計算をはるかにより高価にする。しかし、$\boldsymbol{A}$が$\boldsymbol{D}$が持つ列より少ない行を持つならば、forward modeに対応する左から右への乗算を実行することはより安価である。
機械学習の外の多くのコミュニティでは、PythonやCのコードのような、従来のプログラミング言語のコードで直接的に行われる微分ソフトウェアを実装したり、これらの言語でかかれる関数を微分するプログラムを自動的に生成したりすることはより一般的である。深層学習コミュニティでは、conputational graphsは一般に特殊化されたライブラリによって生成される明示的なデータ構造で表現される。その特殊化されたアプローチは、すべての演算に対して`bprop`メソッドを定義するようライブラリ開発者に要求したり、ライブラリのユーザを定義済みの演算のみに制限したりするという欠点を持つ。特殊化されたアプローチは依然として、カスタマイズされた逆伝播規則を演算ごとに開発することができるという利点も持ち、automatic procedureがおそらく複製できないであろうnonobviousな方法で速度や安定性を開発者が改善することを可能にする。
従って、逆伝播法は勾配を計算する唯一の方法でも最適な方法でもないが、深層学習コミュニティのために懸命に働き続ける実践的な手法である。将来的には、深層ネットワークに対する微分技術は、深層学習の専門家が自動微分の多岐にわたる分野における進歩に気が付くに従って改善するかもしれない。

### 高階微分

いくつかのソフトウェアフレームワークは高階微分の使用をサポートする。深層学習のソフトウェアフレームワークの中では、これは少なくともTheanoとTensorFlowを含む。これらのライブラリは微分に対する式を記述するために、微分されるオリジナルの関数を記述するのに使うのと同種のデータ構造を用いる。これは記号的な微分の仕組み[machinery]が微分に適用できることを意味する。
深層学習の文脈では、スカラの関数の単一の二階微分を計算することは稀である。代わりに、我々は通常ではヘッセ行列の特性に関心を持つ。関数$f : \mathbb{R}^n \rightarrow \mathbb{R}$があるならば、そのヘッセ行列は$n \times n$の大きさを持つ。一般的な深層学習アプリケーションでは、$n$はモデルにおけるパラメータ数であるだろう。これは容易に何十億にもなり得るだろう。故に、ヘッセ行列全体は表現することさえも実現不可能である。
ヘシアンを明示的に計算するのではなく、一般的な深層学習アプローチはクリロフ[Krylov]の手法を用いることになっている。Krylovの手法は、行列とベクトルの積以外の演算を用いずに、近似的に逆行列を求める、固有ベクトルや固有値の近似を求めるといった、様々な演算を行うための反復的なテクニック集である。
ヘシアンでKrylovの手法を用いるためには、ヘッセ行列$\boldsymbol{H}$と任意のベクトル$\boldsymbol{v}$の積を計算できる必要があるのみである。これを行うための素直なテクニック[@Christianson1992]は以下を計算することである。

$$
\boldsymbol{H} \boldsymbol{v} = \nabla_{\boldsymbol{x}} \left[ (\nabla_{\boldsymbol{x}} f(x))^\top \boldsymbol{v} \right]
$$

この式における両方の勾配計算は適切なソフトウェアライブラリによって自動で計算されるかもしれない。外側の勾配の式が内側の勾配の式の関数の勾配を取ることに注意する。
$\boldsymbol{v}$がcomputational graphによって生み出されるベクトルそれ自体であるならば、自動微分ソフトウェアが$\boldsymbol{v}$を生成したグラフを通して微分すべきでないと明示することは重要である。
ヘシアンを計算することは通常では得策ではないが、ヘシアンとベクトルの積で行うことができる。これは単にすべての$i = 1, \dots, n$に対して$\boldsymbol{H} \boldsymbol{e}^{(i)}$を計算する。ここで、$\boldsymbol{e}^{(i)}$は$e_i^{(i)} = 1$であり、かつ、その他すべての成分が$0$に等しいone-hotベクトルである。

## 歴史記録

フィードフォワードネットワークは関数近似における誤差を最小化するために勾配降下法を用いることに基づく効率的な非線形関数近似器として見ることができる。この視点から、近代的なフィードフォワードネットワークは一般的な関数近似タスクに関する何世紀に渡る進歩の集大成である。
逆伝播法の下地と成る連鎖律は17世紀に発明された[@Leibniz1676; @LHopital1696]。微積分学や代数学は閉形式における最適化問題を解くのに長らく使われてきたが、勾配降下法は19世紀に入るまで最適化問題への解を反復的に近似するための技術として導入されなかった[@Cauchy1847]。
1940年代の初頭には、これらの関数近似技術はパーセプトロンのような機械学習モデルを動機付けるために使われた。しかし、最初期のモデルは線形モデルに基づいていた。Marvin Minskyを含む批評家たちは、XOR関数を学習できないといった、線形モデル族の欠陥のいくつかを指摘した。これは、ニューラルネットワークアプローチ全体に対する反動につながった。
非線形関数を学習することは多層パーセプトロンの発展やそのようなモデルを通して勾配を計算する方法を必要とした。動的計画法に基づく連鎖律の効率的なアプリケーションは1960年代と1970年代に現れ始めた。そのほとんどは制御アプリケーション用であったが[@Kelley1960; @Bryson1961; @Dreyfus1962; @Bryson1969; @Dreyfus1973]、感度分析用もあった[@Linnainmaa1976]。@Werbos1981はこれらの技術を人工ニューラルネットワークを訓練するのに適用することを提案した。このアイデアは様々な方法で独立して再発見された後に実践で最終的に開発された[@LeCun1985; @Parker1985; @Rumelhart1986a]。Parallel Distributed Processingという本はある章で逆伝播法によるはじめて成功した実験の結果を提示した[@Rumelhart1986b]。これは、逆伝播法の普及に大きく貢献し、多層ニューラルネットワークにおける研究の活発な期間の口火を切った。その本の著者ら、特に、RumelhartとHintonによって提唱されたこのアイデアは逆伝播法をはるかに超えている。これらは認識や学習のいくつかの中心的な側面の取り得る計算の実装についての重要なアイデアを含んでいる。これは、この学派が学習や記憶のlocusとしてのニューロン間の接続に重きを置くので、コネクショニズムという名前に分類された。特に、これらのアイデアは分散表現の概念を含んでいる[@Hinton1986]。
逆伝播法の成功に続き、ニューラルネットワーク研究は人気を得、1990年代初頭にはピークに達した。その後、他の機械学習技術は2006年に始まった近代的深層学習の再興までさらに人気となった。
近代のフィードフォワードネットワークの背後にある核となるアイデアは1980年代から本質的には変化してきていない。同じ逆伝播アルゴリズムや同じ勾配降下法へのアプローチは依然として使われている。1986年から2015年までのニューラルネットワークのパフォーマンスにおける改善のほとんどは2つの要因に起因すると考えることができる。1つ目として、データセットが大きくなると、統計的な一般化がニューラルネットワークに対して課題となる度合いを減少させた。2つ目として、ニューラルネットワークは、よりパワフルなコンピュータやより良いソフトウェアインフラを理由に、かなり大きくなった。少量のアルゴリズム的な変更もまたニューラルネットワークを著しく改善した。
これらのアルゴリズム的変更の1つは損失関数の交差エントロピー族で平均二乗誤差を置き換えることであった。平均二乗誤差は1980年代や1990年代には人気であったが、アイデアが統計学コミュニティと機械学習コミュニティの間に広がるに従って、交差エントロピーの損失や最大尤度の原則に徐々に置き換えられた。交差エントロピーの損失の使用はsigmoidやsoftmaxの出力を持つモデルのパフォーマンスを大幅に改善した。これは、以前に平均二乗誤差の損失を用いていたときには飽和や低速な学習に悩まされていた。
フィードフォワードネットワークのパフォーマンスを大幅に改善した他の主なアルゴリズム的変更は***
