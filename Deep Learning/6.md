# 深層フィードフォワードネットワーク

フィードフォワードニューラルネットワーク[feedforward neural networks]とか多層パーセプトロン[multilayer perceptrons]（MLPs）とも呼ばれる、深層フィードフォワードネットワーク[deep feedforward networks]は典型的な深層学習モデルである。フィードフォワードネットワークの目標はある関数$f^*$を近似することである。例えば、分類器に対して、$y = f^*(\boldsymbol{x})$は入力$\boldsymbol{x}$をカテゴリ$y$に対応させる。フィードフォワードネットワークはマッピング$\boldsymbol{y} = f(\boldsymbol{x}; \boldsymbol{\theta})$を定義し、最良の関数近似となるパラメータ$\boldsymbol{\theta}$の値を学習する。
これらのモデルは、$\boldsymbol{x}$から計算される関数を通り、$f$を定義するのに使われる途中計算を通り、最終的に出力$\boldsymbol{y}$へ情報が流れるので、フィードフォワード[feedforward]と呼ばれる。モデルの出力がそれ自体に戻されるフィードバック[feedback]接続は存在しない。フィードフォワードニューラルネットワークがフィードバック接続を含むよう拡張されるとき、これらは、[@sec:10]で示されるように、リカレントニューラルネットワーク[recurrent neural networks]と呼ばれる。
フィードフォワードネットワークは機械学習の専門家にとって極めて重要である。これらは多くの重要な商用アプリケーションの基礎を形成する。例えば、写真からの物体認識で使われる畳み込みネットワークは特殊化された種類のフィードフォワードネットワークである。フィードフォワードネットワークはリカレントネットワークに至るための概念的な足がかり[stepping stone]である。これは多くの自然言語アプリケーションに力を与える。

フィードフォワードニューラルネットワークは、多数の様々な関数とともに構成することで一般に表現されるので、ネットワークと呼ばれる。そのモデルはどの関数がともに構成されるかを述べる有向非巡回グラフ[directed acyclic graph]と関連付けられる。例えば、$f(\boldsymbol{x}) = f^{(3)}(f^{(2)}(f^{(1)}(\boldsymbol{x})))$を形成するために、鎖状に接続される3つの関数$f^{(1)}$、$f^{(2)}$、$f^{(3)}$があるかもしれない。これらの鎖構造は最も一般的に使われるニューラルネットワークの構造である。この場合、$f^{(1)}$はネットワークの第一層[first layer]と呼ばれ、$f^{(2)}$は第二層と呼ばれ、以下同文。その鎖の全体の長さはモデルの深さ[depth]を与える。「深層学習」という名前はこの用語法により生じる。フィードフォワードネットワークの最後の層は出力層[output layer]と呼ばれる。ニューラルネットワークの訓練中では、$f(\boldsymbol{x})$を$f^*(\boldsymbol{x})$と一致させる。訓練データは異なる訓練点で計算される$f^*(\boldsymbol{x})$のノイジーで近似的なexamplesを提供する。各example$\boldsymbol{x}$にはラベル$y \approx f^*(\boldsymbol{x})$が伴う。訓練examplesは出力層が各点$\boldsymbol{x}$でしなければならないことを直接指定する。すなわち、$y$に近い値を生成しなければならないということである。他の層の振る舞いは訓練データによって直接指定されない。学習アルゴリズムは望ましい出力を生成するためにこれらの層の使い方を決定しなければならないが、訓練データはそれぞれ別個の層がすべきことを語らない。代わりに、学習アルゴリズムは$f^*$の近似を最もうまく実装するためにこれらの層の使い方を決めなければならない。訓練データはこれらの層のそれぞれに対して望ましい出力を示さないので、これらは隠れ層[hidden layers]と呼ばれる。
最後に、これらのネットワークは、これらが恐らく神経科学に触発されるので、ニューラルと呼ばれる。ネットワークの各隠れ層は一般にベクトル値的である。これらの隠れ層の次元はモデルの幅[width]を定める。ベクトルの各要素はニューロンに類似した役割を担うと解釈できるだろう。この層を単一のベクトルからベクトルへの関数を表現するとみなすのではなく、並列にそれぞれがベクトルからスカラへの関数を表現するように振る舞う多くのユニット[units]から成るとみなすこともできる。各ユニットは多くの他のユニットから入力を受け取り、自身の活性化の値を計算するという意味でニューロンに似ている。ベクトル値的な表現の多数の層を用いるというアイデアは神経科学から着想を得ている[be drawn from]。これらの表現を計算するのに使われる関数$f^{(i)}(\boldsymbol{x})$の選択もまた生物学的なニューロンが計算する関数についての神経学的な見解によって大まかに導かれる。しかしながら、近代的なニューラルネットワーク研究は多くの数学および工学の分野によって導かれ、ニューラルネットワークの目標は脳を完璧にモデル化することではない。脳の関数のモデルとしてではなく、脳について知っていることからいくつかの洞察を時折得て、統計的な汎化を達成するために設計された関数近似の機械としてフィードフォワードネットワークをみなすことは最も適当である。
フィードフォワードネットワークを理解する1つの方法は、線形モデルから始めて、これらの制限を克服する方法を考えることである。ロジスティック回帰や線形回帰のような線形モデルは、閉形式で、または、凸最適化に、のいずれかで効率的かつ確実にフィットさせることができるので、魅力的である。線形モデルは、モデル容量が一次関数に制限されるので、そのモデルが2つの入力変数の間の相互作用を理解できない、という明らかな欠陥も持つ。
線形モデルを$\boldsymbol{x}$の非線形関数を表現するよう拡張するためには、$\boldsymbol{x}$自体ではなく変換した入力$\phi(\boldsymbol{x})$に線形モデルを適用することができる。ここで、$\phi$は非線形な変換である。等価的に、我々は、$\phi$マッピングを暗黙的に適用することに基づいた非線形な学習アルゴリズムを得るために、[@sec:5.7.2]で述べられるカーネルトリックを適用できる。我々は$\phi$を、$\boldsymbol{x}$を述べる特徴の集合をもたらす、または、$\boldsymbol{x}$に対する新しい表現をもたらすとみなすことができる。
故に、疑問となるのはマッピング$\phi$を選択する方法である。

1. 1つの選択肢は、RBFカーネルに基づくカーネルマシンで暗黙的に用いられる無限次元の$\phi$のような、非常に汎用的な$\phi$を用いることである。$\phi(\boldsymbol{x})$が十分に高次元であるならば、我々は常に訓練セットにフィットするのに十分な容量を持つが、テストセットへの汎化はしばしばpoorなままである。非常に汎用的な特徴マッピングは通常、局所的な平滑さの原則にのみ基づき、発展的な問題を解くための十分な事前情報をエンコードしない。
2. もう1つの選択肢は$\phi$を手作業で設計することである。深層学習の到来までは、これが支配的なアプローチであった。音声認識やコンピュータビジョンのような異なる分野に特化した専門家によって、分野間をほとんど移ったりせずに、別個のタスクごとに何十年の労力を必要とする。
3. 深層学習の戦略は$\phi$を学習することである。このアプローチでは、モデル$y = f(\boldsymbol{x}; \boldsymbol{\theta}, \boldsymbol{w}) = \phi(\boldsymbol{x}; \boldsymbol{\theta})^\top \boldsymbol{w}$を持つ。我々はいま、関数の広範囲のクラスから$\phi$を学習するのに使うパラメータ$\boldsymbol{\theta}$、および、$\phi(\boldsymbol{x})$から望ましい出力へマッピングするパラメータ$\boldsymbol{w}$を持つ。これは隠れ層を定義する$\phi$を持つ深層フィードフォワードネットワークの例である。このアプローチは3つの中で唯一、訓練問題の凸性を断念するが、その益は害を上回る。このアプローチでは、$\phi(\boldsymbol{x}; \boldsymbol{\theta})$として表現をパラメータ化し、良い表現に対応する$\boldsymbol{\theta}$をもとめるために最適化アルゴリズムを用いる。望むならば、このアプローチは高度に汎用的にする、すなわち、非常に広範囲な仲間$\phi(\boldsymbol{x}; \boldsymbol{\theta})$を用いることでそれを行うことによって1番目のアプローチの利点を獲得できる。深層学習は二番目のアプローチの利点を獲得することもできる。人間の専門家はうまく処理するであろうと期待する仲間$\phi(\boldsymbol{x}; \boldsymbol{\theta})$を設計することによって汎化を助けるための知識をエンコードできる。この利点は人間の設計者が、正確に正しい関すを求めるのではなく、正しい一般的な関数の仲間を求める必要があるだけであるということである。

特徴を学習することによってモデルを改良するためのこの一般的な原則はこの章で述べられるフィードフォワードネットワーク以上に拡張される。これは本書を通して述べられるすべての種類のモデルに適用される深層学習の繰り返されるテーマである。フィードフォワードネットワークはフィードバック接続を欠いた$\boldsymbol{x}$から$\boldsymbol{y}$への決定論的なマッピングの学習に対してこの原則の応用である。後に示される他のモデルは確率的なマッピング、フィードバックを伴う関数、および、単一のベクトル上の確率分布の学習に対してこれらの原則を適用する。
我々は本章をフィードフォワードネットワークの単純な例から始める。次に、フィードフォワードネットワークを配備するために必要な設計上の決定のそれぞれを扱う。第一に、フィードフォワードネットワークを訓練することは線形モデルに対して必要なものと同じ設計上の決定の多く、すなわち、オプティマイザ、コスト関数、出力ユニットの形式を選択することを行う必要がある。我々はこれらの勾配ベースの学習の基礎を再調査し、続けて、フィードフォワードネットワークに固有の設計上の決定をのいくつかと突き合わせる。フィードフォワードネットワークが隠れ層の概念を導入したことで、我々は隠れ層の値を計算するのに使われるであろう活性化関数[activation functions]を選ぶ必要がある。また、ネットワークがどれだけの層を含むべきか、それらの層が互いにどのように接続すべきか、各層にどれだけのユニットがあるべきか、を含めて、ネットワークのアーキテクチャも設計しなければならない。深層ニューラルネットワークにおける学習は複雑な関数の勾配を計算する必要がある。我々は、これらの勾配を効率的に計算するために使うことができる、逆伝播[back-propagation]アルゴリズムとその近代的な一般化を示す。最後に、いくつかの歴史的な視点で閉める。

## 例：XORの学習

フィードフォワードネットワークのアイデアをより具体的にするため、我々は非常に単純なタスクに関する完全に機能するフィードフォワードネットワークの例、すなわち、XOR関数の学習から始める。
XOR（排他的論理和）関数は2つのバイナリ値$x_1$と$x_2$に関する操作である。これらのバイナリ値の1つだけが$1$に等しいとき、XOR関数は$1$を返す。そうでなければ、$0$を返す。XOR関数は学習したい目的関数$y = f^*(\boldsymbol{x})$をもたらす。我々のモデルは関数$y = f(\boldsymbol{x}; \boldsymbol{\theta})$をもたらし、我々の学習アルゴリズムは$f^*$にできるだけ似た$f$を作るためにパラメータ$\boldsymbol{\theta}$を適合させる。
この単純な例では、統計的な一般化について考えさせられることはないだろう。我々はネットワークが4つの点$\mathbb{X} = {[0, 0]^\top, [0, 1]^\top, [1, 0]^\top, \text{ and } [1, 1]^\top}$に関して正確に処理することを欲する。我々はこれらの点の4つすべてでネットワークを訓練するだろう。唯一の課題は訓練セットをフィットさせることのみである。
我々はこの問題を回帰問題として扱い、平均二乗誤差の損失関数を用いることができる。我々はこの例に対する計算をできるだけ単純化しようとしてこの損失関数を選んだ。実践的なアプリケーションでは、MSEは通常ではバイナリデータのモデル化に対して適切なコスト関数ではない。より適切なアプローチは[@6.2.2.2]で述べられる。
訓練セット全体で計算を行うとすると、MSEの損失関数は以下となる。

$$
J(\boldsymbol{\theta}) = \frac{1}{4} \sum_{\boldsymbol{x} \in \mathbb{X}} (f^*(\boldsymbol{x}) - f(\boldsymbol{x}; \boldsymbol{\theta}))^2
$$

すると、我々はモデルの形式$f(\boldsymbol{x}; \boldsymbol{\theta})$を選択しなければならない。$\boldsymbol{w}$と$b$から成る$\boldsymbol{\theta}$を持つ線形モデルを選択するとしよう。我々のモデルは以下であると定義される。

$$
f(\boldsymbol{x}; \boldsymbol{w}, b) = \boldsymbol{x}^\top \boldsymbol{w} + b
$$

我々は正規方程式を用いて$\boldsymbol{w}$と$b$に関する閉形式で$J(\boldsymbol{\theta})$を最小化できる。
正規方程式を解くと、$\boldsymbol{w} = \boldsymbol{0}$と$b = \frac{1}{2}$を得る。この線形モデルはいたるところで$0.5$を出力するだけである。なぜこんなことが起こるのだろう？[@fig:6.1]はなぜ線形モデルがXOR関数を表現できないかを示す。この問題を解決する1つの方法は線形モデルが解を表現できるような異なる特徴空間を学習するモデルを用いることである。

![表現を学習することでXOR問題を解く。プロット上に表示される太字の数字は学習した関数が各点で出力しなければならない値を示す。（左）元の値に直接適用した線形モデルはXOR関数を実装できない。$x_1 = 0$のとき、モデルの出力は$x_2$が増加するに従って増加しなければならない。$x_1 = 1$のとき、モデルの出力は$x_2$が増加するに従って減少しなければならない。線形モデルは固定の係数$w_2$を$x_2$に適用しなければならない。従って、その線形モデルは$x_2$に関する係数を変化させるために$x_1$の値を用いることができず、この問題を解くことができない。（右）ニューラルネットワークによって抽出された特徴で表現される変換した空間では、線形モデルはその問題を解くことができる。我々の解法の例では、$1$を出力しなければならない2つの点は特徴空間において単一の点に潰されてしまっている。別の言い方をすれば、その非線形な特徴は$\boldsymbol{x} = [1, 0]^\top$と$\boldsymbol{x} = [0, 1]^\top$の両方を特徴空間における単一の点$\boldsymbol{h} = [1, 0]^\top$にマッピングさせた。その線形モデルは$h_1$における増加と$h_2$における減少として関数を記述できる。この例では、特徴空間を学習することに対する動機は訓練セットにフィットできるようにモデル容量をより大きくすることのみである。より現実的なアプリケーションでは、学習した表現もまたモデルが汎化するのに役立ち得る。](fig/6-1.png){#fig:6.1}

具体的に言うと、我々は2つの隠れユニットを含む1つの隠れ層を持つ単純なフィードフォワードネットワークを導入するだろう。このモデルの図は[@fig:6.2]を参照のこと。このフィードフォワードネットワークは関数$f^{(1)}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c})$によって計算される隠れユニットのベクトル$\boldsymbol{h}$を持つ。そして、これらの隠れユニットの値は第二層に対する入力として使われる。第二層はネットワークの出力層である。出力層は依然として単なる線形回帰モデルであるが、いまや$\boldsymbol{x}$ではなく$\boldsymbol{h}$に適用されている。ネットワークはともに連鎖する2つの関数$\boldsymbol{h} = f^{(1)}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c})$と$f^{(2)}(\boldsymbol{h}; \boldsymbol{w}, b)$を含み、完全なモデルは$f(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c}, \boldsymbol{w}, b) = f^{(2)}(f^{(1)}(\boldsymbol{x}))$となる。

![2つの異なるスタイルで描かれるフィードフォワードネットワークの例。具体的に、これはXORの例を解くのに使われるフィードフォワードネットワークである。これは2つのユニットを含む単一の隠れ層を持つ。（左）このスタイルでは、すべてのユニットをグラフにおけるノードとして描く。このスタイルは明示的で曖昧さがないが、この例より大きなネットワークに対しては、場所を取りすぎる可能性がある。（右）このスタイルでは、層の活性化を表現するベクトル全体ごとにグラフにおけるノードを描く。このスタイルは更にコンパクトである。時折、我々はこのグラフの辺に2つの層の間の関係を記述するパラメータの名前で注釈を付ける。ここでは、行列$\boldsymbol{W}$が$\boldsymbol{x}$から$\boldsymbol{h}$へのマッピングを記述し、ベクトル$\boldsymbol{w}$が$\boldsymbol{h}$から$y$へのマッピングを記述することを示す。我々は一般に、この種の図をラベル付けするとき、各層に関連付けられる切片パラメータを省略する。](fig/6-2.png){#fig:6.2}

$f^{(1)}$が計算すべき関数とは何だろうか？線形モデルはこれまで我々の役に立ってきたので、$f^{(1)}$を同様に線形にしたくなるかもしれない。残念ながら、$f^{(1)}$が線形であったならば、全体としてのフィードフォワードネットワークはその入力の一次関数をそのままにしただろう。今のところは切片項を無視して、$f^{(1)}(\boldsymbol{x}) = \boldsymbol{W}^\top \boldsymbol{x}$および$f^{(2)}(\boldsymbol{h}) = \boldsymbol{h}^\top \boldsymbol{w}$であるとする。故に、$f(\boldsymbol{x}) = \boldsymbol{x}^\top \boldsymbol{W} \boldsymbol{w}$である。我々は$\boldsymbol{w}' = \boldsymbol{W} \boldsymbol{w}$であるところの$f(\boldsymbol{x}) = \boldsymbol{x}^\top \boldsymbol{w}'$としてこの関数を表現できるだろう。
明らかに、我々は特徴を記述するための非線形な関数を用いなければならない。ほとんどのニューラルネットワークは学習したパラメータによって制御され、活性化関数と呼ばれる固定の非線形関数へと続くアフィン変換を用いてこれを行う。$\boldsymbol{h} = g(\boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{c})$を定義することによって、ここではその戦略を用いる。ここで、$\boldsymbol{W}$は線形変換の重みをもたらし、$\boldsymbol{c}$はバイアスをもたらす。以前に、線形回帰モデルを記述するため、入力ベクトルから出力スカラへのアフィン変換を記述するための重みのベクトルとスカラのバイアスのパラメータを用いた。いま、我々はベクトル$\boldsymbol{x}$からベクトル$\boldsymbol{h}$へのアフィン変換を記述するので、バイアスパラメータのベクトル全体が必要である。活性化関数$g$は一般に$h_i = g(\boldsymbol{x}^\top \boldsymbol{W}_{:,i} + c_i)$を用いて要素ごとに適用される関数となるように選ばれる。近代的なニューラルネットワークでは、既定のオススメは、[@fig:6.3]に図示される、$g(z) = \max{0, z}$によって定義される正規化線形ユニット[rectified linear unit]またはReLU[@Jarrett2009; @Nair2010; @Glorot2011a]を用いることである。

![正規化線形[rectified linear]活性化関数。この活性化関数はほとんどのフィードフォワードニューラルネットワークで使用を推奨される既定の活性化関数である。線形変換の出力にこの関数を適用することは非線形変換を生み出す。しかし、その関数は2つの線形な区分を持つ線形区分関数であると言う意味において非常に線形に近い状態を維持する。正規化線形ユニットはほぼ線形であるので、線形モデルが勾配ベースの手法で最適化されることを容易にする特性の多くを保持する。線形モデルをうまく汎化させる特性の多くも保持する。計算機科学全体に共通の原則とは最小限の構成要素から複雑なシステムを作ることができるということである。0か1を格納できることだけが必要なチューリングマシンのメモリと同じように、正規化線形関数から普遍的関数近似器を作ることができる。](fig/6-3.png){#fig:6.3}

我々は以下として完全なネットワークを指定できる。

$$
f(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{c}, \boldsymbol{w}, b) = \boldsymbol{w}^\top \max{0, \boldsymbol{W}^\top \boldsymbol{x} + \boldsymbol{c}} + b
$$

故に、我々はXOR問題への解を指定できる。以下であるとする。

$$
\boldsymbol{W} = \left[ \begin{matrix}
1 & 1 \\
1 & 1
\end{matrix} \right]
$$

$$
\boldsymbol{c} = \left[ \begin{matrix}
0 \\
1
\end{matrix} \right]
$$

$$
\boldsymbol{w} = \left[ \begin{matrix}
1 \\
-2
\end{matrix} \right]
$$

そして、$b = 0$である。
すると、モデルが入力のバッチをどのように処理するかを見て回ることができる。$\boldsymbol{X}$を、各行に1つのexampleを持ち、バイナリの入力空間における4つすべての点を含む計画行列であるとする。

$$
\boldsymbol{X} = \left[ \begin{matrix}
0 & 0 \\
0 & 1 \\
1 & 0 \\
1 & 1
\end{matrix} \right]
$$

ニューラルネットワークにおける第1ステップは第一層の重み行列を入力行列に乗算することである。

$$
\boldsymbol{XW} = \left[ \begin{matrix}
0 & 0 \\
1 & 1 \\
1 & 1 \\
2 & 2
\end{matrix} \right]
$$

次に、以下をえるためにバイアスベクトル$\boldsymbol{c}$を加算する。

$$
\left[ \begin{matrix}
0 & -1 \\
1 & 0 \\
1 & 0 \\
2 & 1
\end{matrix} \right]
$$

この空間では、examplesすべては傾斜$1$の線に沿って存在する。我々はこの線にそって移動するので、出力は$0$で始まり、$1$に上がり、そして、$0$に下がる必要がある。線形モデルはこのような関数を実装できない。exampleごとの$\boldsymbol{h}$の値を計算し終えるため、我々は正規化された[rectified]線形変換を適用する。

$$
\left[ \begin{matrix}
0 & 0 \\
1 & 0 \\
1 & 0 \\
2 & 1
\end{matrix} \right]
$$

この変換はexamples間の関係を変化させてしまう。これらはもはや単一の直線上に存在しない。[@fig:6.1]に示される通り、これらは、線形モデルがこの問題を解くことができるところの空間に置かれる。
重みベクトル$\boldsymbol{w}$を乗算して終いとする。

$$
\left[ \begin{matrix}
0 \\
1 \\
1 \\
0
\end{matrix} \right]
$$

ニューラルネットワークはバッチにおけるすべてのexampleに対する正しい答えを得た。
この例では、我々は単純に解を指定し、そして、誤差ゼロを得ることを示した。現実の状況では、無数のモデルパラメータと無数の訓練examplesがあるかもしれず、そのため、ここで行ったように解を推測できない。代わりに、勾配ベースの最適化アルゴリズムは非常に小さな誤差を生成するパラメータを求めることができる。XOR問題に対して示した解は損失関数の最小におけるものであり、そのため、勾配降下法はこの点に収束できるだろう。勾配降下法が求めることもできるであろうXOR問題に対する他の等価的な解が存在する。勾配降下法の収束点はパラメータの初期値に依存する。実践では、勾配降下法は、ここで示したもののような、明確で容易に理解できる整数値の解を求めることは通常ではないだろう。

## 勾配ベース学習

ニューラルネットワークを設計および訓練することは勾配降下法を伴う他のいずれかの機械学習モデルを訓練するのとそこまで違いがない。[@sec:5.10]では、最適化の手順、コスト関数、モデルの族を指定することで機械学習アルゴリズムを作る方法を述べた。
これまでに見てきた線形モデルとニューラルネットワークとの最も大きな違いはニューラルネットワークの非線形性が最も関心のある損失関数を非凸にしてしまうことである。これはニューラルネットワークが通常では、ロジスティック回帰またはSVMsを訓練するのに使われる大域収束性の保証を持つ線形回帰モデルまたは凸最適化アルゴリズムを訓練するのに使われる一次方程式ソルバではなく、単にコスト関数を非常に低い値にするだけの反復的で勾配ベースのオプティマイザを用いることで訓練されることを意味する。凸最適化は任意の食パラメータから開始して収束する（理論的にはそうだが、実践ではロバストだが数値的な問題に遭遇する可能性がある）。非凸な損失関数に適用される確率的勾配降下法はそのような収束保証を持たず、初期パラメータの値に敏感である。フィードフォワードニューラルネットワークにとって、すべての重みを小さなランダム値に初期化することは重要である。フィードフォワードネットワークや他のほとんどすべての深層モデルを訓練するのに使われる反復的な勾配ベースの最適化アルゴリズムは[@sec:8]にてその詳細を述べ、それとともに、特にパラメータの初期化を[@sec:8.4]で述べる。今のところは、訓練アルゴリズムがほとんど常にあれやこれやでコスト関数を降下するために勾配を用いることに依存していると理解するので十分である。[@sec:4.3]で導入される特定のアルゴリズムは勾配降下法のアイデアの改良および洗練したものであり、より具体的に言えば、[@sec:5.9]で導入されるものはほとんどの場合の確率的勾配降下法の改良である。
もちろん、我々は線形回帰、勾配降下法を伴うサポートベクトルマシンのようなモデルも訓練することができる。実際に、これは訓練セットが極めて大きいときに一般的である。この視点から、ニューラルネットワークを訓練することは他のいずれのモデルを訓練することともほとんど違いがない。勾配を計算することはニューラルネットワークに対する方が若干複雑であるが、依然として効率的かつ厳密に行うことができる。[@sec:6.5]では、逆伝播アルゴリズムおよび逆伝播アルゴリズムを一般化した近代的なものを用いて勾配を得る方法を述べる。
他の機械学習モデルと同様に、勾配ベースの学習を適用するため、我々はコスト関数を選択しなければならず、モデルの出力を表現する方法を選択しなければならない。我々はニューラルネットワークのシナリオに特に重点をおいてこれらの設計上の懸念事項を再訪する。

### コスト関数

深層ニューラルネットワークの設計の重要な側面はコスト関数の選択である。ニューラルネットワークに対するコスト関数は、線形モデルのような、他のパラメトリックモデルに対するコスト関数と多かれ少なかれ同じである。
ほとんどの場合、我々のパラメトリックモデルは分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$を定義し、我々は単純に最大尤度の原則を用いる。これは我々が訓練データとコスト関数としてのモデルの予測値の間の交差エントロピーを用いることを意味する。
時折、我々はより単純なアプローチを取る。そこでは、$\boldsymbol{y}$上の完全な確率分布を予測するのではなく、$\boldsymbol{x}$で条件付けられた$\boldsymbol{y}$のある統計量を単に予測するだけである。特殊化された損失関数はこれらの推定値の予測器を訓練することを可能にする。
ニューラルネットワークを訓練するのに使われる総コスト関数はしばしば、正則化項を持つここで述べたprimary cost functionsのひとつを組み合わせるだろう。我々は[@sec:5.2.2]で線形モデルに適用される正規化の単純な例のいくつかをすでに見てきている。線形モデルに使われるweight decayアプローチもまた深層ニューラルネットワークに直接適用可能であり、最も人気のある正則化戦略に入る。ニューラルネットワークに対するより発展的な正則化戦略は[@sec:7]で述べられる。

#### 最大尤度を伴う条件付き分布の学習

最も近代的なニューラルネットワークは最大尤度を用いて訓練される。これはコスト関数が単純に負の対数尤度であり、訓練データとモデル分布の間の交差エントロピーとして等価的に述べられることを意味する。

$$
J(\boldsymbol{\theta}) = -\mathbb{E}_{\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}} \sim \hat{p}_{data}} \log p_{model}(\boldsymbol{y} | \boldsymbol{x})
$$

コスト関数の特定の形式は、$\log p_{model}$の特定の形式に依存して、モデルからモデルへと変化する。上記の式の展開は一般にモデルパラメータに依存せず、破棄できるだろういくつかの項を生み出す。例えば、[@sec:5.5.1]で見た通り、$p_{model}(\boldsymbol{y} | \boldsymbol{x}) = \mathcal{N}(\boldsymbol{y}; f(\boldsymbol{x}; \boldsymbol{\theta}), \boldsymbol{I})$であるならば、$\frac{1}{2}$のスケーリングファクタと$\boldsymbol{\theta}$に依存しない項を除いて[up to]、以下の平均二乗誤差のコストを復元する。

$$
J(\theta) = \frac{1}{2} \mathbb{E}_{\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}} \sim \hat{p}_{data}} \| \boldsymbol{y} - f(\boldsymbol{x}; \boldsymbol{\theta}) \|^2 + \text{const}
$$

破棄される定数は、この場合ではパラメータ化しないことを選択した、ガウス分布の分散に基づいている。以前に、我々は出力分布を伴う最尤推定と平均二乗誤差の最小化の間の等価性が線形モデルに対して成り立つことを確認したが、実際には、その等価性はガウシアンの平均を予測するのに使われる$f(\boldsymbol{x}; \boldsymbol{\theta})$にも関わらず成り立つ。
最大尤度からコスト関数を導出するというこのアプローチの利点はモデルごとにコスト関数を設計することの負荷を取り除くことである。モデル$p(\boldsymbol{y} | \boldsymbol{x})$を指定することはコスト関数$\log p(\boldsymbol{y} | \boldsymbol{x})$を自動的に決定する。
ニューラルネットワーク全体で繰り返されるひとつのテーマは、コスト関数の勾配が学習アルゴリズムに対する良き導き手として役立つのに十分大きくかつ予測可能でなければならないということである。飽和する（非常に平坦になる）関数は、勾配を非常に小さくするので、この目的を損なう。多くの場合、これは隠れユニットや出力ユニットの出力を生成するのに使われる活性化関数が飽和するために発生する。負の対数尤度は多くのモデルに対してこの問題を回避するのに役立つ。いくつかの出力ユニットは引数が非常に大きな負であるときに飽和し得る$\exp$関数を伴う。負の対数尤度のコスト関数における$\log$関数はいくつかの出力ユニットの$\exp$をもとに戻す。我々は[@sec:6.2.2]でコスト関数と出力ユニットの選択との相互作用を考察するだろう。
最尤推定を行うのに使われる交差エントロピーのコストの普通でない特性のひとつは実践で一般に使われるモデルへ適用されるときに最小値を通常では持たないことである。離散的な出力変数に対して、ほとんどのモデルはそのような0または1の確率を表現できないが、任意に近いところを表現できる方法でパラメータ化される。ロジスティック回帰はそのようなモデルの例である。実数値の出力変数に対して、（例えば、出力のガウス分布の分散パラメーtあを学習することで）モデルが出力分布の密度を制御できるならば、正しい訓練セットの出力に極めて高密度を割り当てることができるようになり、結果として交差エントロピーが負の無限大に近づく。[@sec:7]で述べられる正則化技術は学習問題を修正するいくつかの異なる方法をもたらす。すなわち、モデルはこの方法においてreap unlimited rewardことができない。

#### 条件付き統計量の学習

完全な確率分布$p(\boldsymbol{y} | \boldsymbol{x}; \boldsymbol{\theta})$を学習する代わりに、我々はしばしば$\boldsymbol{x}$を与えられたときの$\boldsymbol{y}$のただ1つの条件付き統計量を学習したい。
例えば、$\boldsymbol{y}$の平均を予測するために採用することを望む予測器$f(\boldsymbol{x}; \boldsymbol{\theta})$を持つかもしれない。
我々が十分に強力なニューラルネットワークを用いるならば、ニューラルネットワークを関数の幅広いクラスから任意の関数$f$を表現できるとみなすことができる。それとともに、このクラスは、特定のパラメトリックな形式持つことによってではなく、連続性や有界性のような特徴によってのみ制限される。この視点から、我々はコスト関数を単なる関数ではなく汎関数[functional]であるとみなすことができる。汎関数は関数から実数値へのマッピングである。故に、我々は学習を、パラメータの集合を選ぶだけではなく、関数を選択することと考えることができる。我々はその最小値をある望ましい特定の関数で発生させるためにコスト汎関数を設計できる。例えば、その最小値が$\boldsymbol{x}$を$\boldsymbol{x}$を与えられたときの$\boldsymbol{y}$の期待値にマッピングする関数にあるようにするためのコスト汎関数を設計できる。関数に関して最適化問題を解くことは、[@sec:19.4.2]で述べられる、変分法[calculus of variations]と呼ばれる数学上の道具を必要とする。この章の内容を理解するのに変分法を理解する必要はない。今のところは、変分法が以下の2つの結果を導くのに使割れるであろうことを理解するひつようがあるだけである。
変分法を用いて導出される1つ目の結果は、以下の最適化問題を解決することが、

$$
f^* = \argmin_f \mathbb{E}_{\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}} \sim p_{data}} \| \boldsymbol{y} - f(\boldsymbol{x}) \|^2
$$

最適化したいクラスの内にこの関数がある限り、以下を生み出すことである。

$$
f^*(\boldsymbol{x}) = \mathbb{E}_{\boldsymbol{\mathbf{y}} \sim p_{data}(\boldsymbol{y} | \boldsymbol{x})} [\boldsymbol{y}]
$$

言い換えれば、我々が真のデータ生成分布からの無限に多くのサンプルで訓練できたならば、平均二乗誤差のコスト関数を最小化することは$\boldsymbol{x}$の値ごとの$\boldsymbol{y}$の平均を予測する関数を与えただろう。
異なるコスト関数は異なる統計量を与える。変分法を用いて導出される2つ目の結果は、以下の関数は$\boldsymbol{x}$ごとの$\boldsymbol{y}$の中間値を予測する関数を生み出す。ただし、そのような関数が最適化したい関数の族によって記述され得るであろう場合に限る。

$$
f^* = \argmin_f \mathbb{E}_{\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}} \sim p_{data}} \| \boldsymbol{y} - f(\boldsymbol{x}) \|_1
$$

このコスト関数は平均絶対誤差[mean absolute error]と一般に呼ばれる。
残念ながら、平均二乗誤差と平均絶対誤差はしばしば、勾配ベースの最適化で用いられるときにまずい結果を引き起こす。いくつかの飽和する出力ユニットは、これらのコスト関数と組み合わさると非常に小さな勾配を生成する。これは、交差エントロピーのコスト関数の方が、分布$p(\boldsymbol{y} | \boldsymbol{x})$の全体を推定する必要がないときでさえも、平均二乗誤差や平均絶対誤差より人気である理由のひとつである。

### 出力ユニット

コスト関数の選択は出力ユニットの選択と強く結びついている。大抵の場合、我々は単純にデータ分布とモデル分布の間の交差エントロピーを用いる。従って、出力を表現する方法の選択は交差エントロピー関数の形式を決定する。
出力として使っても良いいずれの種類のニューラルネットワークユニットも隠れ層として用いることができる。ここでは、モデルの出力としてこれらのユニットを用いることに焦点を当てるが、原則として、これらは同様に内部的に用いることができる。我々は[@sec:6.3]で隠れユニットとしてこれらを用いることに関する追加の詳細とともにこれらのユニットを再訪する。
本節全体を通して、我々はフィードフォワードネットワークが$\boldsymbol{h} = f(\boldsymbol{x}; \boldsymbol{\theta})$で定義される隠れ特徴の集合をもたらすとする。故に、出力層の役割はネットワークが処理しなければならないタスクを完了するための特徴からいくつかの追加の変換を提供することである。

#### ガウシアン出力分布に対する線形ユニット

単純な種類の出力ユニットのひとつは非線形性を持たないアフィン変換に基づいている。これらはしばしば単に線形ユニットと呼ばれる。
特徴$\boldsymbol{h}$を与えられた場合、線形な出力ユニットの層はベクトル$\hat{\boldsymbol{y}} = \boldsymbol{W}^\top \boldsymbol{h} + \boldsymbol{b}$を生成する。
線形な出力層はしばしば条件付きガウス分布の平均を生成するのに使われる。

$$
p(\boldsymbol{y} | \boldsymbol{x}) = \mathcal{N}(\boldsymbol{y}; \hat{\boldsymbol{y}}, \boldsymbol{I})
$$

故に、対数尤度を最大化することは平均二乗誤差を最小化することと等価である。
最大尤度のフレームワークはガウシアンの共分散を学習すること、または、ガウシアンの共分散を入力の関数とすることを簡単にする。しかし、共分散はすべての入力に対して正定値行列であるように制約しなければならない。線形な出力層ではそのような制約を満足させるのは難しく、そのために、一般的には他の出力ユニットが共分散をパラメータ化するために使われる。共分散のモデル化へのアプローチは、[@sec:6.2.2.4]にて、手短に述べられる。
線形ユニットは飽和しないので、これらは勾配ベースの最適化アルゴリズムにそこまでの困難をもたらさず、多種多様な最適化アルゴリズムで使われるだろう。

#### ベルヌーイ出力分布に対するシグモイドユニット

多くのタスクは二値変数$y$の値を予測する必要がある。2つのクラスを伴う分類問題はこの形式に当てはめることができる。最大尤度のアプローチは$\boldsymbol{x}$で条件付けられた$\boldsymbol{y}$上のベルヌーイ分布を定義することである。
ベルヌーイ分布は単なる単一の数値によって定義される。ニューラルネットは$P(y = 1 | \boldsymbol{x})$のみを予測する必要がある。この数値が有効な確率であるためには、区間$[0, 1]$になければならない。
この制約を満たすことはある慎重な設計上の努力を必要とする。有効な確率を得るために線形ユニットを用いてその値をしきい値としたとする。

$$
P(y = 1 | \boldsymbol{x}) = \max{0, \min{1, \boldsymbol{w}^\top \boldsymbol{h} + b}}
$$

もちろん、これは有効な条件付き分布を定義するだろうが、勾配降下法を用いて非常に効率的に訓練することはできないだろう。$\boldsymbol{w}^\top \boldsymbol{h} + b$が単位区間の外側にいるときならいつでも、そのパラメータに関するモデルの出力の勾配は$\boldsymbol{0}$となるだろう。$\boldsymbol{0}$の勾配は、学習アルゴリズムが対応するパラメータを改善する方法に対するガイドをもはや持たないので、普通は問題となる。
代わりに、モデルが間違った答えを持っているときでも常に強い勾配が存在することを保証する異なるアプローチを用いることがより良い。このアプローチは最大尤度と組み合わされたシグモイド出力ユニットを用いることに基づく。
シグモイド出力ユニットは以下によって定義される。

$$
\hat{y} = \sigma \left( \boldsymbol{w}^\top \boldsymbol{h} + b \right)
$$

ここで、$\sigma$は[@sec:3.10]で述べられるロジスティックシグモイド関数である。
我々は2つの構成要素を持つとシグモイド出力ユニットを考えることができる。1つ目は、$\boldsymbol{w}^\top \boldsymbol{h} + b$を計算するために線形な層を用いる。2つ目は、$z$を確率に変換するためにシグモイド活性化関数を用いる。
値$z$を用いて$y$上の確率分布を定義する方法を考察するために今のところは$x$への依存性を省略する。シグモイドは合計が$1$にならない非正規化確率分布$\tilde{P}(y)$を構築するとこによって動機付けできる。故に、我々は有効な確率分布を得るために適切な定数で除算できる。非正規化対数確率が$y$および$z$において線形であるという仮定からはじめるならば、非正規化確率を得るためにべき乗することができる。故に、我々はこれが$z$のsigmoidal変換によって制御されるベルヌーイ分布を生み出すことを確認するために正規化する。

$$
\log \tilde{P}(y) = yz
$$

$$
\tilde{P}(y) = \exp(yz)
$$

$$
P(y) = \frac{\exp(yz)}{\sum_{y' = 0}^1 \exp(y'z)}
$$

$$
P(y) = \sigma((2y - 1)z)
$$

べき乗と正規化に基づく確率分布は統計モデリング界隈全体で一般的である。そのような二値変数上の分布を定義する$z$変数はロジット[logit]と呼ばれる。
対数空間において確率を予測することへのこのアプローチは最尤学習で用いることは自然である。最大尤度で使われるコスト関数が$-\log P(y | \boldsymbol{x})$であるので、コスト関数中の$\log$はシグモイドの$\exp$をもとに戻す。この効果が無いと、シグモイドの飽和は勾配ベースの学習が順調に進むことを妨げる。シグモイドによってパラメータ化されたベルヌーイの最尤学習に対する損失関数は以下である。

$$
J(\boldsymbol{\theta}) = -\log P(y | \boldsymbol{x})
$$

$$
= -\log \sigma((2y - 1)z)
$$

$$
= \zeta((1 - 2y)z)
$$

この導出は[@sec:3.10]からのいくつかの特性を使用する。softplus関数の観点で損失を書き換えることで、$(1 - 2y)z$が非常に大きな負であるときにのみ飽和することを確認できる。故に、飽和はモデルがすでに正しい答えを持っているとき、すなわち、$y = 1$かつ$z$が非常に大きな正である、または、$y = 0$かつ$z$が非常に大きな負であるときにのみ発生する。$z$が間違った符号を持つとき、softplus関数への引数$(1 - 2y)z$は$|z|$に単純化されても良い。$z$が間違った符号を持ちながら、$|z|$が
大きくなるに従って、softplus関数は、単純にその引数$|z|$を返す方に向かって漸近する。$z$に関する微分は$\text{sign}(z)$に漸近し、そのため、極めて不正確な$z$の極限において、softmax関数はまったく勾配を縮小しない。この特性は、勾配ベースの学習が間違えた$z$をすぐさまに正しくすることができることを意味するので、有用である。
解析的に言うと、シグモイドが有効な確率の閉区間$[0, 1]$全体を用いるのではなく、開区間$(0, 1)$に制限された値を返すので、シグモイドの対数は常に定義され、有限である。ソフトウェア実装において、数値的な問題を避けるため、$\hat{y} = \sigma(z)$の関数としてではなく、$z$の関数として負の対数尤度を書くことが最良である。シグモイド関数がゼロにアンダーフローするならば、$\hat{y}$の対数を取ることは負の無限大を生み出す。

#### Multinoulli出力分布のためのsoftmaxユニット

$n$個の取り得る値を持つ離散変数上の確率分布を表現することを望むならいつでも、softmax関数を用いても良い。これは二値変数上の確率分布を表現するのに使われたシグモイド関数の一般化と見ることができる。
softmax関数は殆どの場合で、$n$個の異なるクラス上の確率分布を表現するため、分類器の出力として用いられる。より稀なことだが、モデルがある内部変数に対する$n$個の異なる選択肢の中から1つを選ぶことを望むならば、softmax関数はモデル自体の内部で用いることができる。
二値変数の場合、単一の数値を生成することを望んだ。

$$
\hat{y} = P(y = 1 | \boldsymbol{x})
$$

この数値は0と1の間にある必要があり、対数尤度の勾配ベース最適化に対してうまく振る舞わせるためのその数値の対数を欲したので、我々は代わりに数値$z = \log \tilde{P}(y = 1 | \boldsymbol{x})$を予測することを選んだ。べき乗と正規化はシグモイド関数によって制御されるベルヌーイ分布をもたらした。
$n$個の値を持つ離散変数の場合に一般化するため、***
