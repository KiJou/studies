# 線形代数

線形代数は科学や工学を通して広く用いられる数学の分野である。未だ線形代数は離散的な数学ではなく連続的な形式なので、多くの計算機科学者はそれの経験が乏しい。線形代数をきちんと理解することは多くの機械学習アルゴリズム、特に、深層学習アルゴリズムに対して理解したり扱ったりするために必須である。故に、我々は重要な線形代数の前提知識の集中的なプレゼンテーションから深層学習のイントロダクションを始める。
あなたがすでに線形代数に親しんでいるならば、この章をスキップしても一向に構わない。あなたがこれらの概念に関して以前に経験しているが、重要な式を復習するための詳細な参照表[reference sheet]を必要とするならば、我々はThe Matrix Cookbook[@Petersen2006]をオススメする。線形代数にまったく触れたことがないならば、この章は本書を読むのに十分なことを教えてくれるだろうが、我々は、[@Shilov1977]のような、線形代数を教えることのみに焦点を当てている他の資料も調べることを推奨している。この章は深層学習を理解するのに必須でない、多くの重要な線形代数の項目を完全に省略している。

## スカラ、ベクトル、行列、テンソル

線形代数の研究はいくつかの種類の数学的対象[mathematical objects]を伴う。

- スカラ：線形数学で研究される他のほとんどの対象が通常では複数の数値の列であるのに対して、スカラはただの単一の数値である。我々はスカラをイタリック体で記述する。我々は通常、スカラに小文字の変数名を与える。これらを導入するとき、それらが何の種類の数値であるかを指定する。例えば、実数のスカラを定義するのに「線の傾斜を$s \in \mathbb{R}$とする」と言えるし、自然数のスカラを定義するのに「単位の数を$n \in \mathbb{N}$とする」と言える。
- ベクトル：ベクトルは数値の列である。数値は順に配置される。我々はその順番の番号でそれぞれ個別の数値を識別できる。一般に、我々はベクトルに、$\boldsymbol{x}$のように、太字の小文字の名前を与える。ベクトルの要素は下付き文字を持つイタリック体でその名前を記述することで識別される。＄$\boldsymbol{x}$の1番目の要素は$x_1$であり、2番目の要素は$x_2$であり、3番目以降も同様である。我々はどんな種類の数値がベクトルに格納されるかを言う必要もある。各要素が$\mathbb{R}$にあり、ベクトルが$n$個の要素を持つならば、ベクトルは、$\mathbb{R}^n$と表記される、$\mathbb{R}$のデカルト積を$n$回行うことで形成される集合の中にある。ベクトルの要素を明示的に区別する必要があるとき、角括弧でトジされる列として記述する。

$$
\boldsymbol{x} = \left[ \begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{array} \right]
$$

我々はベクトルが、異なる軸に沿った座標をもたらす各要素を持つ、空間上の点を識別するとみなすことができる。
時折、我々はベクトルの要素の集合を指し示す必要がある。この場合、我々はその番号を含む集合を定義し、下付き文字として集合を記述する。例えば、$x_1$、$x_3$、$x_6$にアクセスするためには、集合$S = {1, 3, 6}$を定義し、$\boldsymbol{x}_S$と記述する。我々は補集合を示すために$-$記号を用いる。例えば、$\boldsymbol{x}_{-1}$は$x_1$を除く$\boldsymbol{x}$のすべての要素を含むベクトルであり、$\boldsymbol{x}_{-S}$は$x_1$、$x_3$、$x_6$を除く$\boldsymbol{x}$のすべての要素を含むベクトルである。
- 行列：行列は数値の２次元配列である。つまり、各要素は1つではなく2つの番号で識別される。我々は通常、$\boldsymbol{A}$のように、行列にボールド体で大文字の変数名を与える。実数値の行列$\boldsymbol{A}$が高さ$h$と幅$n$を持つならば、$\boldsymbol{A} \in \mathbb{R}^{m \times n}$と言う。我々は通常、行列の要素をボールド体ではなくイタリックでその名前を用いて識別する。また、その番号はコンマで分けて並べられる。例えば、$A_{1,1}$は$\boldsymbol{A}$の左上の成分であり、$A_{m,n}$は右下の成分である。我々は水平座標として$:$を記述することで垂直座標$i$のすべての数値を識別できる。例えば、$\boldsymbol{A}_{i,:}$は垂直座標$i$と$\boldsymbol{A}$の水平の交差領域を示す。これは$\boldsymbol{A}$の$i$番目の行として知られる。同様に、$\boldsymbol{A}_{:,i}$は$\boldsymbol{A}$の$i$番目の列である。我々が行列の要素を明示的に認識する必要があるとき、各カッコでトジされる列として記述する。

$$
\left[ \begin{array}{cc}
A_{1,1} & A_{1,2} \\
A_{3,1} & A_{3,2}
\end{array} \right]
$$

時折、我々は単一の文字でない行列の値の表記を示す必要があるかもしれない。この場合、ひその表記の後に下付き文字を用いるが、いずれも小文字に変換しない。例えば、$f(\boldsymbol{A})_{i,j}$は関数$f$を$\boldsymbol{A}$に適用して計算した行列の要素$(i,j)$をもたらす。
- テンソル：いくつかの場合、我々は2つより多い軸を持つ配列を必要とするだろう。一般的な場合、軸の変数を持つ規則的な格子上に配置される数値の配列はテンソルとして知られる。我々は"A"という名前のテンソルを$\mathsf{A}$の書体で表記する。我々は$\mathit{\mathsf{A}}_{i,j,k}$と記述することで座標$(i,j,k)$における$\mathsf{A}$の要素を識別する。

行列での重要な操作のひとつは転置である。行列の転置は対角線を境にした行列の鏡像である。この対角線は主対角線と呼ばれ、左上のカドから始まり、右下へと向かう。この操作の図解は[@fig:2.1]を参照のこと。我々は行列$\boldsymbol{A}$の転置を$\boldsymbol{A}^\top$と示し、これは以下のように定義される。

$$
(\boldsymbol{A}^\top)_{i,j} = A_{j,i}
$$

ベクトルは1列のみを含む行列として考えることができる。故に、ベクトルの転置は1行のみを持つ行列である。時折、我々は1行の行列として文中にその要素を書き出すことで、例えば$\boldsymbol{x} = [x_1, x_2, x_3]$のように、ベクトルを定義する。

![行列の転置は主対角線を境にした鏡像としてみなすことができる。](fig/2-1.png){#fig:2.1}

スカラは単一の成分のみを持つ行列として考えることができる。これにより、スカラはそれ自身の転置$a = a^\top$であることがわかる。
我々は、それらが同じ形である場合に限り、単に対応する要素を足し合わせることで、行列を互いに足すことができる。すなわち、$\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{B}$は$C_{i,j} = A_{i,j} + B_{i,j}$である。
我々は、行列の各要素にその操作を行うことで、スカラを行列に足したり掛けたりできる。すなわち、$\boldsymbol{D} = a \cdot \boldsymbol{B} + c$は$D_{i,j} = a \cdot B_{i,j} + c$である。
深層学習の文脈では、我々はいくつかの従来とは異なる表記法も用いる。我々は行列とベクトルを足し合わせて別の行列を生み出すことを許可する。すまわち、$\boldsymbol{C} = \boldsymbol{A} + \boldsymbol{b}$は$C_{i,j} = A_{i,j} + b_{j}$である。言い換えれば、ベクトル$\boldsymbol{b}$は行列の各行に足される。この省略表記は加算を行う前に各行にコピーされる$\boldsymbol{b}$で行列を定義する必要性を取り除く。この多くの場所への$\boldsymbol{b}$の暗黙的なコピーはbroadcastingと呼ばれる。

## 行列とベクトルの乗算

行列を伴う最も重要な操作のひとつは2つの行列の掛け算である。行列$\boldsymbol{A}$と$\boldsymbol{B}$の行列積は第三の行列$\boldsymbol{C}$である。この積が定義されるために、$\boldsymbol{A}$は$\boldsymbol{B}$が持つ行数と同じ列数を持たなければならない。$\boldsymbol{A}$が$m \times n$の形であり、$\boldsymbol{B}$が$n \times p$であれば、$\boldsymbol{C}$は$m \times p$の形である。我々は単に2つ以上の行列同士を配置することで行列積を記述できる。例えば以下のようになる。

$$
\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}
$$

この積の演算は以下のように定義される。

$$
C_{i,j} = \sum_k A_{i,k} B_{k,j}
$$

2つの行列の標準的な積は単に個々の要素の積を含む行列ではないことに注意したい。そのような操作は存在し、要素ごとの積[element-wise product]とかアダマール積[Hadamard product]と呼ばれ、$\boldsymbol{A}　\odot \boldsymbol{B}$と表記する。
同じ次元のベクトル$\boldsymbol{x}$と$\boldsymbol{y}$の内積は行列の積$\boldsymbol{x}^\top \boldsymbol{y}$である。我々は行列の積$\boldsymbol{C} = \boldsymbol{A} \boldsymbol{B}$を$\boldsymbol{A}$の$i$行目と$\boldsymbol{B}$の$j$列目の内積として$C_{i,j}$を計算するとみなすことができる。
行列の積の演算は行列の数学的な解析をより便利にする多くの有用な特性を持つ。例えば、行列の乗算は分配法則を満たす[distributive]。

$$
\boldsymbol{A} (\boldsymbol{B} + \boldsymbol{C}) = \boldsymbol{A} \boldsymbol{B} + \boldsymbol{A} \boldsymbol{C}
$$

また、結合法則も満たす[associative]。

$$
\boldsymbol{A} (\boldsymbol{B} \boldsymbol{C}) = (\boldsymbol{A} \boldsymbol{B}) \boldsymbol{C}
$$
{#eq:2.8}

行列の乗算は、スカラの乗算とは異なり、交換法則を満たさない[not commutative]（$\boldsymbol{A} \boldsymbol{B} = \boldsymbol{B} \boldsymbol{A}$は常に成り立つとは限らない）。しかし、2つのベクトルの内積は交換法則を満たす。

$$
\boldsymbol{x}^\top \boldsymbol{y} = \boldsymbol{y}^\top \boldsymbol{x}
$$

行列の積の転置は以下のような単純な形式を持つ。

$$
(\boldsymbol{A} \boldsymbol{B})^\top = \boldsymbol{B}^\top \boldsymbol{A}^\top
$$

これは、そのような積の値がスカラであるということ活用することで[@eq:2.8]を実証することができるようになる。故に、それ自身の転置と等価である。

$$
\boldsymbol{x}^\top \boldsymbol{y} = (\boldsymbol{x}^\top \boldsymbol{y})^\top = \boldsymbol{y}^\top \boldsymbol{x}
$$

本書の焦点は線形代数ではないので、我々はここで行列の積の有用な特性の包括的なリストを明らかにしようとはしないが、読者の皆は更にたくさん存在することに気付いているはずである。
今や我々は一次方程式の系を書き記すための線形代数の表記法を十分に知っている。

$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}
$$
{#eq:2.11}

ここで、$\boldsymbol{A} \in \mathbb{R}^{m \times n}$は既知の行列であり、$\boldsymbol{b} \in \mathbb{R}^m$は既知のベクトルであり、$\boldsymbol{x} \in \mathbb{R}^n$は解きたい未知の変数のベクトルである。$\boldsymbol{x}$の各要素$x_i$はこれらの未知の変数の1つである。$\boldsymbol{A}$の各行と$\boldsymbol{b}$の各要素は別の制約をもたらす。我々は[@eq:2.11]を以下のように書き直すことができる。

$$
\boldsymbol{A}_{1,:} \boldsymbol{x} = b_1
$$

$$
\boldsymbol{A}_{2,:} \boldsymbol{x} = b_2
$$

$$
\dots
$$

$$
\boldsymbol{A}_{m,:} \boldsymbol{x} = b_m
$$

また、より明示的にするならば、

$$
\boldsymbol{A}_{1,1} x_1 + \boldsymbol{A}_{1,2} x_2 + \cdots \boldsymbol{A}_{1,n} x_n = b_1
$$

$$
\boldsymbol{A}_{2,1} x_1 + \boldsymbol{A}_{2,2} x_2 + \cdots \boldsymbol{A}_{2,n} x_n = b_2
$$

$$
\dots
$$

$$
\boldsymbol{A}_{m,1} x_1 + \boldsymbol{A}_{m,2} x_2 + \cdots \boldsymbol{A}_{m,n} x_n = b_m
$$

行列対ベクトルの積の表記法はこの形式の式に対するよりコンパクトな表現をもたらす。

## 単位行列と逆行列

線形代数は$\boldsymbol{A}$の多くの値に対して[@eq:2.11]を解析的に解くことができるようになる逆行列と呼ばれる強力なツールをもたらす。
逆行列を説明するためには、まず、単位行列の概念を定義する必要がある。単位行列は、その行列をベクトルにかけたとき、いかなるベクトルも変化させない行列である。我々は$n$次元のベクトルを保持する単位行列を$\boldsymbol{I}_n$と表記する。正式には、$\boldsymbol{I}_n \in \mathbb{R}^{n \times n}$であり、

$$
\forall \boldsymbol{x} \in \boldsymbol{R}^n, \boldsymbol{I}_n \boldsymbol{x} = \boldsymbol{x}
$$

単位行列の構造は単純である。主対角線に沿ったすべての成分は1であり、その他のすべての成分は0である。例として[@fig:2.2]を参照のこと。

![単位行列の例。これは$\boldsymbol{I}_3$](fig/2-2.png){#fig:2.2}

$\boldsymbol{A}$の逆行列は$\boldsymbol{A}^{-1}$と表記し、以下のような行列として定義される。

$$
\boldsymbol{A}^{-1} \boldsymbol{A} = \boldsymbol{I}_n
$$

すると、以下の手順を用いて[@eq:2.11]を解くことができる。

$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}
$$

$$
\boldsymbol{A}^{-1} \boldsymbol{A} \boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}
$$

$$
\boldsymbol{I}_n \boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}
$$

$$
\boldsymbol{x} = \boldsymbol{A}^{-1} \boldsymbol{b}
$$

もちろん、この処理は$\boldsymbol{A}^{-1}$を見つけられるということに依存する。我々は以下の章で$\boldsymbol{A}^{-1}$が存在する条件を検討する。
$\boldsymbol{A}^{-1}$が存在するとき、いくつかの異なるアルゴリズムは閉形式においてこれを見つけることができる。理論上、同じ逆行列は$\boldsymbol{b}$の様々な値に対して何度もその式を解くことに使える。$\boldsymbol{A}^{-1}$は主に理論的なツールとして役に立つが、実際には多くのソフトウェアアプリケーションに対して実践で使わるべきではない。$\boldsymbol{A}^{-1}$はディジタルコンピュータ上では限定的な精度でのみ表現される可能性があるので、$\boldsymbol{b}$の値を使うアルゴリズムは通常、$\boldsymbol{x}$のより正確な推定値を得ることができる。

## 線形従属と線形包

存在する$\boldsymbol{A}^{-1}$に対して、[@eq:2.11]は$\boldsymbol{b}$の値ごとにたった1つの解のみをもたなければならない。式の系がある$\boldsymbol{b}$の値に対する解を持たなかったり無限に多くの解を持ったりする可能性もある。しかし、特定の$\boldsymbol{b}$に対して無限より少なく1つより多い解を取りえない。つまり、$\boldsymbol{x}$と$\boldsymbol{y}$の両方が解でありならば、以下はいかなる実数$\alpha$において解でもある。

$$
\boldsymbol{z} = \alpha \boldsymbol{x} + (1 - \alpha) \boldsymbol{y}
$$

その式がいくつの解を持つかを解析するため、$\boldsymbol{A}$の列を原点（すべて0のベクトルで指定される点）から向かうことができる様々な方向を指定すると考え、そして、$\boldsymbol{b}$に至る道がいくつあるかを決定する。この視点では、$\boldsymbol{x}$の各要素はこれらの方向のそれぞれにどれだけ距離だけ向かうべきであるかを指定する。$i$列目の方向にどれだけの距離を移動するかを指定する$x_i$として、

$$
\boldsymbol{A} \boldsymbol{x} = \sum_i x_i \boldsymbol{A}_{:,i}
$$

一般に、この種の操作は線形結合[linear combination]と呼ばれる。正式には、あるベクトルの集合${\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}}$の線形結合は対応するスカラの係数を各ベクトル$v^{(i)}$にかけ、その結果を足し合わせることで求められる。

$$
\sum_i c_i \boldsymbol{v}^{(i)}
$$

ベクトルの集合の張る空間[span]は元のベクトルの線形結合によって得られるすべての点の集合である。故に、$\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}$が解を持つかどうかを決定することは$\boldsymbol{b}$が$\boldsymbol{A}$の列の張る空間の中にあるかどうかを確かめることに等しい。この特定の張る空間は$\boldsymbol{A}$の列空間とか値域[range]として知られる。
したがって、系$\boldsymbol{A} \boldsymbol{x} = \boldsymbol{b}$が$\boldsymbol{b} \in \mathbb{R}^m$のすべての値に対して解を持つために、我々はその列空間が$\mathbb{R}^m$全体であることを必要とする。$\mathbb{R}^m$におけるいずれかの点がその列空間から外れている場合、その点は解を持たない$\boldsymbol{b}$の潜在的な値である。$\boldsymbol{A}$の列空間が$\mathbb{R}^m$全体であるという必要条件は$\boldsymbol{A}$が少なくとも$m$個の列を持つことを即座に暗示する。このとき、$n \ge m$である。そうでなければ、列空間の次元は$m$より小さくなるだろう。例えば、3x2行列を考えよう。対象の$\boldsymbol{b}$は3次元であるが、$\boldsymbol{x}$は2次元である。なので、最善の状態で$\boldsymbol{x}$の値を修正することは2次元平面を$\mathbb{R}^3$の内に描き出すことを可能にする。この式は$\boldsymbol{b}$がその平面上にある場合に限り解を持つ。
$n \ge m$は解を持つためのすべての点に対するただ一つの必要条件である。これは、列のいくつかが冗長である可能性があるので、十分条件ではない。列の両方が同一である2x2行列を考えよう。これは、複製された列の1つ分のみを含むので、2x1行列と同じ列空間を持つ。言い換えれば、その列空間は依然として単なる線であり、2個の列があったとしても、$\mathbb{R}^2$全体を取り囲むことができない。
正式には、この種の冗長性は線形従属[linear dependence]として知られる。その集合のベクトルが他のベクトルの線形結合でないならば、ベクトルの集合は線形独立[linear independent]である。その集合における他のベクトルの線形結合である集合にベクトルを追加するならば、その新しいベクトルはその集合の張る空間にいかなる点も追加しない。これは行列の列空間が$\mathbb{R}^m$全体を取り囲むことを意味し、その行列は$m$個の線形独立の列の少なくとも1つの集合を含まなければならない。この条件は[@eq:2.11]が$\boldsymbol{b}$のすべての値に対して解を持つための必要十分条件である。その必要条件は集合が、少なくとも$m$個ではなく、厳密に$m$個の線形独立な列を持つことに注意すること。$m$次元のベクトルの集合は$m$個より多い相互に線形独立な列を持ち得ないが、$m$個より多い列を持つ行列はそのような集合を1より多く持ち得る。
行列が逆行列を持つためには、追加で[@eq:2.11]が$\boldsymbol{b}$の各値に対して多くとも1つの解を持つことを保証する必要がある。そうするために、その行列が多くとも$m$個の列を持つことを確かめる必要がある。そうでなければ、各解をパラメータ化する1つより多い方法が存在する。
両方とも、これは行列が正方[square]でなければならないことを意味する。つまり、$m = n$、かつ、すべての列が線形独立である。線形従属な列を持つ正方行列は特異[singular]として知られる。
$\boldsymbol{A}$が正則でない、または、正則だが特異であるならば、その式を解くことは依然として可能であるが、我々は解を求めるために行列の逆を求める手法を用いることが出来ない。
ここまで、我々は左から掛けられるものとして逆行列を述べてきた。右からかけられる逆行列を定義することも可能である。

$$
\boldsymbol{A} \boldsymbol{A}^{-1} = \boldsymbol{I}
$$

正則行列では、左の逆と右の逆は等しい。

## ノルム

時折、我々はベクトルの大きさを測る必要がある。機械学習において、我々は一般にノルムと呼ばれる関数を用いてベクトルの大きさを測る。正式には、$L^p$ノルムは$p \in \mathbb{R}, p \ge 1$に対して以下で求まる。


$$
\|\boldsymbol{x}\|_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}}
$$
{#eq:2.30}

$L^p$ノルムを含むノルムはベクトルを非負の値にマッピングする関数である。直観レベルでは、ベクトル$\boldsymbol{x}$のノルムは原点から点$\boldsymbol{x}$までの距離を測る。より厳密に言えば、ノルムは以下の性質を満たす任意の関数$f$である。

- $f(\boldsymbol{x}) = 0 \Leftarrow \boldsymbol{x} = \boldsymbol{0}$
- $f(\boldsymbol{x} + \boldsymbol{y}) \le f(\boldsymbol{x}) + f(\boldsymbol{y})$（三角不等式）
- $\forall \alpha \in \mathbb{R}, f(\alpha \boldsymbol{x}) = |\alpha| f(\boldsymbol{x})$

$p = 2$を持つ$L^2$ノルムは、単に原点から$\boldsymbol{x}$で示される点までのユークリッド距離である、ユークリッドノルム[Euclidean norm]として知られる。$L^2$ノルムは、下付きの2が省かれて、単に$\|\boldsymbol{x}\|$としてしばしば表記され、機械学習においてかなり頻繁に使われる。$\boldsymbol{x}^\top \boldsymbol{x}$として簡単に計算できる$L^2$ノルムの二乗を使ってベクトルの大きさを測ることも一般的である。
$L^2$ノルムの二乗は$L^2$ノルムそれ自体よりも数学的にも計算的にも更に便利に機能する。例えば、$\boldsymbol{x}$の各要素に関する$L^2$ノルムの二乗のそれぞれの微分は$\boldsymbol{x}$の関連する要素にのみ依存するが、$L^2$ノルムのすべての微分はベクトル全体に依存する。多くの状況で、$L^2$ノルムの二乗は、原点近くで非常にゆっくりと増加するので、望ましくないかもしれない。いくつかの機械学習アプリケーションでは、ピッタリ0である要素と0でない小さい要素との間を区別することが重要である。これらの場合、我々は、数学的な単純さを維持するがすべての位置で同じ割合で成長するような関数に切り替える。すなわち、$L^1$ノルムである。$L^1$ノルムは以下のように単純化され得る。

$$
\|\boldsymbol{x}\|_1 = \sum_i |x_i|
$$

$L^1$ノルムはゼロと非ゼロの違いが非常に重要であるときに機械学習で一般的に使われる。毎回、$\boldsymbol{x}$の要素は$\epsilon$だけ0から離れる。つまり、$L^1$ノルムは$\epsilon$だけ増加する。
我々は時折に非ゼロの要素数を数えることでベクトルの大きさを測る。ある著者はこの関数を「$L^0$ノルム」としているが、これは不正確な用語である。ベクトルにおける非ゼロの成分の個数は、$\alpha$でベクトルをスケーリングしても非ゼロの成分の個数が変化しないので、ノルムではない。$L^1$ノルムは非ゼロの成分の個数の代わりとしてしばしば用いられる。
機械学習で一般的に現れる他のノルムのひとつとして$L^\infty$があり、最大ノルム[max norm]としても知られる。このノルムはベクトルにおいて最大の大きさを持つ要素の絶対値に単純化する。

$$
\|\boldsymbol{x}\|_\infty = \max_i |x_i|
$$

時折、我々は行列の大きさを測ることも望むかもしれない。深層学習の文脈において、これを行う最も一般的な方法は、ベクトルの$L^2$と似ている、otherwise obscure Frobenius normを用いることである。

$$
\|A\|_F = \sqrt{\sum_{i,j} A^2_{i,j}}
$$

2つのベクトルの内積はノルムに関して書き直すことができる。具体的には、

$$
\boldsymbol{x}^\top \boldsymbol{y} = \|\boldsymbol{x}\|_2 \|\boldsymbol{y}\|_2 \cos \theta
$$

ここで、$\theta$は$\boldsymbol{x}$と$\boldsymbol{y}$のなす角である。

## 特別な種類の行列およびベクトル

いくつかの特別な種類の行列やベクトルは特に有用である。
対角行列[diagonal matrix]はほとんどが0で構成され、主対角線に沿ってのみ非ゼロの成分を持つ。正式には、行列$\boldsymbol{D}$は$i \ne j$のすべてに対して$D_{i,j} = 0$である場合に限り対角である。我々は対角行列の例のひとつをすでに見てきている。それは、対角成分がすべて1である単位行列である。我々は、その対角成分がベクトル$\boldsymbol{v}$の成分によって与えられる正方対角行列を示すために$\text{diag}(\boldsymbol{v})$と記述する。対角行列は、対角行列との乗算が効率よく計算できるので、いくらか興味深い。$\text{diag}(\boldsymbol{v}) \boldsymbol{x}$をけいさんするためには、各要素$x_i$を$v_i$でスケールするだけでよい。言い換えれば、$\text{diag}(\boldsymbol{v}) \boldsymbol{x} = \boldsymbol{v} \odot \boldsymbol{x}$である。正方対角行列の逆もまた効率的である。その逆行列は対角成分が非ゼロである場合にのみ存在し、その場合、$\text{diag}(\boldsymbol{v})^{-1} = \text{diag}([1/v_1, \dots, 1/v_n]^\top)$である。多くの場合、我々は任意の行列に関して汎用的な機械学習アルゴリズムを導くかもしれないが、いくつかの行列を対角であると制限することでより安価な（かつ、説明の少ない[less descriptive]）アルゴリズムを得るかもしれない。
すべての対角行列が正方である必要はない。矩形[rectangular]の対角行列を構築することが可能である。正方でない対角行列は逆を持たないが、依然として安価に乗算することができる。正方でない対角行列$\boldsymbol{D}$に対して、積$\boldsymbol{D} \boldsymbol{x}$は$\boldsymbol{x}$の各要素をスケーリングすること、$\boldsymbol{D}$が縦長の場合はその結果にいくつかの0を連結するか、$\boldsymbol{D}$が横長の場合はベクトルの最後の要素のいくつかを破棄するか、のいずれかを行うことを伴うだろう。
対称行列は自身の転置と等しい行列である。

$$
\boldsymbol{A} = \boldsymbol{A}^\top
$$

対称行列[symmetric matrix]は、その成分が引数の順番に依存しない2つの引数の関数によって生成されるとき、しばしば発生する。例えば、$\boldsymbol{A}$が点$i$から点$j$までの距離を求める$\boldsymbol{A}_{i,j}$と持つ距離測定の行列であるならば、距離関数は対称であるので、$\boldsymbol{A}_{i,j} = \boldsymbol{A}_{j,i}$である。
単位ベクトル[unit vector]は単位ノルム[unit norm]を持つベクトルである。

$$
\|\boldsymbol{x}\|_2 = 1
$$

ベクトル$\boldsymbol{x}$とベクトル$\boldsymbol{y}$は、$\boldsymbol{x}^\top \boldsymbol{y} = 0$であるならば、互いに直交[orthogonal]である。両方のベクトルが非ゼロのノルムを持つならば、これは互いに90度の角度にあることを意味する。$\mathbb{R}^n$においｙｒ，多くとも$n$個のベクトルは非ゼロのノルムと互いに直交であるかもしれない。そのベクトルが直交であるだけでなく単位ベクトルも持つならば、我々はそれを正規直交[orthonormal]と呼ぶ。
直交行列[orthogonal matrix]は、その行が互いに正規直交であり、その列が互いに正規直交であるような正方行列である。

$$
\boldsymbol{A}^\top \boldsymbol{A} = \boldsymbol{A} \boldsymbol{A}^\top = \boldsymbol{I}
$$

これは暗に以下であることを示す。

$$
\boldsymbol{A}^{-1} = \boldsymbol{A}^\top
$$

つまり、その逆を計算するのが非常に安価であるので、直交行列は興味深い。直交行列の定義には細心の注意を払ってほしい。直観に反して、これらの行は直交であるだけではなく、まったくもって正規直交である。その行や列が直交であるが正規直交ではない行列に特別な用語は当てられていない。

## 固有値分解 {#sec:2.7}

多くの数学的対象は、それらを構成要素に分けること、または、そららに普遍的な特性を見つけること、によってより良く理解できるのであって、それらを表現しようと選んだ方法によってもたらされるわけではない。
例えば、整数は素因数に分解できる。数$12$を表現する方法は10進数と2進数のどちらで記述するかに依存して変化するだろうが、$12 = 2 \times 2 \times 3$は常に真であるだろう。この表現により、我々は、例えば、$12$は$5$で割れないとか、12の倍数は3で割れるといった、有用な特性を結論付けることができる。
我々が整数の真の性質に関することを素因数に分解することで発見できるのと同じように、我々は、要素の配列としての行列の表現では明らかでない機能特性に関する情報を示す方法で行列を分解することもできる。
最も広く使われる種類の行列分解のひとつは固有値分解[eigendecomposition]と呼ばれる。これは、行列を固有ベクトルと固有値の集合に分解する。
正方行列$\boldsymbol{A}$の固有ベクトル[eigenvector]は、$\boldsymbol{A}$による乗算が$\boldsymbol{v}$のスケーリングのみに置き換えるような非ゼロのベクトル$\boldsymbol{v}$である。

$$
\boldsymbol{A} \boldsymbol{v} = \lambda \boldsymbol{v}
$$

スカラ$\lambda$はこの固有ベクトルに対応する固有値[eigenvalue]として知られる（$\boldsymbol{v}^\top \boldsymbol{A} = \lambda \boldsymbol{v}^\top$のような左固有ベクトルを求めることもできるが、我々は通常、右固有ベクトルについて考える）。
$\boldsymbol{v}$が$\boldsymbol{A}$の固有ベクトルであるならば、$s \in \mathbb{R}, s \ne 0$に対する任意の再スケーリングされたベクトル$s \boldsymbol{v}$である。更に、$s \boldsymbol{v}$は依然として同じ固有値を持つ。この理由のため、我々は通常、単位固有ベクトルのみを探す。
行列$\boldsymbol{A}$が固有値${\lambda_1, \dots, \lambda_n}$に対応する$n$個の線形独立な固有ベクトル${\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}}$を持つとする。我々は列あたり1つの行列ベクトルを持つ$\boldsymbol{V}$を形作るためにすべての固有ベクトルを結合しても良い。つまり、$\boldsymbol{V} = [\boldsymbol{v}^{(1)}, \dots, \boldsymbol{v}^{(n)}]$である。同様に、我々はベクトル$\boldsymbol{\lambda} = [\lambda_1, \dots, \lambda_n]^\top$を形成するために固有値を結合できる。$\boldsymbol{A}$の固有値分解は以下で求められる。

$$
\boldsymbol{A} = \boldsymbol{V} \text{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}
$$

我々は特定の固有値と固有ベクトルを持つ行列を*作る*ことが望ましい方向に空間を引き伸ばすことを可能にすることを見たことがある。それでも、我々はしばしば行列を固有値と固有ベクトルに分解したい。そうすることは、整数を素因数に分解することが整数の振る舞いを理解するのに役立つ可能性があることと同様に、行列のある特性を解析するのに役立つ可能性がある。
すべての行列が固有値と固有ベクトルに分解できるわけではない。ある場合では、分解は存在するが、実数ではなく複素数を伴う。幸いにも、本書では、通常、単純な分解を持つ特定の種類の行列のみを分解する必要がある。具体的には、すべてが実対称行列は実数値の固有ベクトルと固有値のみを用いた式に分解することができる。

$$
\boldsymbol{A} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top
$$

ここで、$\boldsymbol{Q}$は行列$\boldsymbol{A}$の固有値から成る直交行列であり、$\boldsymbol{\Lambda}$は対角行列である。固有値$\Lambda_{i,i}$は、$\boldsymbol{Q}_{:,i}$と表記される$\boldsymbol{Q}$の列$i$における固有ベクトルと関連している。$\boldsymbol{Q}$は直交行列であるので、我々は$\boldsymbol{A}$を方向$\boldsymbol{v}^{(i)}$に$\lambda_i$だけ空間をスケーリングするとみなすことができる。例として[@fig:2.3]を参照のこと。

![固有ベクトルと固有値の影響の例。ここに、固有値$\lambda_1$を持つ$\boldsymbol{v}^{(1)}$と$\lambda_2$を持つ$\boldsymbol{v}^{(2)}$という2つの正規直交な固有ベクトルを持つ行列$\boldsymbol{A}$がある。（左）すべての単位ベクトル$\boldsymbol{u} \in \mathbb{R}^2$の集合を単位円としてプロットした。（右）すべての点$\boldsymbol{A} \boldsymbol{u}$の集合をプロットした。$\boldsymbol{A}$の単位円の歪ませ方を観察することで、方向$\boldsymbol{v}^{(i)}$に$\lambda_i$だけ空間をスケールすることを確認できる。](fig/2-3.png){#fig:2.3}

いかなる実対称行列$\boldsymbol{A}$も固有値分解を持つことが保証されているが、その固有値分解はユニークではないかもしれない。いずれかの2つ以上の固有ベクトルが同じ固有値を共有するならば、その張る空間にある任意の直交ベクトルの集合もその固有値を持つ固有ベクトルであり、代わりに、これらの固有ベクトルを用いて$\boldsymbol{Q}$を同等に選択することができるだろう。慣例により、我々は通常、降順に$\boldsymbol{A}$の成分を並べ替える。この慣例のもとでは、固有値分解は、すべての固有値がユニークである場合に限り、ユニークである。
行列の固有値分解はその行列に関する多くの有用な事実を教えてくれる。その行列は、固有値のいずれかがゼロである場合に限り、特異である。実対称行列の固有値分解は、$\|\boldsymbol{x}\|_2 = 1$に従う$f(\boldsymbol{x}) = \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x}$の形の二次方程式を最適化するのにも使うことができる。$\boldsymbol{x}$が$\boldsymbol{A}$の固有ベクトルに等しいならば、$f$は対応する固有値の値を取る。制約領域内での$f$の最大値は最大の固有値であり、制約領域内でのその最小値は最小の固有値である。
固有値がすべて正である行列は正定値[positive definite]と呼ばれる。固有値がすべて正かゼロである行列は半正定値[positive semidefinite]と呼ばれる。同様に、すべての固有値が負であれば、それは負定値[negative definite]であり、すべての固有値が負かゼロであれば、それは半負定値[negative semidefinite]である。半正定値行列は、$\forall \boldsymbol{x}, \boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} \ge 0$であることが保証されるので、興味深い。正定値行列は加えて$\boldsymbol{x}^\top \boldsymbol{A} \boldsymbol{x} = 0 \Rightarrow \boldsymbol{x} = \boldsymbol{0}$であることが保証される。

## 特異値分解

[@sec:2.7]では、行列を固有ベクトルと固有値に分解する方法を確認した。特異値分解[singular value decomposition; SVD]は行列を特異ベクトルと特異値に分解するもうひとつの方法をもたらす。SVDは固有値分解が明らかにするようないくつかの同種の情報を見つけることができる。しかし、SVDはより一般に適用できる。すべての実行列は特異値分解を持つが、固有値分解でも同様に真であるとは限らない。例えば、行列が正方でないならば、固有値分解は定義されず、代わりに、特異値分解を使わなければならない。
固有値分解が、以下のように$\boldsymbol{A}$を書き直せるような、固有ベクトルの行列$\boldsymbol{V}$と固有値のベクトル$\boldsymbol{\lambda}$を見つけるために行列$\boldsymbol{A}$を解析することを伴うことを思い出してほしい。

$$
\boldsymbol{A} = \boldsymbol{V} \text{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}
$$

特異値分解は、今度は3つの行列の積として$\boldsymbol{A}$を記述しようとすることを除いて、似ている。

$$
\boldsymbol{A} = \boldsymbol{U} \boldsymbol{D} \boldsymbol{V}^\top
$$

$\boldsymbol{A}$が$m \times n$行列であるとする。すると、$\boldsymbol{U}$は$m \times m$行列、$\boldsymbol{D}$は$m \times n$行列、$\boldsymbol{V}$は$n \times n$行列として定義される。
これらの行列のそれぞれは特別な構造を持つように定義される。行列$\boldsymbol{U}$および$\boldsymbol{V}$は両方とも直交行列として定義される。行列$\boldsymbol{D}$は対角行列として定義される。$\boldsymbol{D}$は正方である必要はないことに注意する。
$\boldsymbol{D}$の対角に沿った要素は行列$\boldsymbol{A}$の特異値[singular values]として知られる。$\boldsymbol{U}$の列は左特異ベクトル[left-singular vectors]として知られる。$\boldsymbol{V}$の列は右特異ベクトル[right-singular vectors]として知られる。
我々は$\boldsymbol{A}$の関数の固有値分解に関して$\boldsymbol{A}$の特異値分解を読み替えることができる。$\boldsymbol{A}$の左特異ベクトルは$\boldsymbol{A} \boldsymbol{A}^\top$の固有ベクトルである。$\boldsymbol{A}$の右特異ベクトルは$\boldsymbol{A}^\top \boldsymbol{A}$の固有ベクトルである。$\boldsymbol{A}$の非ゼロの特異値は$\boldsymbol{A}^\top \boldsymbol{A}$の固有値の平方根である。$\boldsymbol{A} \boldsymbol{A}^\top$についても同様に真である。
おそらく、SVDの最も有用な特徴は、次の章で見ていくが、正方でない行列への行列の逆を部分的に一般化するのに使えることである。

## Moore-Penroseの擬似逆行列

逆行列は正方出ない行列に対して定義されない。以下の一次方程式を解くことができるような、行列$\boldsymbol{A}$の左逆行列$\boldsymbol{B}$を作りたいとする。

$$
\boldsymbol{A} \boldsymbol{x} = \boldsymbol{y}
$$

両辺に左から掛けることで、以下を得る。

$$
\boldsymbol{x} = \boldsymbol{B} \boldsymbol{y}
$$

問題の構造に依存して、$\boldsymbol{A}$から$\boldsymbol{B}$へのユニークなマッピングを設計できないかもしれない。
$\boldsymbol{A}$が縦長の場合、この式が解を持たない可能性がある。$\boldsymbol{A}$が横長の場合、取り得る複数の解が存在する。
Moore-Penroseの擬似逆行列はこれらの場合にいくらかの進展をもたらすことを可能とする。$\boldsymbol{A}$の擬似逆行列は以下の
行列として定義される。

$$
\boldsymbol{A}^+ = \lim_{\alpha \searrow 0} (\boldsymbol{A}^\top \boldsymbol{A} + \alpha \boldsymbol{I})^{-1} \boldsymbol{A}^\top
$$

擬似逆行列を計算するための実践的なアルゴリズムはこの定義ではなく以下の式に基づく。

$$
\boldsymbol{A}^+ = \boldsymbol{V} \boldsymbol{D}^+ \boldsymbol{U}^\top
$$

ここで、$\boldsymbol{U}$、$\boldsymbol{D}$、$\boldsymbol{V}$は$\boldsymbol{A}$の特異値分解であり、対角行列$\boldsymbol{D}$の擬似逆行列$\boldsymbol{D}^+$は非ゼロの要素の逆数を取り、その結果の行列の転置を取ることで得られる。
$\boldsymbol{A}$が行より多い列を持つとき、擬似逆行列を用いて一次方程式を解くことは多くの取り得る解のひとつをもたらす。具体的には、すべての取り得る解の中で最小ユークリッドノルム$\|\boldsymbol{x}\|_2$を持つ解$\boldsymbol{x} = \boldsymbol{A}^+ \boldsymbol{y}$をもたらす。
$\boldsymbol{A}$が列より多い行を持つとき、解を持たない可能性がある。この場合、擬似逆行列を用いると、$\boldsymbol{A} \boldsymbol{x}$がユークリッドノルム$\|\boldsymbol{A} \boldsymbol{x} - \boldsymbol{y}\|_2$に関する$\boldsymbol{y}$にできるだけ近いときの$\boldsymbol{x}$をもたらす。

## トレース演算子

トレース演算子は行列のすべての対角成分の総和を求める。

$$
\text{Tr}(\boldsymbol{A}) = \sum_i \boldsymbol{A}_{i,i}
$$

トレース演算子は様々な理由により有用である。総和の表記法に頼らずに記述することが困難ないくつかの操作は行列の積とトレース演算子を用いて記述ことができる。例えば、トレース演算子は行列のFrobeniusノルムを書き表す代替方法をもたらす。

$$
\|A\|_F = \sqrt{\text{Tr}(\boldsymbol{A} \boldsymbol{A}^\top)}
$$

トレース演算子の観点から式を書き表すと、多くの有用な恒等式[identities]を用いて式を操作する機会が開ける。例えば、トレース演算子は転置に対して不変である。

$$
\text{Tr}(\boldsymbol{A}) = \text(\boldsymbol{A}^\top)
$$

対応する行列の形状が結果の積を定義させることを可能にするならば、多くのファクタから構成される正方行列のトレースは最初の位置に最後のファクタを移動しても不変である。

$$
\text{Tr}(\boldsymbol{A} \boldsymbol{B} \boldsymbol{C}) = \text{Tr}(\boldsymbol{C} \boldsymbol{A} \boldsymbol{B}) = \text{Tr}(\boldsymbol{B} \boldsymbol{C} \boldsymbol{A})
$$

または、より一般的に、

$$
\text{Tr} \left( \prod_{i=1}^n F^{(i)} \right) = \text{Tr} \left( F^{(n)} \prod_{i=1}^{n-1} F^{(i)} \right)
$$

この循環置換に対する不変性は結果の積が異なる形状を持っていても成り立つ。例えば、$\boldsymbol{A} \in \mathbb{R}^{m \times n}$と$\boldsymbol{B} \in \mathbb{R}^{n \times m}$に対して、$\boldsymbol{A}\boldsymbol{B} \in \mathbb{R}^{m \times m}$であり$\boldsymbol{B}\boldsymbol{A} \in \mathbb{R}^{n \times n}$であるとしても、いかが成り立つ。

$$
\text{Tr}(\boldsymbol{A}\boldsymbol{B}) = \text{Tr}(\boldsymbol{B}\boldsymbol{A})
$$

もうひとつの覚えておきたい有用な事実として、スカラはそれ自身のトレースであるということである。すなわち、$a = \text{Tr}(a)$である。

## 行列式

$\det(\boldsymbol{A})$と表記される正方行列の行列式は行列を実スカラにマッピングする関数である。行列式はその行列のすべての固有値の積に等しい。行列式の絶対値は行列による乗算が空間をどれだけ拡大または縮小するかの尺度とみなすことができる。行列式が0であれば、そのボリュームのすべてを失わせるように、空間は少なくとも1次元に完全に縮小される。行列式が1であれば、その変換はボリュームを維持する。

## 例：主成分分析

単純な機械学習アルゴリズムのひとつである、主成分分析[principal components analysis; PCA]は基本的な線形代数の知識のみを用いて導出できる。
$\mathbb{R}^n$にある$m$個の点の集合${\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)}}$があるとして、これらの点に非可逆圧縮を適用したい。非可逆圧縮は、メモリをそれほど必要としないがいくらかの精度が失われるかもしれない方法でその点を格納することを意味する。我々はできるだけ失う精度を少なくしたい。
これらの点をエンコードする方法のひとつは、こららの低次元バージョンを表現することである。各点$\boldsymbol{x}^{(i)} \in \mathbb{R}^n$に対して、対応するコードベクトル$\boldsymbol{c}^{(i)} \in \mathbb{R}^l$を求める。$l$が$n$より小さければ、コードの点を格納することは元データを格納するよりも使うメモリが少なくなるだろう。我々は入力に対するコードを生み出すいくつかのエンコード関数$f(\boldsymbol{x}) = \boldsymbol{c}$とそのコードを与えると再構築された入力を生み出すデコード関数$\boldsymbol{x} \approx g(f(\boldsymbol{x}))$を見つけたい。
PCAはデコード関数の選択として定義される。具体的には、デコーダを非常に単純にするために、我々はコードを$\mathbb{R}^n$にマッピングし直そうと行列の乗算を用いることにする。$g(\boldsymbol{c}) = \boldsymbol{D} \boldsymbol{c}$とする。ここで、$\boldsymbol{D} \in \mathbb{R}^{n \times l}$はデコードを定義する行列である。
このデコーダに対する最適なコードを計算することは難しい問題となり得るかもしれない。エンコードの問題を簡単に保つため、PCAは$\boldsymbol{D}$の列が互いに直交することを強いる。（$l = n$でない限り、$\boldsymbol{D}$は厳密に言えば「直交行列」ではないことに注意する。）
これまでに述べたような問題は、すべての点に対して比例して$c_i$を減らす場合、$\boldsymbol{D}_{:,i}$のスケールを増やすことができるので、多くの解を取り得る。その問題に単独の解を与えるため、$\boldsymbol{D}$のすべての列を単位ノルムを持つように制限する。
この基本アイデアを我々が実装できるアルゴリズムへ変えるため、我々がすべき最初のことは入力の点$\boldsymbol{x}$ごとに最適なコード点$\boldsymbol{c}^*$を生成する方法を理解することである。これを行う方法のひとつは入力の点$\boldsymbol{x}$とその再構築されたもの$g(\boldsymbol{c}^*)$の距離を最小化することである。我々はこの距離をノルムを用いて測ることができる。主成分アルゴリズムにおいて、我々は$L^2$ノルムを用いる。

$$
\boldsymbol{c}^* \argmin_{\boldsymbol{c}} \|\boldsymbol{x} - g(\boldsymbol{c})\|_2
$$

我々は、その両方が同じ$\boldsymbol{c}$の値に最小化されうので、$L^2$ノルムそれ自体を用いる代わりに$L^2$ノルムの二乗に切り替えることができる。両方は、$L^2$ノルムが非負であり、二乗の演算が非負の変数に対して単調増加するので、同じ$\boldsymbol{c}$の値によって最小化される。

$$
\boldsymbol{c}^* \argmin_{\boldsymbol{c}} \|\boldsymbol{x} - g(\boldsymbol{c})\|^2_2
$$

最小化する関数は以下のように単純化する。

$$
(\boldsymbol{x} - g(\boldsymbol{c}))^\top (\boldsymbol{x} - g(\boldsymbol{c}))
$$

（[@eq:2.30]の$L^2$ノルムの定義より）

$$
= \boldsymbol{x}^\top \boldsymbol{x} - \boldsymbol{x}^\top g(\boldsymbol{c}) - g(\boldsymbol{c})^\top \boldsymbol{x} + g(\boldsymbol{c})^\top g(\boldsymbol{c})
$$

（分配法則により、$g(\boldsymbol{c})^\top \boldsymbol{x}$がそれ自身の転置と等しいので）

$$
= \boldsymbol{x}^\top \boldsymbol{x} - 2\boldsymbol{x}^\top g(\boldsymbol{c}) + g(\boldsymbol{c})^\top g(\boldsymbol{c})
$$

我々は、$\boldsymbol{c}$に依存しない第一の項を省略するように、再び最小化される関数を変化させることができる。

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top g(\boldsymbol{c}) + g(\boldsymbol{c})^\top g(\boldsymbol{c})
$$

さらに歩みを進めるため、$g(\boldsymbol{c})$の定義で置き換えなければならない。

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{D}^\top  \boldsymbol{D} \boldsymbol{c}
$$

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{I}_l \boldsymbol{c}
$$

（$\boldsymbol{D}$の直交性と単位ノルム制約により）

$$
\boldsymbol{c}^* = \argmin_{\boldsymbol{c}} -2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{c}
$$

我々はこの最適化問題をベクトル解析[vector calculus]を用いて解くことができる（どうやってこれを行うかを知らないならば[@sec:4.3]を参照）。

$$
\nabla_{\boldsymbol{c}} (-2\boldsymbol{x}^\top \boldsymbol{D} \boldsymbol{c} + \boldsymbol{c}^\top \boldsymbol{c}) = \boldsymbol{0}
$$

$$
-2\boldsymbol{D}^\top \boldsymbol{x} + 2\boldsymbol{c} = \boldsymbol{0}
$$

$$
\boldsymbol{c} = \boldsymbol{D}^\top \boldsymbol{x}
$$

これはアルゴリズムを効率的にする。つまり、行列対ベクトルの操作だけを用いて$\boldsymbol{x}$を最適にエンコードすることができる。ベクトルをエンコードするため、我々はエンコーダ関数を適用する。

$$
f(\boldsymbol{x}) = \boldsymbol{D}^\top \boldsymbol{x}
$$

さらなる行列の乗算を用いて、PCA reconstructionの演算を定義することもできる。

$$
r(\boldsymbol{x}) = g(f(\boldsymbol{x})) = \boldsymbol{D} \boldsymbol{D}^\top \boldsymbol{x}
$$
{#eq:2.67}

次に、エンコード行列$\boldsymbol{D}$を決める必要がある。そうするために、我々は入力と再構築との間の$L^2$距離を最小化するというアイデアに立ち戻る。我々はすべての点をデコードするのに同一の行列$\boldsymbol{D}$を用いるつもりなので、点を分けて考えなくてよくなる。代わりに、すべての次元とすべての点上で計算される誤差の行列のFrobeniusノルムを最小化しなければならない。

$$
\boldsymbol{D}^* = \argmin_{\boldsymbol{D}} \sqrt{\sum_{i,j} \left( x_j^{(i)} - r(\boldsymbol{x}^{(i)})_j \right)^2} \text{ subject to } \boldsymbol{D}^\top \boldsymbol{D} = \boldsymbol{I}_l
$$
{#eq:2.68}

$\boldsymbol{D}^*$を求めるアルゴリズムを導くため、我々は$l = 1$の場合を考えることから始める。この場合、$\boldsymbol{D}$はただの単一のベクトル$\boldsymbol{d}$である。[@eq:2.67]を[@eq:2.68]に置き換え、$\boldsymbol{D}$を$\boldsymbol{d}$に単純化すると、この問題は以下のように整理される。

$$
\boldsymbol{d}^* = \argmin_{\boldsymbol{d}} \sum_i \| \boldsymbol{x}^{(i)} - \boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{x}^{(i)} \|_2^2 \text{ subject to } \|\boldsymbol{d}\|_2 = 1
$$

上記の式は置換を行う最も直接的な方法であるが、式を記述するための最も様式的に満足のいく方法ではない。これはスカラの値$\boldsymbol{d}^\top \boldsymbol{x}^{(i)}$がベクトル$\boldsymbol{d}$の右側にある。スカラの係数は慣例的には操作するベクトルの左側に記述される。故に、このような式は通常、以下のように書く。

$$
\boldsymbol{d}^* = \argmin_{\boldsymbol{d}} \sum_i \| \boldsymbol{x}^{(i)} - \boldsymbol{d}^\top \boldsymbol{x}^{(i)} \boldsymbol{d} \|_2^2 \text{ subject to } \|\boldsymbol{d}\|_2 = 1
$$

または、スカラがそれ自体の転置であることを活用して、以下のようになる。

$$
\boldsymbol{d}^* = \argmin_{\boldsymbol{d}} \sum_i \| \boldsymbol{x}^{(i)} - \boldsymbol{x}^{(i)\top} \boldsymbol{d} \boldsymbol{d} \|_2^2 \text{ subject to } \|\boldsymbol{d}\|_2 = 1
$$

読者はこのようなcosmeticな並び替えに慣れることを目標にすべきである。

この時点で、別個のexampleのベクトルの合計としてではなく、examplesの単一の計画行列[design matrix]に関する問題に書き換えるのに役立ち得る。これはよりコンパクトな表記法を用いることを可能にする。$\boldsymbol{X} \in \mathbb{R}^{m \times n}$を、$\boldsymbol{X}_{i,:} = \boldsymbol{x}^{(i)\top}$のような、点を説明するすべてのベクトルを積み上げることで定義される行列とする。すると、我々は以下のようにこの問題を書き直すことができる。

$$
\boldsymbol{d}^* = \argmin_{\boldsymbol{d}} \| \boldsymbol{X} - \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top \|_F^2 \text{ subject to } \boldsymbol{d}^\top \boldsymbol{d} = 1
$$

とりあえず制約を無視すると、Frobeniusノルムの部分を以下のように単純化できる。

$$
\argmin_{\boldsymbol{d}} \| \boldsymbol{X} - \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top \|_F^2
$$

$$
= \argmin_{\boldsymbol{d}} \text{Tr} \left( (\boldsymbol{X} - \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top)^\top (\boldsymbol{X} - \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) \right)
$$

（[@eq:2.49]により）

$$
= \argmin_{\boldsymbol{d}} \text{Tr} \left( \boldsymbol{X}^\top \boldsymbol{X} - \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top - \boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X} + \boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top \right)
$$

$$
= \argmin_{\boldsymbol{d}} \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X}) - \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) - \text{Tr}(\boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X}) + \text{Tr}(\boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top)
$$

$$
= \argmin_{\boldsymbol{d}} - \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) - \text{Tr}(\boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X}) + \text{Tr}(\boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top)
$$

（$\boldsymbol{d}$を伴わない項は$\argmin$に影響を及ぼさないので）

$$
= \argmin_{\boldsymbol{d}} - 2\text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) + \text{Tr}(\boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top)
$$

（[@eq:2.52]から、トレース内の行列の順番を回転させることができるので）

$$
= \argmin_{\boldsymbol{d}} - 2\text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) + \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{d} \boldsymbol{d}^\top)
$$

（もう一度同じ特性を使う）

この時点で、制約を再導入する。

$$
\argmin_{\boldsymbol{d}} - 2\text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) + \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top \boldsymbol{d} \boldsymbol{d}^\top) \text{ subject to } \boldsymbol{d}^\top \boldsymbol{d} = 1
$$

$$
= \argmin_{\boldsymbol{d}} - 2\text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) + \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) \text{ subject to } \boldsymbol{d}^\top \boldsymbol{d} = 1
$$

（制約により）

$$
= \argmin_{\boldsymbol{d}} -\text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) \text{ subject to } \boldsymbol{d}^\top \boldsymbol{d} = 1
$$

$$
= \argmax_{\boldsymbol{d}} \text{Tr}(\boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d} \boldsymbol{d}^\top) \text{ subject to } \boldsymbol{d}^\top \boldsymbol{d} = 1
$$

$$
= \argmax_{\boldsymbol{d}} \text{Tr}(\boldsymbol{d}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{d}) \text{ subject to } \boldsymbol{d}^\top \boldsymbol{d} = 1
$$

この最適化問題は固有値分解を用いて解かれるかもしれない。具体的には、最適な$\boldsymbol{d}$は最大の固有値に対応する$\boldsymbol{X}^\top \boldsymbol{X}$固有ベクトルによって与えられる。
この導出は$l = 1$の場合に特有であり、第一主成分のみを復元する。より一般的に言えば、主成分の基底を復元したいとき、行列$\boldsymbol{D}$は最大の固有値に対応する$l$個の固有ベクトルによって与えられる。これは帰納法による証明[proof by induction]を用いて示されるかもしれない。我々は演習としてこの証明を書いてみることをオススメする。
線形代数は深層学習を理解するのに必要な基礎数学の分野のひとつである。もうひとつの重要となる、機械学習においていたるところにに存在する数学の分野は確率論であり、次章で示される。

