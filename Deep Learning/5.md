# 機械学習の基礎

深層学習は機械学習の特定の種類である。深層学習を良く理解するためには、機械学習の基本原則をしっかりと理解しなければならない。この章は以降の本書全体を通して適用される最重要の一般的な原則についての簡潔な授業を提供する。新参の読者や視野を広くしたい方は、[@Murphy2012]や[@Bishop2006]のような、基礎部分をより包括的にカバーする機械学習のテキストを検討することをオススメする。あなたがすでに機械学習の基礎に親しんでいるならば、[@sec:5.11]まで飛ばしてもらって構わない。この節は深層学習アルゴリズムの発展に強く影響を与えてきた伝統的な機械学習技術に対するいくつかの視点をカバーする。

学習アルゴリズムとは何かを定義するところから始めて、線形回帰アルゴリズムを使って一例を示そう。そこで次に、訓練データを一致させる課題が新しいデータに一般化するパターンを見つける課題とどれだけ異なるかを記述することにする。ほとんどの機械学習アルゴリズムはハイパーパラメータ[hyperparameters]と呼ばれる設定を持つ。これは、学習アルゴリズム自体の外で決定されなければならない。つまり、我々は追加データを用いてこれらを設定する方法を議論する。機械学習は実質的に、複雑な関数を統計的に推定するためにコンピュータを使用することを重んじ、これらの関数の周りの信頼区間を証明することを軽んじる、応用的な統計学の一形式である。従って、我々は、頻度論的推定量[frequentist estimators]とベイズ推定[Bayesian inference]という、2つの統計学への中心的アプローチを示す。ほとんどの機械学習アルゴリズムは、教師あり学習と教師なし学習のカテゴリに分けられる。そこで、我々はこれらのカテゴリを説明し、各カテゴリに由来するいくつかの単純な学習アルゴリズムの例を与える。ほとんどの深層学習アルゴリズムは確率的勾配降下法とよばれる最適化アルゴリズムに基づいている。我々は、機械学習アルゴリズムを作るために、最適化アルゴリズム、コスト関数、モデル、データセットといった、様々なアルゴリズムの構成要素を組み合わせる方法を述べる。最後に、[@sec:5.11]では、一般化するために伝統的な機械学習の能力を制限していた要因のいくつかを述べる。これらの課題は、これらの障害を克服する深層学習アルゴリズムの開発を動機付けてきた。

## 学習アルゴリズム

機械学習アルゴリズムとはデータから学習することができるアルゴリズムである。しかし、学習とは何を意味するのだろうか？ @Mitchell1997 は、「ある種別のタスク$T$およびパフォーマンス尺度$P$に関する経験$E$によって、$T$におけるタスクでの、$P$によって測定されるパフォーマンスが改善する場合、コンピュータプログラムは$E$から学習すると言う。」という、簡潔な定義を与えている。経験$E$、タスク$T$、パフォーマンス尺度$P$は多種多様なものが想像できるが、我々は本書においてこれらのエンティティのそれぞれに使われるかもしれないものを形式的に定義しようとしたりはしない。代わりに、次節にて、直観的な説明と機械学習アルゴリズムを構築するのに使える様々な種類のタスク、パフォーマンス尺度、経験の例を提供する。

### タスク$T$

機械学習は人間が記述および設計した固定のプログラムで解くには難しすぎるタスクに対処することが可能となる。科学的および哲学的視点から見ると、その理解を深めることは知性の基となる原則の理解を深めることを必然的に伴うので、機械学習は興味深い。
この「タスク」という語の比較的に形式的な定義において、学習の工程それ自体はタスクではない。学習はタスクを処理するための能力を獲得する方法である。例えば、ロボットを歩けるようにしたい場合、歩くことはタスクである。我々は、歩き方を学習するようにロボットをプログラミングしたり、手動で歩き方を指定するプログラムを直接書いてみようとしたりすることができる。
機械学習のタスクは通常、機械学習システムがどのようなexampleを処理すべきかに関して記述される。exampleは機械学習システムに処理させたい物体または事象から定量的に計測された特徴[features]の集合である。我々は一般にexampleを各成分$x_i$が別の特徴であるようなベクトル$\boldsymbol{x} \in \mathbb{R}^n$と表現する。例えば、画像の特徴は通常、その画像の中のピクセルの値である。
多くの種類のタスクは機械学習によって解くことができる。最も一般的な機械学習タスクのいくつかには以下が含まれる。

- 分類[classification]：この種のタスクでは、コンピュータプログラムはある入力が$k$個のカテゴリのいずれに属すかを特定することを求められる。このタスクを解くために、学習アルゴリズムは通常、関数$f : \mathbb{R}^n \rightarrow {1, \dots, k}$を生み出すことを求められる。$y = f(\boldsymbol{x})$であるとき、モデルはベクトル$\boldsymbol{x}$で記述される入力を数値コード$y$で識別されるカテゴリに割り当てる。他にも、例えば、$f$が種別の確率分布を出力するような、様々な分類法が存在する。分類タスクの一例として、物体認識がある。これは、（通常ではピクセルの明るさの値の集合として記述される）画像を入力とし、画像中の物体を識別する数値コードを出力とする。例えば、Willow GarageのPR2というロボットは異なる種類の飲み物を認識し、命令によって人々に届けることができるウェイターととして振る舞うことができる[@Goodfellow2010]。近年の物体認識は深層学習によるものが最も洗練されている[@Krizhevsky2012; @Ioffe2015]。物体認識はコンピュータが顔を認識することを可能にする技術[@Taigman2014]と基本は同じである。これは、写真アルバムの中の人たちを自動的にタグ付けしたり、コンピュータがユーザとより自然に対話したりするために使うことができる。
- 欠けている入力を伴う分類[classification with missing inputs]：コンピュータ・プログラムがその入力ベクトルにすべての測定値を常に与えることを保証しない場合、分類はより挑戦的になる。分類タスクを解くために、学習アルゴリズムはベクトルの入力からカテゴリの出力へマッピングする単一の関数を定義しなければならない。入力のいくつかが欠けているとき、学習アルゴリズムは、単一の分類関数を与えるのではなく、関数の集合を学習しなければならない。各関数はその欠けている入力の異なる部分集合を持つ$\boldsymbol{x}$を分類することに対応する。この種の状況は医療診断において、多くの種の医療検査は高価であり侵襲的[invasive]であるので、頻繁に発生する。そのような関数の大きな集合を効率的に定義する方法のひとつはすべての関連する変数上の確率分布を学習し、欠けている変数を周辺化すること[marginalizing out]で分類タスクを解くことである。$n$個の入力変数では、欠けている入力の取り得る集合ごとに必要となる$2^n$個の異なる分類関数すべてを得られるが、コンピュータプログラムは同時確率分布を記述する単一の関数のみを学習する必要がある。この方法でそのようなタスクに適用される深層確率モデルの例は[@Goodfellow2013b]を参照のこと。この節で述べられるその他のタスクの多くは欠けている入力とともに機能するように一般化することもできる。つまり、欠けている入力を伴う分類は機械学習ができることの単なる一例である。
- 認識[regression]：この種類のタスクでは、コンピュータプログラムはある入力が与えられた場合に数値を予測することを求められる。このタスクを解くために、学習アルゴリズムは関数$f : \mathbb{R}^n \rightarrow \mathbb{R}$を出力することを求められる。この種類のタスクは、出力の形式が異なることを除いて、分類と似ている。認識タスクの例は、被保険者になされるであろう保険金支払額の期待値[expected claim amount]の予測（保険料[insurance premiums]を設定するために使われる）、または、有価証券[securities]の将来的な価格の予測である。
- 文字起こし[transcription]：この種類のタスクでは、機械学習システムはある種のデータの比較的に非構造的な表現を調べて、離散的な文章形式に情報を書き起こすことを求められる。例えば、光学的な文字認識では、コンピュータプログラムは文章の画像を含む写真を見せられ、文字列の形式（例えば、ASCIIかUnicode形式）でこの文章を返すことを求められる。Googleストリートビューはこの方法で郵便番号を処理するために深層学習を用いている[@Goodfellow2014d]。もうひとつの例は音声認識[speech recognition]である。そこでは、コンピュータプログラムは音声波形を与えられ、録音の中で話された言葉を記述する文字列か単語IDを吐き出す。深層学習は、Microsoft、IBM、Googleを含む主要な企業で用いられる近代的な音声認識システムの極めて重要な要素である[@Hinton2012b]。
- 機械翻訳[machine translation]：機械翻訳タスクでは、入力はある言語における記号の列から予め構成され、コンピュータプログラムはこれを別の言語における記号の列に変換しなければならない。これは一般的に、英語からフランス語への翻訳のように、自然言語へ応用される。深層学習は最近になってこの種のタスクに重要な影響をもたらし始めている[@Sutskever2014; @Bahdanau2015]。
- 構造化された出力[structured output]：構造化された出力のタスクは出力が異なる要素間の重要な関係を持つベクトル（または、複数の値から成る他のデータ構造）である任意のタスクである。これは広範囲のカテゴリであり、他の多くのタスクと同様に、上記で述べられる書き起こしや翻訳のタスクを包含する[subsume]。一例として、構文解析[parsing]がある。これは、動詞、名詞、副詞などとして木の節をタグ付けすることでその文法構造を記述する木に自然言語の文をマッピングすることである。構文解析タスクに適用される深層学習の例は[@Collobert2011]を参照のこと。もうひとつの例は画像のピクセル単位の領域分割である。ここでは、コンピュータプログラムは画像中のすべてのピクセルに特定のカテゴリを割り当てる。例えば、深層学習は空撮写真で道路の位置に注釈を付けるのに使うことができる[@Mnih2010]。出力形式はこれらの注釈スタイルのタスクと同じくらいに入力の構造を鏡写しにする必要はない。例えば、画像の表題付け[image captioning]では、コンピュータプログラムは画像を調べ、画像を説明する自然言語の文を出力する[@Kiros2014a; @Kiros2014b; @Mao2015; @Vinyals2015b; @Donahue2014; @Karpathy2015; @Fang2015; @Xu2015]。これらのタスクは、プログラムがすべてで強固な相互関連を持ついくつかの値を出力しなければならないので、構造化出力タスク[structured output tasks]と呼ばれる。例えば、image captioningプログラムで生成される単語は正しい文を形作らなければならない。
- 異常検知[anomaly detection]：この種のタスクでは、コンピュータプログラムは事象または物体の集合を選別し、普通ではない[unusual]または非定型[atypical]であるとしてこれらの一部をフラグを立てる。異常検知タスクの例はクレジットカード詐欺[fraud]の検知である。買い物の習慣をモデル化することで、クレジットカード会社はカードの不正使用を検知できる。泥棒があなたのクレジットカードやクレジットカード情報を盗む場合、泥棒の買い物はしばしばあなた自身とは異なる購買タイプ上の確率分布を生じさせることがあるだろう。クレジットカード会社は、そのカードが特徴的ではない買い物に使われたらすぐに取引を一時停止することで詐欺を防ぐことができる。異常検知の手法の調査については[@Chandola2009]を参照のこと。
- 合成と標本化[synthesis and sampling]：この種のタスクでは、機械学習アルゴリズムは訓練データにあるものと似ている新しいexamplesを生成することを求められる。機械学習を介する合成および標本化は、大量のコンテンツを手作業で作るのが高価だったり、退屈だったり、時間がかかりすぎたりするであろうときのメディアアプリケーションに対して役立つ可能性がある。例えば、ビデオゲームは大きな物体や地形に対するテクスチャを、アーティストに各ピクセルを手作業でラベル付けしてもらうのではなく、自動的に生成できる[@Luo2013]。いくつかの場合、我々は標本化や合成のプロシージャが入力を与えられたときに特定の種類の出力を生成してほしいと考える。例えば、音声生成タスクでは、文字の形の文[written sentence]を与え、プログラムにその文の声のバージョン[spoken version]を含む音声波形を吐くよう求める。これは構造化出力タスクの一種であるが、各入力に対するたった1つの正解の出力が存在しないような定量化が追加されている。そして、我々は、出力がより自然で現実味を帯びているように見せるために、出力において大量のバラツキを明示的に望む。
- 欠落値の補完[imputation of missing values]：この種のタスクでは、機械学習アルゴリズムは新しいexample$\boldsymbol{x} \in \mathbb{R}^n$を与えられるが、$\boldsymbol{x}$のいくつかの成分$x_i$が欠落している。アルゴリズムは欠けている成分の値の推定をもたらす必要がある。
- ノイズ除去[denoising]：この種のタスクでは、機械学習アルゴリズムは不明の破損プロセスによってキレイなexample$\boldsymbol{x} \in \mathbb{R}^n$から破損したexample$\boldsymbol{x}^\tilde \in \mathbb{R}^n$を入力として与えられる。学習者はキレイなexample$\boldsymbol{x}$をその破損したバージョン$\boldsymbol{x}^\tilde$から予測する、または、より一般的には条件付き確率分布$p(\boldsymbol{x} | \boldsymbol{x}^\tilde)$を予測する必要がある。
- 密度推定[density estimation]または確率質量関数の推定[probability mass function estimation]：密度推定問題では、機械学習アルゴリズムは関数$p_{model} : \mathbb{R}^n \rightarrow \mathbb{R}$を学習することを求められる。ここで、$p_{model}(\boldsymbol{x})$はexamplesが描かれる空間上の確率密度関数（$\boldsymbol{\mathbf{x}}$が連続的である場合）または確率質量関数（$\boldsymbol{\mathbf{x}}$が離散的である場合）として解釈できる。そのようなタスクをうまくこなすために（パフォーマンス尺度$P$を述べるときにそれが意味するものを厳密に示そうと思う）、アルゴリズムはそこに示されるデータの構造を学習する必要がある。examplesが密集している所や発生する可能性が低い所を知っていなければならない。前述されたほとんどのタスクは学習アルゴリズムが少なくとも確率分布の構造を暗黙のうちにキャプチャする必要がある。密度推定はその分布を明示的にキャプチャすることを可能にする。原則として、我々は同様に他のタスクを解くためにその分布上の計算を行うことができる。例えば、確率分布$p(\boldsymbol{x})$を得るために確率推定を行ったとすれば、欠落値補完タスクを解くためにその分布を用いることができる。値$x_i$が欠けていて、$\mathbb{x}_{-i}$で表記されるその他すべての値が与えられている場合、それ上の分布が$p(x_i | \boldsymbol{x}_{-i})$によって与えられることを知っている。実践では、多くの場合で$p(\boldsymbol{x})$で必要な処理が計算上扱いづらいので、密度推定はこれらすべての関連するタスクを解くことを常に可能としない。

もちろん、多くのその他のタスクやタスクの種類が取り得る。我々がここに並べたタスクの種類は、タスクの厳密な分類体系を定義するためではなく、機械学習ができることの例を提供するためのみを意図している。

### パフォーマンス尺度$P$

機械学習アルゴリズムの能力を評価するため、我々はそのパフォーマンスの定量的な尺度を設計しなければならない。通常、このパフォーマンス尺度$P$はシステムによってもたらされるタスク$T$に特有なものである。
分類、欠けている入力を伴う分類、文字起こしのようなタスクに対しては、しばしばモデルの正確さ[accuracy]を計測する。正確さはモデルが正しい出力を生み出しているexamplesの単なる割合である。モデルが正しくない出力を生み出しているexamplesの割合である誤り率[error rate]を計測することで同等の情報も得られる。我々はしばしば誤り率を0-1 lossの期待値として参照する。特定のexampleでの0-1 lossは正しく分類されれば$0$となり、そうでなければ$1$となる。密度推定のようなタスクでは、正確さ、誤り率、その他いずれの種類の0-1 lossを測定することも意味をなさない。代わりに、exampleごとに連続値のスコアをモデルに与える別のパフォーマンスの測定基準を用いなければならない。最も一般的なアプローチは、モデルがいくつかの
examplesに割り当てる平均多数確率を報告することである。
通常、それが現実世界に展開させたときにどれだけうまく機能するだろうかを決定するので、我々は機械学習アルゴリズムがそれまでに見たことのないデータをどれだけうまく処理するかについて興味がある。故に、我々は機械学習システムを訓練するために使われるデータとは別のテスト用データセット[test set of data]を用いてこれらのパフォーマンス尺度を評価する。
パフォーマンス尺度の選択は率直で客観的であるように見えるかもしれないが、システムの望ましい振る舞いにうまく対応するパフォーマンス尺度を選ぶことはしばしば難しい。
いくつかの場合、これは計測すべきものを決めることが難しいためである。例えば、文字起こしタスクを処理するとき、文字列全体を書き起こすという点でシステムの正確さを計測すべきだろうか、または、いくつかの文字列の要素が正解しているときに部分点を与えるようなよりきめ細かいパフォーマンス尺度を用いるべきだろうか？認識タスクを処理するとき、頻繁に中くらいのミスをする場合、または、まれに非常に大きなミスをする場合であれば、システムをより減点すべきだろうか？これらの種類の設計上の選択はアプリケーションに依存する。
その他の場合、我々は理想的には計測したい量を知っているが、それを測定することは実用的ではない。例えば、これは密度推定の文脈において頻繁に発生する。最適な確率モデルの多くは暗黙的にのみ確率分布を表現する。多くのそのようなモデルでは空間の特定の点に割り当てられる実際の確率の値を計算することは解決困難である。これらの場合、設計目的に依然として対応する代替基準を設計するか、望ましい基準に対する良好な近似を設計するか、をしなければならない。

### 経験$E$

機械学習アルゴリズムは、学習プロセス中に持ち得る経験の種類によって、教師なし[unsupervised]と教師あり[supervised]に大別することができる。
本書における学習アルゴリズムのほとんどはデータセット[dataset]全体を経験することができると理解できる。データセットは、[@sec:5.1.1]で定義されるように、多数のexamplesの集合である。時折、我々はexamplesをデータポイント[data points]と呼ぶ。
統計学者や機械学習の研究者によって研究される最古のデータセットひとつはIris datasetである[@Fisher1936]。これは150本のアヤメ属の花の様々な部分の計測値の集合である。それぞれの個々の花は1つのexampleに対応する。各exampleにある特徴は花の各部分の計測値である。つまり、萼片[sepal]の長さ、萼片の幅、花弁[petal]の長さ、花弁の幅である。そのデータセットはそれぞれの花がどの種に属しているかも記録している。そのデータセットには3つの異なる種が示されている。
教師なし学習アルゴリズム[unsupervised learning algorithms]は多数の特徴を含むデータセットを経験し、このデータセットの構造の有用な特性を学習する。深層学習の文脈において、我々は通常、密度推定にあるように明示的に、または、合成やノイズ除去のようなタスクのように暗黙的に、データセットを生成する確率分布全体を学習したい。いくつかのその他の教師なし学習アルゴリズムは、似たexamplesのクラスタにデータセットを分けることから成るクラスタリングのような、他の役割をこなす。
教師あり学習アルゴリズム[supervised learning algorithms]は特徴を含むデータセットを経験するが、各exampleはラベル[label]または対象[target]に関連付けられてもいる。例えば、Iris datasetはそれぞれのアヤメの花の種類で注釈が付けられている。教師あり学習アルゴリズムはIris datasetを学んだり、これらの計測値に基づいてアヤメの花を3つの異なる種類に分類することを学習したりすることができる。
大雑把に言えば、教師なし学習は無作為なベクトル$\boldsymbol{\mathbf{x}}$のいくつかのexamplesを観察し、確率分布$p(\boldsymbol{\mathbf{x}})$、または、その分布のいくつかの興味深い特性を暗黙的または明示的に学習しようと試みることを伴う。一方で、教師あり学習は無作為なベクトル$\boldsymbol{\mathbf{x}}$や関連する値またはベクトル$\boldsymbol{\mathbf{y}}$のいくつかのexamplesを観察し、$\boldsymbol{\mathbf{x}}$から$\boldsymbol{\mathbf{y}}$を、通常では$p(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$を推定することによって、予測することを学習することを伴う。教師あり学習という用語は機械学習システムにすべきことを示す講師や教師によってもたらされる対象$\boldsymbol{\mathbf{y}}$の視点に由来する。教師なし学習では、講師や教師がおらず、アルゴリズムはこの導きなしにデータを理解するために学習しなければならない。
教師なし学習と教師あり学習は公式に定義された用語ではない。これらの線引きはしばしば曖昧である。多くの機械学習技術は両方のタスクを処理するために使うことができる。例えば、確率の連鎖律は、ベクトル$\boldsymbol{\mathbf{x}} \in \mathbb{R}^n$に対して、同時分布が以下のように分解できることを示している。

$$
p(\boldsymbol{\mathbf{x}}) = \prod_{i = 1}^n p(\mathbf{x}_i | \mathbf{x}_1, \dots, \mathbf{x}_{i-1})
$$

この分解は、モデル化$p(\boldsymbol{\mathbf{x}})$の表面上は教師なしである問題を$n$個の教師あり学習の問題に分けることで解くことができることを意味する。あるいは、学習$p(y | \boldsymbol{\mathbf{x}})$の教師あり学習の問題を同時分布$p(\boldsymbol{\mathbf{x}}, y)$を学習するための伝統的な教師なし学習技術を用いて以下を推論することで解くことができる。

$$
p(y | \boldsymbol{\mathbf{x}}) = \frac{p(\boldsymbol{\mathbf{x}}, y)}{\sum_{y'} p(\boldsymbol{\mathbf{x}}, y')}
$$

教師なし学習と教師あり学習は完全に形式的で明確な概念ではないにも関わらず、これらは機械学習アルゴリズムで行うことのいくつかをおおまかに分類するのに役立っている。慣例として、人々は認識、分類、構造化出力の問題を教師あり学習と呼ぶ。他のタスクの裏付けとしての密度推定は通常、教師なし学習とみなされる。
他の種類の学習パラダイムが取り得る。例えば、半教師あり学習では、いくつかのexamplesはsupervision targetを含むが、それ以外は含まない。multi-instance学習では、examplesの集合全体はある種別のexampleを含むかどうかでラベル付けされるが、集合の個々のメンバーはラベル付けされない。深層モデルを伴うmulti-instance学習の最近の例は[@Kotzias2015]を参照のこと。
いくつかの機械学習アルゴリズムは単に固定のデータセットを経験するだけではない。例えば、強化学習[reinforcement learning]アルゴリズムは環境と相互作用する。つまり、学習アルゴリズムとその経験の間にフィードバックループが存在する。そのようなアルゴリズムは本書の範疇を超える。強化学習に関する情報は@Sutton1998か@Bertsekas1996を、強化学習への深層学習のアプローチは@Mnih2013を参照ください。
ほとんどの機械学習アルゴリズムは単純にデータセットを経験する。データセットはいろんな方法で記述できる。すべての場合で、データセットはexamplesの集合である。これは、特徴の集合でもある。
データセットを記述する一般的な方法のひとつは計画行列[design matrix]である。計画行列は各行に異なるexampleを含む行列である。その行列の各列は異なる特徴に対応する。例えば、Iris datasetはexampleごとに4つの特徴を持つ150個のexamplesから成る。これは、計画行列$\boldsymbol{X} \in \mathbb{R}^{150 \times 4}$によってそのデータセットを表現できることを意味する。ここで、$X_{i,1}$は花$i$の萼片の長さであり、$X_{i,2}$は花$i$の萼片の幅であり、以降も同様である。我々は計画行列のデータセットをどのように処理するかという観点で本書における学習アルゴリズムのほとんどを説明する。
もちろん、データセットを計画行列として述べるために、各exampleをベクトルとして説明することが可能でなければならず、これらのベクトルのそれぞれは同じ大きさでなければならない。これは常にできるとは限らない。例えば、様々な幅や高さを持つ写真の集合がある場合、異なる写真は異なるピクセル数を含むだろう。なので、同じベクトル長で記述できるであろう写真はそのすべてではない。[@sec:9.7]や[@sec:10]では、このような不均質なデータの様々な種類を扱う方法を述べる。これらのような場合では、$m$行の行列としてデータセットを記述するのではなく、$m$個の要素を含む集合${\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \dots, \boldsymbol{x}^{(m)}}$として記述する。この表記法は任意の2つのexampleベクトル$\boldsymbol{x}^{(i)}$と$\boldsymbol{x}^{(j)}$が同じ大きさを持つことを暗に示さない。
教師あり学習の場合、そのexampleは特徴の集合と同様にラベルまたは対象を含んでいる。例えば、写真からの物体認識を行うために学習アルゴリズムを用いたい場合、写真のそれぞれに写っているものを指定する必要がある。我々はこれを、人間を0で示し、車を1で示し、猫を2で示し、といったような数値コードで行うかもしれない。しばしば、特徴の観測結果$\boldsymbol{X}$の計画行列を含むデータセットで行うとき、example$i$に対するラベルをもたらす$y_i$を持つラベルのベクトル$\boldsymbol{y}$ももたらす。
もちろん、時折、ラベルは単一の値以上のものになり得るだろう。例えば、文全体を書き起こすために音声認識システムを訓練したい場合、各exampleの文に対するラベルは単語の列である。
教師あり学習と教師なし学習の正式な定義が存在しないのと同様に、データセットや経験の厳密な分類法は存在しない。ここで述べられる構造はほとんどの場合をカバーするが、新しいアプリケーションには新しいものを設計することが常に可能である。

### 例：線形回帰

経験を通してあるタスクでのコンピュータプログラムのパフォーマンスを改善することができるアルゴリズムとしての機械学習アルゴリズムの定義は幾分か抽象的である。これをより具体的にするため、我々は単純な機械学習アルゴリズムの例を示す。すなわち、線形回帰[linear regression]である。我々は、アルゴリズムの挙動を理解するのに役立つ機械学習のさらなる概念を導入するたびに、この例にたびたび立ち戻るだろう。
名前が暗示する通り、線形回帰は回帰問題を解く。言い換えれば、その目標はベクトル$\boldsymbol{x} \in \mathbb{R}^n$を入力として取り、出力としてスカラ$y \in \mathbb{R}$の値を予測することができるシステムを作ることである。線形回帰の出力は入力の一次関数である。求めるべき$y$を我々のモデルが予測した値を$\hat{y}$とする。我々はその出力を以下と定義する。

$$
\hat{y} = \boldsymbol{w}^\tap \boldsymbol{x}
$$

ここで、$\boldsymbol{w} \in \mathbb{R}^n$はパラメータ[parameters]のベクトルである。
パラメータはシステムの振る舞いを制御する値である。この場合、$w_i$はすべての特徴からの寄与の総和を取る前に特徴$x_i$で乗算する係数である。我々は$\boldsymbol{w}$を各特徴が予測にどれだけの影響を与えるかを決定する重み[weights]の集合とみなすことができる。$x_i$が正の重み$w_i$を受ける場合、その特徴の値が増加すると予測値$\hat{y}$の値は増加する。特徴が負の重みを受ける場合、その特徴の値が増加すると予測値の値は減少する。特徴の重みが絶対値で大きい場合、予測値において大きな影響をもたらす。特徴の重みがゼロであるならば、予測値に影響をもたらさない。
従って、我々はタスク$T$の定義を、$\hat{y} = \boldsymbol{w}^\top \boldsymbol{x}$を出力することで$\boldsymbol{x}$から$y$を予測すること、とする。次に、我々はパフォーマンス尺度$P$の定義を必要とする。
訓練には使わずにモデルがどれだけうまく処理するかを評価するためだけに使うような、$m$個のexampleの入力の計画行列があるとする。これらのexamplesのそれぞれに対して$y$の正しい値をもたらす回帰対象のベクトルもある。このデータセットは評価に使われるだけなので、我々はこれをテストセットと呼ぶ。我々は入力の計画行列を$\boldsymbol{X}^{(test)}$と表し、回帰対象のベクトルを$\boldsymbol{y}^{(test)}$と表す。
モデルのパフォーマンスを測定する方法のひとつはテストセットでのモデルの平均二乗誤差[mean squared error]を計算することである。$\boldsymbol{y}^{(test)}$がテストセットでのモデルの予測値をもたらすならば、平均二乗誤差は以下によって求められる。

$$
\text{MSE}_{test} = \frac{1}{m} \sum_i (\boldsymbol{\hat{y}}^{(test)} - \boldsymbol{y}^{(test)})_i^2
$$

直観として、この誤差の尺度が$\boldsymbol{\hat{y}}^{(test)} = \boldsymbol{y}^{(test)}$のときに$0$に減少することを理解できる。我々は以下であることも確認できる。

$$
\text{MSE}_{test} = \frac{1}{m} \| \boldsymbol{\hat{y}}^{(test)} - \boldsymbol{y}^{(test)} \|_2^2
$$

つまり、その誤差は予測値と対象の間のユークリッド距離が増加するたびに増加する。
機械学習アルゴリズムを作るために、我々は、アルゴリズムが訓練セット$(\boldsymbol{X}^{train}, \boldsymbol{y}^{train})$を観察することによって経験を得る事ができるときに$\text{MSE}_{test}$を減少させる方法で重み$\boldsymbol{w}$を改善するであろうアルゴリズムを設計する必要がある。これを行う直観的な方法のひとつ（これについては後に[@sec:5.5.1]でその根拠を示す）は訓練セットでの平均二乗誤差$\text{MSE}_{train}$を最小化することである。
$\text{MSE}_{train}$を最小化するために、その勾配が$\boldsymbol{0}$となる所で解くことができる。

$$
\nabla_{\boldsymbol{w}} \text{MSE}_{train} = 0
$$

$$
\Rightarrow \nabla_{\boldsymbol{w}} \frac{1}{m} \| \boldsymbol{\hat{y}}^{(train)} - \boldsymbol{y}^{(train)} \|_2^2 = 0
$$

$$
\Rightarrow \frac{1}{m} \nabla_{\boldsymbol{w}} \| \boldsymbol{X}^{(train)} \boldsymbol{w} - \boldsymbol{y}^{(train)} \|_2^2 = 0
$$

$$
\Rightarrow \nabla_{\boldsymbol{w}} (\boldsymbol{X}^{(train)} \boldsymbol{w} - \boldsymbol{y}^{(train)})^\top (\boldsymbol{X}^{(train)} \boldsymbol{w} - \boldsymbol{y}^{(train)}) = 0
$$

$$
\Rightarrow \nabla_{\boldsymbol{w}} ( \boldsymbol{w}^\top \boldsymbol{X}^{(train) \top} \boldsymbol{X}^{(train)} \boldsymbol{w} - 2 \boldsymbol{w}^\top \boldsymbol{X}^{(train) \top} \boldsymbol{y}^{(train)} + \boldsymbol{y}^{(train) \top} \boldsymbol{y}^{(train)}) = 0
$$

$$
\Rightarrow 2 \boldsymbol{X}^{(train) \top} \boldsymbol{X}^{(train)} \boldsymbol{w} - 2 \boldsymbol{X}^{(train) \top} \boldsymbol{y}^{(train)} = 0
$$

$$
\Rightarrow \boldsymbol{w} = (\boldsymbol{X}^{(train) \top} \boldsymbol{X}^{(train)})^{-1} \boldsymbol{X}^{(train) \top} \boldsymbol{y}^{(train)}
$$

その解が[@eq:5.12]によって求められる連立方程式は正規方程式[normal equations]として知られる。[@eq:5.12]を計算することは単純な学習アルゴリズムを構成する。線形回帰の学習アルゴリズムの動きの例は[@fig:5.1]を参照のこと。

![各点が1つの特徴を含む10個のデータポイントから成る訓練セットを持つ線形回帰問題。ただ1つの特徴のみが存在するので、重みベクトル$\boldsymbol{w}$は学習のための単一のパラメータ$w_1$のみを含む。（左）線形回帰は線$y = w_1$がすべての訓練点をできるだけ通るような$w_1$の設定を学習することを確認する。（右）プロットされた点は正規方程式で求められる$w_1$の値を示す。これは、訓練セットでの平均二乗誤差を最小化することを確認できる。](fig/5-1.png){#fig:5.1}

線形回帰という用語はしばしば、切片項$b$という追加パラメータを持つもう少し洗練されたモデルを指すのに使われる。このモデルでは、以下となる。

$$
\hat{y} = \boldsymbol{w}^\top \boldsymbol{x} + b
$$

すなわち、パラメータから予測値へのマッピングは依然として一次関数であるが、特徴から予測値へのマッピングはアフィン関数となっている。このアフィン関数への拡張は、モデルの予測値のプロットが依然として線のように見えるが、原点を通る必要がないことを意味する。バイアスパラメータ$b$を追加する代わりに、重みのみを持つが、常に$1$にセットされる追加の成分を持つ引数$\boldsymbol{x}$を持つモデルを使う続けることもできる。追加の$1$の成分に対応する重みはバイアスパラメータの役割を担う。我々は、本書を通してアフィン関数を指すときに、頻繁に「線形」という用語を用いる。
切片項$b$はしばしばアフィン変換のバイアス[bias]パラメータと呼ばれる。この用語法は、変換の出力がいずれの入力も存在しないときに$b$となるようバイアスされるという視点から派生する。この用語は、統計的推定アルゴリズムの期待される推定量は真の量に等しくない、という統計的なバイアスのアイデアとは異なる。
もちろん、線形回帰は極めて単純で限定的な学習アルゴリズムであるが、どのようにして学習アルゴリズムがうまく機能することができるかの例をもたらす。次節において、我々は学習アルゴリズム設計の基礎をなす基本原則のいくつかを述べ、それらの原則がどのようにしてより複雑な学習アルゴリズムを作るのに使うことができるかを実証する。

## 容量、過剰適合、過少適合

機械学習における中心的課題は、モデルを訓練したときのものだけでなく、アルゴリズムが新規で未見[new, previously unseen]の入力に対してうまく処理しなければならないということである。以前に観察していない入力でうまく処理する能力は汎化[generalization]と呼ばれる。
一般に、機械学習モデルを訓練するとき、我々は訓練セットへのアクセス権を持つ。すなわち、訓練誤差[training error]と呼ばれる訓練セットでの誤差の尺度を計算することができ、そして、この訓練誤差を減らす。これまでのところで言えば、我々が述べてきたものは単なる最適化問題である。最適化から機械学習を隔てるものは、我々が汎化誤差[generalization error]を欲していることである。これは、テスト誤差[test error]とも呼ばれ、同様に小さくされるべきものである。汎化誤差は新規の入力における誤差の期待値として定義される。ここで、期待値は、システムが実践で遭遇すると期待される入力の分布から描かれる、様々な取り得る入力に渡って取られる。
我々は一般に、訓練セットとは別に集められたテストセットのexamplesでのパフォーマンスを計測することで機械学習モデルの汎化誤差を推定する。
線形回帰の例では、訓練誤差を最小化することでモデルを訓練した。

$$
\frac{1}{m^{(train)}} \| \boldsymbol{X}^{(train)} \boldsymbol{w} - \boldsymbol{y}^{(train)} \|_2^2
$$

しかし、実際に気をつけるべきはテスト誤差$\frac{1}{m^{(test)}} \| \boldsymbol{X}^{(test)} \boldsymbol{w} - \boldsymbol{y}^{(test)} \|_2^2$である。
どうすれば訓練セットのみを観察できるときにテストセットにおけるパフォーマンスに影響を与えることができるか？統計的学習理論[statistical learning theory]の分野がいくつかの答えをもたらしてくれる。訓練セットとテストセットが任意に集められる場合、できることはほとんどない。訓練セットとテストセットの集められ方についてのいくつかの仮定を設けることができれば、いくらか歩みを進めることができる。
訓練データとテストデータはデータ生成過程[data-generating process]と呼ばれるデータセット上の確率分布によって生成される。一般に、まとめて独立同分布仮定[i.i.d. assumptions]として知られる仮定の集合を作る。これらの仮定は、各データセット内のexamplesがお互いに独立[independent]であり、訓練セットとテストセットが、お互いに同じ確率分布で描かれることで、同様に分布している[identically distributed]とする。この仮定は単一のexample上の確率分布でデータ生成過程を記述することを可能にする。我々はこの共有された基礎をなす分布をデータ生成分布[data-generating distribution]と呼び、$p_{data}$と表記する。この確率的フレームワークとi.i.d.仮定は訓練誤差とテスト誤差の間の関係を数学的に研究することを可能にする。
我々が訓練誤差とテスト誤差の間に見出だせる直接のつながりのひとつは無作為に選択するモデルの訓練誤差の期待値がそのモデルのテスト誤差の期待値に等しいことである。確率分布$p(\boldsymbol{x}, y)$を持ち、訓練セットとテストセットを生成するために繰り返しそこからサンプリングを行うとする。両方の期待値は同じデータセットサンプリング過程を用いて形成されるので、ある固定の値$\boldsymbol{w}$に対して、訓練セットの誤差の期待値はテストセットの誤差の期待値とまったく同じである。2つのconditionsはサンプリングを行うデータセットに割り当てる名前が異なるだけである。
もちろん、機械学習アルゴリズムを用いるとき、我々は前もってパラメータを固定せず、両方のデータセットをサンプリングする。訓練セットをサンプリングし、訓練セットの誤差をへらすためにパラメータを選択するのに用い、そして、テストセットをサンプリングする。この過程に基づくと、テスト誤差の期待値は訓練誤差の期待値以上となる。機械学習アルゴリズムがどれだけうまく処理するかを決定する要因は以下を行う能力である。

1. 訓練誤差を小さくする
2. 訓練誤差とテスト誤差のギャップを小さくする

これらの2つの要因は機械学習における2つの中心的な課題に対応する。すなわち、過少適合[underfitting]と過剰適合[overfitting]である。アンダーフィッティングはモデルが訓練セットで十分に小さな誤差の値を得られないときに発生する。オーバーフィッティングは訓練誤差とテスト誤差の間のギャップが大きすぎるときに発生する。
我々はその容量[capacity]を変更することでモデルがオーバーフィットかアンダーフィットのどちらを起こしやすいかを制御することができる。噛み砕いて言えば、モデルの容量とは多種多様な関数にフィットする能力のことである。低容量のモデルは訓練セットをフィットさせるのに苦戦するだろう。高容量のモデルはテストセットで役に立たないような訓練セットの特性を暗記することで過学習する可能性がある。
学習アルゴリズムの容量を制御する方法のひとつは、仮説空間[hypothesis space]、すなわち、学習アルゴリズムが解として選択できる関数の集合を選択することによるものである。例えば、線形回帰アルゴリズムはその仮説空間として入力のすべての一次関数の集合を持つ。我々は線形回帰をその仮説空間に、単なる一次関数ではなく、多項式を含めるように一般化することができる。そうすることでモデルの容量が増加する。
次数[degree]１の多項式はすでに馴染みのある線形回帰モデルをもたらす。これは以下の予測値を持つ。

$$
\hat{y} = b + wx
$$

線形回帰モデルにもたらされる別の特徴として$x^2$を導入することで、$x$の関数として二次関数であるモデルを学習できる。

$$
\hat{y} = b + w_1 x + w_2 x^2
$$

このモデルはその入力の二次関数を実装するにも関わらず、出力は依然としてパラメータの一次関数である。そのため、我々は依然として閉形式でモデルを訓練するために正規方程式を用いることができる。追加の特徴としてさらなる指数の$x$を加え続けることができる。例えば、次数9の多項式を得るには、以下となる。

$$
\hat{y} = b + \sum_{i = 1}^9 w_i x^i
$$

機械学習アルゴリズムは、これらの容量が、処理する必要があるタスクの実際の複雑さと与えられる訓練データ量に対して適切であるとき、一般に適切に処理するだろう。不十分な容量を持つモデルは複雑なタスクを解くことができない。高容量のモデルは複雑なタスクを解くことができるが、その容量が現在のタスクを解くのに必要な量より大きいとき、過学習するかもしれない。
[@fig:5.2]はこの原則を動きで示す。真の基礎にある関数が二次であるところの問題にフィットしようと試みる一次、二次、9次の予測器を比較する。一次関数は真の基礎をなす問題にある曲がり具合を捕捉できない。すなわち、アンダーフィットしている。9次の予測器は正しい関数を表現する能力があるが、訓練examplesより多いパラメータを持つので、訓練点をきっちり通る他の無限に多くの関数を表現する能力もある。非常に広範囲に渡る様々な解が存在するとき、うまく汎化する解を選択する機会はほとんどない。この例では、二次モデルはタスクの真の構造に完璧に一致している。なので、新しいデータにうまく一般化する。
これまでに、我々はモデルの容量を変化させる方法をひとつだけ述べてきた。すなわち、持っている入力の特徴の数を変化させ、同時にこれらの特徴に関連する新しいパラメータを追加することによる方法である。実際にはモデルの容量を変化させる方法はたくさん存在する。容量はモデルの選択によってのみ決定されるものではない。モデルは、訓練目的を減少させるためにパラメータを変化させるとき、学習アルゴリズムがどの関数の族から選ぶことができるかを指定する。これはモデルのrepresentational capacityと呼ばれる。多くの場合、この族の中で最良の関数を見つけることは難しい最適化問題である。実践では、学習アルゴリズムは最良の関数を実際に見つけることはせず、単に訓練誤差を大きく減らすだけである。最適化アルゴリズムの不備のようなこれらの追加の制限は、学習アルゴリズムの実効容量[effective capacity]がモデルの族のrepresentational capacityより小さいかもしれないことを意味する。

![この例の訓練セットに3つのモデルをフィットさせる。訓練セットは、$x$の値を無作為にサンプリングし、二次関数を計算することで決定論的に$y$を選択することによって、合成的に生成する。（左）データにフィットされる一次関数は、データに現れる曲線を捕捉出来ない、というアンダーフィッティングに悩まされる。（中）データにフィットされる二次関数は未見の点にうまく一般化する。これはそこまで大きなオーバーフィッティングおよびアンダーフィッティングに悩まされることはない。（右）データにフィットされる次数9の多項式はオーバーフィッティングに悩まされる。ここでは、我々は劣決定[underdetermined]の正規方程式を解くためにMoore-Penroseの擬似逆行列を用いた。この解はすべての訓練点をきっちり通るが、我々は正確な構造を抽出するのに十分幸運ではなかった。実際の下地の関数には現れない2つの訓練点の間の深い谷がある。データの左側で鋭く増加してもいるが、真の関数はこの範囲では減少する。](fig/5-2.png){#fig:5.2}

機械学習モデルの汎化を改善する事に関する近年のアイデアは、少なくともプトレマイオスくらい初期の哲学者にまでさかのぼる思考の改良である。多くの初期の学者は、現在ではオッカムの剃刀[Occam;s razor]（1287頃～1347）として最も広く知られている倹約の原則[principle of parsimony]を頼りにする。この原則は、既知の観察結果を同様にうまく説明している競合する仮説のうち、「最も単純な」ものを選択すべきである、ということを示している。このアイデアは形式化され、20世紀に統計的学習理論の創設者らによってより精緻に作り上げられた[@Vapnik1971; @Vapnik1982; @Blumer1989; @Vapnik1995]。
統計的学習理論はモデルの容量を定量化する様々な方法を提供する。これらの中で、最も良く知られているのは、Vapnik-Chervonenkis次元[Vapnik-Chervonenkis dimension]、または、VC次元である。VC次元は二値分類器の容量を計測する。VC次元は、分類器が任意にラベル付けできる$m$個の異なる$\boldsymbol{x}$の点の訓練セットが存在するときの$m$の取り得る最大値であると定義される。モデルの容量を定量化することは統計的学習理論が定量的な予測値を作ることを可能にする。統計的学習理論における最も重要な結果は、訓練誤差と汎化誤差の間の食い違い度[discrepancy]がモデル容量の増加に従って大きくなり、訓練examples数の増加に従って小さくなる量によって上から抑えられる[bounded from above]ことを示している[@Vapnik1971; @Vapnik1982; @Blumer1989; @Vapnik1995]。これらの境界は機械学習アルゴリズムが行えるような知的な正当化[intelectual justification]をもたらすが、深層学習アルゴリズムで行うときには実際にはほとんど使われない。これは、ひとつにはその境界がしばしば非常に緩くなるためであり、ひとつには深層学習アルゴリズムの容量を決定することがかなり難しくなり得るためである。深層学習アルゴリズムの容量を決定することの問題は、実効容量が最適化アルゴリズムの能力によって制限されるので、特に難しく、我々は深層学習において活用される一般的な非凸最適化問題の理論的理解をほんの少し有するのみである。
我々は、（訓練誤差とテスト誤差の間のギャップが小さくなるため、）単純な関数の方が一般化される可能性が高くなる一方、依然として低い訓練誤差を達成するため、十分に複雑な仮説を選択する必要があることを思い出す必要がある。一般に、訓練誤差は、モデル容量が増加するたびに、取り得る誤差の最小値に漸近するまで、減少する（誤差の尺度が最小値を持つと仮定した場合）。一般に、汎化誤差はモデル容量の関数としてU字型の曲線を持つ。これは[@fig:5.3]に図示される。

![容量と誤差の典型的な関係。訓練誤差とテスト誤差は別々に振る舞う。グラフの左端では、訓練誤差と汎化誤差は共に高い。これはunderfitting regimeである。容量が増加するに従って、訓練誤差は減少するが、訓練誤差と汎化誤差のギャップは増加する。最終的に、このギャップの大きさが訓練誤差の減少を上回り、overfitting regimeに突入する。ここでは、容量が最適容量を超えて大きくなりすぎている。](fig/5-3.png){#fig:5.3}

任意に高い容量の最も極端なケースに近づくため、我々はノンパラメトリックモデルの概念を導入する。ここまで、我々は線形回帰のようなパラメトリックモデルのみを見てきた。パラメトリックモデルは、いずれのデータが観測される前に、有限で固定の大きさを持つパラメータベクトルによって記述される関数を学習する。ノンパラメトリックモデルはそのような制限を持たない。
時折、ノンパラメトリックモデルは（取り得るすべての確率分布上で探索するアルゴリズムのような）実践において実装できない単なる理論的な抽象概念である。しかし、我々は複雑さを訓練セットの大きさの関数とすることによって実践的なノンパラメトリックモデルを設計することもできる。そのようなアルゴリズムの例のひとつは最近傍回帰[nearest neighbor regression]である。固定長の重みのベクトルを持つ線形回帰と異なり、最近傍回帰モデルは訓練セットから$\boldsymbol{X}$と$\boldsymbol{y}$を単に格納する。テスト点$\boldsymbol{x}$を分類することを求めるとき、モデルは訓練セット内の最も近い成分を探し出し、関連する回帰対象を返す。言い換えれば、$\i = argmin \| \boldsymbol{X}_{i,:} - \boldsymbol{x} \|_2^2$であるところの$\hat{y} = y_i$である。そのアルゴリズムは学習済み距離尺度のような$L^2$ノルム以外の距離測度に一般化することもできる[@Goldberger2005]。アルゴリズムが同率で最近傍であるすべての$\boldsymbol{X}_{i,:}$に対する$y_i$の値を平均化することによって同率状態を打開することができ、そして、このアルゴリズムは任意の回帰データセットで取り得る最小の訓練誤差を達成することができる（訓練誤差は、2つの同一の入力が異なる出力に関連する場合、ゼロより大きくなるかもしれない）。
最後に、我々はパラメトリックな学習アルゴリズムを必要に応じてパラメータ数を増やした別のアルゴリズムで包み込むことによってノンパラメトリックな学習アルゴリズムを作ることもできる。例えば、我々は入力の多項式展開の上での線形回帰によって学習された多項式の次数を変化させる学習の外側のループを想像できるだろう。
理想的なモデルはデータを生成する真の確率分布を知っているという神託である。そのようなモデルでさえ、依然として分布にいくらかのノイズが存在するので、多くの問題による誤差が課されるだろう。教師あり学習の場合、$\boldsymbol{x}$から$y$へのマッピングは本質的に確率的であり得るだろうし、$y$は$\boldsymbol{x}$に含まれるこれら以外の他の変数を伴う決定論的関数であり得るだろう。真の分布$p(\boldsymbol{x}, y)$から予測値を作る神託によって課される誤差はベイズ誤差[Bayes error]と呼ばれる。
訓練誤差と汎化誤差は訓練セットの大きさが変化するたびに変化する。汎化誤差の期待値は訓練examplesの数が増加しても一切増加しない可能性がある。ノンパラメトリックモデルでは、取り得る最良の誤差が達成されるまで、データが多いほどより良い汎化を生み出す。最適容量より小さい容量を持つ任意の固定のパラメトリックモデルはベイズ誤差を超える誤差の値に漸近するだろう。図は[@fig:5.4]を参照のこと。モデルが最適な容量を持ちながらも、未だ、訓練誤差と汎化誤差の間のギャップが大きいことが起こり得ることに注意する。この状況では、より多くの訓練examplesを集めることでこのギャップを減らすことができるかもしれない。

![訓練誤差とテスト誤差における訓練データセットの大きさの影響、および、最適なモデル容量における同様のもの。我々は、次数5の多項式に中程度のノイズ量を加えることに基づいた合成回帰問題を構成し、単一のテストセットを生成し、いくつかの異なる大きさの訓練セットを生成した。大きさごとに、95%の信頼区間を示すエラーバーをプロットするために
40個の異なる訓練セットを生成した。（上）二次モデルとテスト誤差を最小化するように選ばれた次数を持つモデルの2つの異なるモデルに対する訓練セットとテストセットのMSE。両方とも閉形式でフィットする。二次モデルでは、訓練セットの大きさが増加するに従って訓練誤差は増加している。これはより大きなデータセットがフィットしづらいためである。同時に、少量の不正確な仮説が訓練データと一致するので、テスト誤差が減少している。二次モデルはタスクを解くのに十分な容量を持っていないので、そのテスト誤差は高い値に漸近する。最適容量でのテスト誤差はベイズ誤差に漸近する。その訓練誤差は、訓練セットの特定のインスタンスを暗記する訓練アルゴリズムの能力のために、ベイズ誤差以下に落ち得る。訓練サイズが無限大へ増加するに従って、いずれの固定容量モデル（ここでは、二次モデル）の訓練誤差も少なくともベイズ誤差まで上昇しなければならない。（下）訓練セットの大きさが増加するに従って、（ここでは最適な多項式のregressorの次数として示される）最適容量は増加する。最適容量はタスクを解くために十分な複雑さへ達した後に変動しなくなる[plateaus]。](fig/5-4.png){#fig:5.4}

### No Free Lunch定理

学習理論は、機械学習アルゴリズムがexamplesの有限の訓練セットからうまく汎化できることを主張する。これはいくつかの論理の基本原則と矛盾するように思える。帰納的な推論、もしくは、examplesの限定的な集合から一般的な規則を推論することは論理的に正しくない。集合のすべてのメンバを述べる規則を論理的に推論するには、その集合のすべてのメンバに関する情報がなければならない。
ひとつには、機械学習は真に論理的な推論で使われるまったく特定の規則ではなく確率的規則のみをもたらすことでこの問題を回避する。機械学習は考慮している集合のほとんどのメンバについて確率的に正しい規則を求めることを約束する。残念ながら、これでも全部の問題を解く訳ではない。機械学習に対するノーフリーランチ定理[no free lunch theorem] [@Wolpert1996]は、取り得るすべてのデータ生成分布上で平均したとすると、すべての分類アルゴリズムは未見の点を分類するときに同じ誤差を持つ、ということを示している。言い換えれば、ある意味で、機械学習アルゴリズムが他のどんなものよりも全般的に優れているということは一切ない。我々が思い描くことができる最も洗練されたアルゴリズムは、すべての点が同じ種別に属すると予測するのと同じ平均パフォーマンスを（取り得るすべてのタスクで）持つ。
幸いにも、これらの結果は取り得るすべてのデータ生成分布で平均するときにのみ満たされる。我々が現実世界のアプリケーションで遭遇する種類の確率分布についての仮定を作る場合、これらの分布でうまく行われる学習アルゴリズムを設計できる。
これは、機械学習研究の目標が普遍的な学習アルゴリズムや絶対的な最良の学習アルゴリズムを探すことではない、ということを意味している。代わりに、我々の目標は、どんな種類の分布がAIエージェントが経験する「現実世界」と関連しているか、そして、どんな種類の機械学習アルゴリズムが我々の気にしている種類のデータ生成分布から描かれるデータをうまく処理するか、を理解することである。

### 正則化

ノーフリーランチ定理は、機械学習アルゴリズムが特定のタスクでうまく処理するように設計しなければならない、ということを暗示している。我々は学習アルゴリズムに選好[preferences]の集合を組み込むことでこれを行う。これらの選好がアルゴリズムに解くことを求める学習問題に沿っているとき、よりうまく処理される。
これまで、具体的に述べてきた学習アルゴリズムを修正する唯一の方法は学習アルゴリズムが選択できる解の仮説空間から関数を追加または削除することでモデルのrepresentational capacity
を増加または減少させることである。我々は回帰問題に対する多項式の次数の増加または減少の特定の例を与えた。我々がこれまでに述べてきた視点は単純化しすぎている。
アルゴリズムの振る舞いは、仮説空間内に許可されている関数の集合をどれだけ大きく作るかによってだけでなく、それらの関数の特定の恒等式によっても強く影響を受ける。これまでに学んできた学習アルゴリズムである線形回帰はその入力の一次関数の集合から成る仮説空間を持つ。これらの一次関数は、入力と出力の関係が本当に線形に近いところの問題に対して有用となり得る。これらは、非常に非線形なように振る舞う問題ではあまり役に立たない。例えば、線形回帰は、$x$から$\sin(x)$を予測するのに使ってみようとした場合、うまく行えないだろう。従って、我々は、どんな種類の関数から解を描くことができるかを選ぶことによって、アルゴリズムのパフォーマンスを制御することができる。
我々は学習アルゴリズムにその仮説空間におけるある解よりも別の解に対する選好を与えることもできる。これは、両方の関数が適格であるが、片方が好ましい、ということを意味している。好ましくない解は好ましい解より大幅に良く訓練データにフィットする場合に限って選択させるだろう。
例えば、我々はweight decayを含めるために線形回帰に対する訓練基準を修正できる。weight decayを伴う線形回帰を行うため、我々は、より小さな二乗$L^2$ノルムを持つように重みに対する選好を表現する訓練および基準の両方の平均二乗誤差を含む総和$J(\boldsymbol{w})$を最小化する。具体的には、以下となる。

$$
J(\boldsymbol{w}) = \text{MSE}_{train} + \lambda \boldsymbol{w}^\top \boldsymbol{w}
$$

ここで、$\lambda$はより小さな重みに対する選好の強さを制御する前もって選択した値である。$\lambda = 0$であるとき、課される選好はなく、$\lambda$が大きくなるに従って重みが小さくなる。$J(\boldsymbol{w})$の最小化は結果として、訓練データにフィットすることと小さくなることのトレードオフである重みの選択となる。これはより小さな傾斜を持つ解、または、より少量の特徴に重みを置く解をもたらす。weight decayを介してオーバフィットまたはアンダーフィットするモデルの傾向を制御できる方法の例として、我々は異なる$\lambda$の値で高次多項式の線形回帰モデルを訓練することができる。その結果は[@fig:5.5]を参照のこと。

![[@fig:5.2]の例の訓練セットに高次多項式の回帰モデルをフィットさせる。真の関数は二次であるが、ここでは次数9のモデルのみを用いる。高次モデルがオーバーフィッティングするのを防止するためにweight decayの具合を変化させる。（左）非常に大きな$\lambda$では、まったく傾斜がない関数を学習するようモデルに強いることができる。これは定数関数しか表現できないので、アンダーフィットする。（中）中くらいの$\lambda$の値では、学習アルゴリズムは正しい一般的な形状を持つ曲線を復元する。モデルはもっと複雑な形状を持つ関数を表現する能力があるにも関わらず、weight descayはより少ない係数で記述されるより単純な関数を用いるよう促した。（右）ゼロに近づく（すなわち、最小正則化を伴う劣決定問題を解くためにMoore-Penroseの擬似逆行列を用いる）weight decayでは、[@fig:5.2]で見たように、9次多項式は大幅にオーバーフィットする。](fig/5-5.png){#fig:5.5}

より一般的には、コスト関数にregularizerと呼ばれるペナルティを加えることで関数$f(\boldsymbol{x}; \boldsymbol{\theta})$を学習するモデルを正則化できる。weight decayの場合、regularizerは$\Omega(\boldsymbol{w}) = \boldsymbol{w}^\top \boldsymbol{w}$である。[@sec:7]では、他の多くのregularizersが取り得るものを見ていく。
別の関数よりもある関数の方を選好することを説明することは、仮説空間にメンバを含めたり除いたりすることよりもモデルの容量を制御する一般的な方法である。仮説空間から関数を除外することはその関数に対する無限に強い選好を表現することとみなすことができる。
我々のweight decayの例では、最小化する基準における追加の項を介して、明示的により小さな重みで定義される一次関数に対する選好を表現した。暗黙的でも明示的でも、異なる解に対して選好を表現する他の方法が多数存在する。共に、これらの様々なアプローチは正則化[regularization]として知られる。正則化とは訓練誤差ではなく汎化誤差を減らすことを意図した学習アルゴリズムを作るために施す任意の修正である。正則化は機械学習の分野の中心的な関心事のひとつであり、その重要性において匹敵するのは最適化のみである。
ノーフリーランチ定理は、最良の機械学習アルゴリズムが存在しない、特に、最適な正則化の形式が存在しないことを明らかにした。代わりに、我々は解きたい特定のタスクに適した正則化の形式を選択しなければならない。一般、そして、特に本書における深層学習の哲学とは、（人間ができるすべての知的なタスクのような）広範囲のタスクすべてが極めて汎用用途の正則化の形式を用いて効率的に解か得るであろうことである。

## ハイパーパラメータと検証セット

ほとんどの機械学習アルゴリズムは、アルゴリズムの振る舞いを制御するのに使うことができる設定であるハイパーパラメータを持つ。ハイパーパラメータの値は学習アルゴリズムそれ自体に適合しない（ある学習アルゴリズムが別の学習アルゴリズムに対する最良のハイパーパラメータを学習するような入れ子の学習プロシージャを設計できるにもかかわらず）。
[@fig:5.2]にある多項式回帰の例は、多項式の次数、という単一のハイパーパラメータを持つ。これは、容量のハイパーパラメータとして振る舞う。weight decayの強さを制御するのに使われる$\lambda$の値はもうひとつのハイパーパラメータの例である。
時折、設定は、最適化するのが難しいので、学習アルゴリズムが学習しないハイパーパラメータとなるよう選択される。さらに頻繁に、訓練セットでそのハイパーパラメータを学習することは適切でないので、設定はハイパーパラメータでなければならない。これはモデル容量を制御するすべてのハイパーパラメータに適用される。訓練セットで学習する場合、そのようなハイパーパラメータは常に取り得る最大のモデル容量を選択するだろうから、結果としてオーバーフィッティングになる（[@fig:5.3]を参照）。例えば、低次多項式かつ正のweight decay設定よりも高次多項式かつ$\lambda = 0$のweight decay設定の方が常にうまく訓練セットにフィットさせることができる。
この問題を解決するためには、訓練アルゴリズムが観察しないexamplesの検証セット[validation set]を必要とする。以前に我々は、学習プロセスが完了した後に、訓練セットと同じ分布に由来するexamplesから構成される、hold-out[訓練セットの一部から提供される]されたテストセットが学習器の汎化誤差を推定するのに使うことができることを説明した。テストexamplesが、ハイパーパラメータを含め、モデルについての選択を行うあらゆるところで使われていないことは重要である。このため、検証セットで使うことができるテストセットからのexampleは一切ない。従って、我々は常に訓練データから検証セットを構築する。具体的には、2つの互いに素な部分集合[two disjoint subsets]に訓練データを分割する。これらの部分集合のひとつはパラメータを学習するのに使われる。もう片方の部分集合は、訓練の後または最中に汎化誤差を推定するのに使われる、検証セットであり、それに従ってハイパーパラメータを更新することができる。パラメータを学習するのに使われるデータの部分集合は、これが訓練プロセス全体で使われるより大きなデータのプールと混同し得るにも関わらず、依然として一般に訓練セットと呼ばれる。ハイパーパラメータの選択を導くために使われるデータの部分集合は検証セットと呼ばれる。一般に、訓練データの約80%が訓練に使われ、20%が検証に使われる。検証セットはハイパーパラメータを「訓練」するのに使われるので、検証セットの誤差は汎化誤差を小さく見積もるだろうが、一般に訓練誤差が行うよりもその量は小さくなる。すべてのハイパーパラメータの最適化が完了した後、汎化誤差はテストセットを用いて推定できるだろう。
実際には、同じテストセットが長年に渡って様々なアルゴリズムのパフォーマンスを評価するのに繰り返し使われてきたとき、特に、そのテストセット上での報告された最新のパフォーマンスを破っている科学コミュニティからの試みのすべてを考慮する場合、我々は同じようにテストセットで楽観的評価を持つようになる。故に、ベンチマークは陳腐化し、訓練済みシステムの本当の実地パフォーマンスを反映しない。ありがたいことに、コミュニティは新しい（そして、通常はより野心的で大きな）ベンチマークデータセットに移行する傾向にある。

### 交差検証

データセットを固定の訓練セットと固定のテスト接tおに分けることは結果としてテストセットが小さくなる場合に問題となり得る。小さなテストセットは推定される平均テスト誤差の周りの統計的な不確かさを暗示することで、与えられたタスクにおいてアルゴリズム$A$がアルゴリズム$B$よりうまく機能することを主張することが難しくなる。
データセットが数十万個以上のexamplesを持つとき、これは深刻な問題ではない。データセットが小さすぎるとき、代替の手順は、増加する計算コストを代償として、平均テスト誤差の推定値においてexamplesのすべてを用いることを可能にする。これらの手順はもとのデータセットの無作為に選択された異なる部分集合または分裂[splits]での訓練およびテスト計算を繰り返すというアイデアに基づく。これらの中で最も一般的なものは、[@lst:5.1]に示される、$k$分割交差検証[k-fold cross-validation]の手順である。ここでは、データセットの区分が重複しない部分集合に分割することで形成される。そして、テスト誤差は$k$個の試行の平均を取ることで推定できるだろう。試行$i$では、$i$番目のデータの部分集合がテストセットとして使われ、残りのデータが訓練セットとして使われる。問題はそのような平均誤差の推定器の分散のunbiasedな推定器が存在しない[@Bengio2004]ので、近似が一般に用いられることである。

```
Define KFoldXV($\mathbb{D}$, $A$, $L$, $k$):
Require: $\mathbb{D}$、与えられたデータセットであり、要素$\boldsymbol{z}^{(i)}$を持つ
Require: A$$、学習アルゴリズムであり、入力としてデータセットを取り、学習済みの関数を出力する関数とみなせる
Require: $L$、損失関数であり、学習済みの関数$f$とexample $\boldsymbol{z}^{(i)} \in \mathbb{D}$から$\mathbb{R}$内のスカラへの関数とみなせる
Require: $k$、foldsの数
  \mathbb{D}$を、その和集合が$\mathbb{D}$となるような、$k$個の相互排他的な部分集合$\mathbb{D}_i$に分割する
  $i$を$1$から$k$まで繰り返す
    $f_i = A(\mathbb{D} \setminus \mathbb{D}_i)$
    $\boldsymbol{z}^{j}$を$\mathbb{D}_i$内の要素で繰り返す
      $e_j = L(f_i, \boldsymbol{z}^{j})$
    繰り返し終わり
  繰り返し終わり
  $\boldsymbol{e}$を返す
```
: $k$分割交差検証のアルゴリズム。これは、小さなテストセットにおける損失$L$の平均が高すぎる分散を持つかもしれないので、与えられたデータセット$\mathbb{D}$が汎化誤差の正確な推定値を生み出すには単純な訓練/テストまたは訓練/検証の分裂に対して小さすぎるとき、学習アルゴリズム$A$の汎化誤差を推定するのに使うことができる。データセット$\mathbb{D}$は要素として抽象的なexamples$\boldsymbol{z^{(i)}}$（$i$番目のexampleに対して）を含んでいる。これは、教師あり学習の場合では$\boldsymbol{z}^{(i)} = (\boldsymbol{x}^{(i)}, \boldsymbol{y}^{(i)})$の入力と対象の対を、教師なし学習では単なる入力$\boldsymbol{z}^{(i)} = \boldsymbol{x}^{(i)}$を表し得るだろう。このアルゴリズムは$\mathbb{D}$におけるexampleごとの誤差のベクトル$\boldsymbol{e}$を返す。この平均は推定される汎化誤差である。ここのexamplesの誤差はその平均の周りの信頼区間を計算するのに使うことができる[@e:5.47]。これらの信頼区間は交差検証を用いるとうまく正当化されないにも関わらず、アルゴリズム$A$の誤差の信頼区間が下の方にあり、アルゴリズム$B$の信頼区間と交差しない場合に限り、アルゴリズム$A$がアルゴリズム$B$より優れていることを宣言するのに使うことは依然として一般的なプラクティスである。



## 推定器、バイアス、分散

点推定は関心のある量の「最良」の推定値をもたらすための試みである。一般に、関心のある量は、[@sec:5.1.4]にある線形回帰の例における重みのように、あるパラメトリックモデルにおける単一のパラメータまたはパラメータのベクトルとなり得るが、a whole functionである可能性もある。
真の値とパラメータの推定値を区別するため、我々の慣例ではパラメータ$\boldsymbol{\theta}$の推定点を$\hat{\boldsymbol{\theta}}$で表記するだろう。
${\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)}}$が$m$個の独立同分布（i.i.d.）のデータ点の集合であるとする。点推定器[point estimators]または統計量[statistic]はそのデータの任意の関数である。

$$
\hat{\boldsymbol{\theta}} = g(\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)})
$$

この定義は、$g$が真の$\boldsymbol{\theta}$に近い値を返す必要も、$g$の範囲が$\boldsymbol{\theta}$の許容できる値の集合と同じである必要さえもない。この点推定器の定義は非常に一般的であり、推定器の設計者に大きな柔軟性を与えるだろう。故に、ほとんどすべての関数が推定器として定量化するが、良い推定器は、訓練データを生成した本物の下地の$\boldsymbol{\theta}$に近い出力を持つ関数である。
今のところ、我々は統計学に対して頻度論者的な視点を取っている。これはつまり、我々は、真のパラメータの値$\boldsymbol{\theta}$が固定だが不明である一方で、点推定$\hat{\boldsymbol{\theta}}$がデータの関数であると仮定している。データは無作為な過程から描かれるので、そのデータのいずれの関数も無作為である。従って、$\hat{\boldsymbol{\theta}}$は確率変数である。

#### 関数推定

時折、我々は関数推定[function estimation]（または、関数近似）を行うことに興味がある。ここで、我々は入力ベクトル$\boldsymbol{x}$が与えられたときの変数$\boldsymbol{y}$を推定しようとしている。$\boldsymbol{y}$と$\boldsymbol{x}$の間の近似の関係を記述する関数$f(\boldsymbol{x})$が存在すると仮定する。例えば、$\boldsymbol{y} = f(\boldsymbol{x} + \epsilon)$であると仮定できるだろう。ここで、$\epsilon$は$\boldsymbol{x}$から予測可能でない$\boldsymbol{y}$の部分を表す。関数推定において、我々はモデルまたは推定$\hat{f}$で$f$を近似することに興味がある。関数推定は実際には単にパラメータ$\boldsymbol{\theta}$を推定することと同じである。つまり、関数推定器$\hat{f}$は単に関数空間における点推定である。（[@sec:5.1.4]で言及される）線形回帰の例や（[@sec:5.2]で言及される）多項式回帰の例の両方は、パラメータ$\boldsymbol{w}$を推定するか$\boldsymbol{x}$から$y$へマッピングする関数$\hat{f}$を推定するかのいずれかとして解釈され得るであろうシナリオを示す。
そこで、我々は点推定器の最も一般的に研究される特性を説明し、それらがこれらの推定器について教えてくれるものを議論する。

### バイアス

推定器のバイアスは以下のように定義される。

$$
\text{bias}(\hat{\boldsymbol{\theta}}_m) = \mathbb{E}(\hat{\boldsymbol{\theta}}_m) - \boldsymbol{\theta}
$$

ここで、期待値はデータ全体に対するもの（確率変数からのサンプルとして理解される）であり、$\boldsymbol{\theta}$はデータ生成分布を定義するのに使われる$\boldsymbol{\theta}$の真の基礎となる値である。推定器$\hat{\boldsymbol{\theta}}_m$は$\text{bias}(\hat{\boldsymbol{\theta}}_m) = \boldsymbol{0}$である場合に不偏[unbiased]であると言う。これは、$\mathbb{E}(\hat{\boldsymbol{\theta}}_m) = \boldsymbol{\theta}$であることを暗に示す。推定器$\hat{\boldsymbol{\theta}}_m$は$\lim_{m \rightarrow \infty} \text{\hat{\boldsymbol{\theta}}_m} = \boldsymbol{0}$である場合に漸近的に不偏[asymptotically unbiased]であると言う。これは$\lim_{m \rightarrow \infty} \mathbb{E}(\hat{\boldsymbol{\theta}}_m) = \boldsymbol{\theta}$であることを暗に示す。

#### 例：ベルヌーイ分布

平均$\theta$を持つベルヌーイ分布に従って独立同分布的であるサンプルの集合${x^{(1)}, \dots, x^{(m)}}$を考える。

$$
P(x^{(i)}; \theta) = \theta^{x^{(i)}} (1 - \theta)^{(1 - x^{(i)})}
$$

この分布の$\theta$パラメータに対する一般的な推定器は訓練サンプルの平均である。

$$
\hat{\theta}_m \frac{1}{m} \sum_{i = 1}^m x^{(i)}
$$

この推定器が偏っているかどうかを決定するため、我々は[@eq:5.22]を[@eq:5.20]に置き換えることができる。

$$
\text{bias}(\hat{\theta}_m) = \mathbb{E}[\hat{\theta}_m] - \theta
$$

$$
= \mathbb{E}[\frac{1}{m} \sum_{i=1}^m x^{(i)}] - \theta
$$

$$
= \frac{1}{m} \sum_{i=1}^m \mathbb{E}[x^{(i)}] - \theta
$$

$$
= \frac{1}{m} \sum_{i=1}^m \sum_{x^{(i)} = 0}^1 (x^{(i)} \theta^{x^{(i)}} (1 - \theta)^{(1 - x^{(i)})}) - \theta
$$

$$
= \frac{1}{m} \sum_{i=1}^m (\theta) - \theta
$$

$$
= \theta - \theta = 0
$$

$\text{bias}(\hat{\theta}) = 0$であるので、推定器$\hat{\theta}$は不偏であると言える。

#### 例：平均のガウス分布推定器

では、ガウス分布$p(x^{(i)}) = \mathcal{N}(x^{(i)}; \mu, \sigma^2)$に従って独立同分布的であるサンプルの集合${x^{(1)}, \dots, x^{(m)}}$を考える。ここで、$i \in {1, \dots, m}$である。ガウスの確率密度関数は以下で与えられることを思い出してほしい。

$$
p(x^{(i)}; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2} \frac{(x^{(i)} - \mu)^2}{\sigma^2} \right)
$$

ガウス分布の平均パラメータの一般的な推定器は標本平均[sample mean]として知られる。

$$
\hat{\mu}_m = \frac{1}{m} \sum_{i=1}^m x^{(i)}
$$

標本平均の偏りを決定するため、その期待値の計算に再び目を向ける。

$$
\text{bias}(\hat{\mu}_m) = \mathbb{E}[\hat{\mu}_m] - \mu
$$

$$
= \mathbb{E} \left[ \frac{1}{m} \sum_{i=1}^m x^{(i)} \right] - \mu
$$

$$
= \left( \frac{1}{m} \sum_{i=1}^m \mathbb{E} \left[ x^{(i)} \right] \right) - \mu
$$

$$
= \left( \frac{1}{m} \sum_{i=1}^m \mu \right) - \mu
$$

$$
\mu - \mu = 0
$$

従って、標本平均がガウスの平均パラメータの不偏の推定器であることが分かる。

#### 例：ガウス分布の分散の推定器

この例では、ガウス分布の分散パラメータ$\sigma^2$の2つの異なる推定器を比較する。我々はいずれかの推定器が偏っているかどうかを知ることに関心がある。
我々が考慮する第1の$\sigma^2$の推定器は標本分散[sample variance]として知られる。

$$
\hat{\sigma}_m^2 = \frac{1}{m} \sum_{i=1}^m \left( x^{(i)} - \hat{\mu}_m \right)^2
$$

ここで、$\hat{\mu}_m$は標本平均である。より形式的に言えば、我々は以下を計算することに関心がある。

$$
\text{bias}(\hat{\sigma}_m^2) = \mathbb{E}[\hat{\sigma}_m^2] - \sigma^2
$$

まずは項$\mathbb{E}[\hat{\sigma}_m^2]$を計算することから始める。

$$
\mathbb{E}[\hat{\sigma}_m^2] = \mathbb{E} \left[ \frac{1}{m} \sum_{i=1}^m \left( x^{(i)} - \hat{\mu}_m \right)^2 \right]
$$

$$
= \frac{m - 1}{m} \sigma^2
$$

[@eq:5.37]に戻って、我々は$\hat{\sigma}^2$のバイアスが$-\sigma^2 / m$であると結論付ける。従って、標本分散は偏った推定器である。

以下の不偏標本分散[unbiased sample variance]の推定器は別のアプローチをもたらす。

$$
\tilde{\sigma}_m^2 = \frac{1}{m - 1} \sum_{i = 1}^m \left( x^{(i)} - \hat{\mu}_m \right)^2
$$

名前が示す通り、この推定器は不偏である。すなわち、$\mathbb{E}[\tilde{\sigma}_m^2] = \sigma^2$であることが分かる。

$$
\mathbb{E}[\tilde{\sigma}_m^2] = \mathbb{E} \left[ \frac{1}{m - 1} \sum_{i = 1}^m \left( x^{(i)} - \hat{\mu}_m \right)^2 \right]
$$

$$
= \frac{m}{m - 1} \mathbb{E}[\hat{\sigma}_m^2]
$$

$$
= \frac{m}{m - 1} \left( \frac{m - 1}{m} \sigma}^2 \right)
$$

$$
= \sigma^2
$$

我々は2つの推定器を持っている。一方はバイアスがあり、他方はバイアスがない。不偏な推定器は明らかに望ましいものであるが、常に「最良」の推定器であるとは限らない。これから見ていくが、しばしば他の重要な特性を持つバイアスありの推定器を用いることがある。

### 分散と標準誤差

考慮したいかもしれない推定器のもうひとつの特性はデータサンプルの関数として変化することが予測される量である。そのバイアスを定めるために推定器の期待値を計算したのと同様に、その分散を計算できる。推定器の分散[variance]は確率変数が訓練セットであるところの以下のような分散である。

$$
\text{Var}(\hat{\theta})
$$

また、分散の二乗根は標準誤差[standard error]と呼ばれ、$\text{SE}(\hat{\theta})$と表記される。
推定器の分散、または、標準誤差は、基礎をなすデータ生成過程からデータセットを独立してリサンプリングするとき、データから計算される推定値がどれだけ変化すると予測されるだろうかという尺度をもたらす。低いバイアスを示す推定器を好むかもしれないのと同様に、我々は比較的低い分散をもつことも好むだろう。
有限個のサンプルを用いるいずれかの統計量を計算するとき、同じ分布から別のサンプルを得て、これらの統計量が異なっていたかもしれないという意味で、真の下地のパラメータの推定値は不確かである。任意の推定器における分散の期待値は定量化したい誤差の源である。
平均の標準誤差は以下で求められる。

$$
\text{SE}(\hat{\mu}_m) = \sqrt{\text{Var} \left[ \frac{1}{m} \sum_{i=1}^m x^{(i)} \right]} = \frac{\sigma}{\sqrt{m}}
$$

ここで、$\sigma^2$はサンプル$x^i$の真の分散である。標準誤差はしばしば$\sigma$の推定値を用いて推定される。残念ながら、サンプルの分散の二乗根や分散の不偏な推定器の二乗根のいずれも標準偏差の不偏な推定値をもたらさない。両方のアプローチは真の標準偏差を過少に推定する傾向があるが、実践では依然として使われている。分散の不偏な推定器の二乗根の方がより過少推定しにくい。大きな$m$に対して、その近似は極めて合理的である。
平均の標準誤差は機械学習の実験において非常に有用である。我々はしばしばテストセットでの誤差の標本平均を計算することで汎化誤差を推定する。テストセットの中のexamples数はこの推定値の正確さを決める。平均が正規分布で近似的に分布するであろうことを教えてくれる中心極限定理の利点を活かし、我々は真の期待値が任意の選ばれた区間に降下する確率を計算するのに標準誤差を用いることができる。例えば、平均$\hat{\mu}_m$を中心とする95%の信頼区間は平均$\hat{\mu}_m$と分散$\text{SE}(\hat{\mu}_m)^2$を持つ正規分布のもとで以下のようになる。

$$
(\hat{\mu}_m - 1.96 \text{SE}(\hat{\mu}_m), \hat{\mu}_m + 1.96 \text{SE}(\hat{\mu}_m))
$$

機械学習の実験において、アルゴリズム$A$の誤差に対する95%信頼区間の上界がアルゴリズム$B$の誤差の95%信頼区間の下界より小さい場合、アルゴリズム$A$はアルゴリズム$B$より優れている、と言うのが一般的である。

#### 例：ベルヌーイ分布

もう一度、ベルヌーイ分布から独立同分布的に描かれるサンプルの集合${x^{(1)}, \dots, x^{(m)}}$を考える（再掲、$P(x^{(i)}; \theta) = \theta^{x^{(i)}} (1 - \theta)^{(1 - x^{(i)})}$）。今回は、推定器$\hat{\theta}_m = \frac{1}{m} \sum_{i=1}^m x^{(i)}$の分散を計算することに関心がある。

$$
\text{Var}(\hat{\theta}_m) = \text{Var} \left( \frac{1}{m} \sum_{i=1}^m x^{(i)} \right)
$$

$$
= \frac{1}{m^2} \sum_{i=1}^m \text{Var} \left( x^{(i)} \right)
$$

$$
= \frac{1}{m^2} \sum_{i=1}^m \theta (1 - \theta)
$$

$$
= \frac{1}{m^2} m \theta (1 - \theta)
$$

$$
= \frac{1}{m} \theta (1 - \theta)
$$

推定器の分散はデータセットのexamples数である$m$の関数に従って減少する。これは、人気の推定器の一般的な特性であり、一貫性を議論するときにこれに立ち返ろうと思う（[@sec:5.4.5]を参照）。

### 平均二乗誤差を最小化するためのバイアスと分散のトレードオフ

バイアスと分散は推定器において2つの異なる誤差の源を計測する。バイアスは関数またはパラメータの真の値からの偏差の期待値を計測する。一方の分散は、データの特定のサンプリングを引き起す可能性があるという推定器の値の期待値からの偏差の尺度をもたらす。
1つはバイアスの方が大きく、1つは分散のほうが大きい2つの推定器の間の選択を与えられるときに何が起こるか？どうやってこれらの間を選択するか？例えば、[@fig:5.2]に示される関数を近似することに興味があり、大きなバイアスを持つモデルと大きな分散に悩まされるモデルの間の選択肢を提示されるのみであると想像してみよう。どうやってこれらの間を選択するか？
このトレードオフに折り合いをつける最も一般的な方法は交差検証を用いることである。経験上、交差検証は多くの現実世界のタスクで大きく成功を収めている。あるいは、推定値の平均二乗誤差（MSE）を比較することもできる。

$$
\text{MSE} = \mathbb{E}[(\hat{\theta}_m - \theta)^2]
$$

$$
= \text{Bias}(\hat{\theta}_m)^2 + \text{Var}(\hat{\theta}_m)
$$

MSEはパラメータ$\theta$の推定器と真の値の間の、二乗誤差の意味での、全体の偏差の期待値である。[@eq:5.54]から明らかである通り、MSEを計算することはバイアスと分散の両方が組み込まれている。望ましい推定器は小さなMSEを持つこれらであり、これらはこれらのバイアスと分散をいくらか抑制するよう管理する推定器である。
バイアスと分散の関係は、容量、アンダーフィッティング、オーバーフィッティングといった機械学習の概念と強く結びついている。汎化誤差がMSEによって計測されるとき（ここで、バイアスと分散が汎化誤差の有意義な要素である）、容量が増加すると分散が増加し、バイアスが減少する傾向がある。これは[@fig:5.6]に図示される。ここでは、容量の関数としてU型の汎化誤差曲線を確認できる。

![容量が増加する（$x$軸）のに従って、バイアス（点線）は減少する傾向にあり、分散（破線）は増加する傾向にある。すると、汎化誤差（太線）に対して別のU型曲線を生み出す。ある軸に沿って容量を変化させる場合には最適な容量が存在し、容量がこの最適値を下回るとアンダーフィットし、上回るとオーバーフィットする。この関係は、[@sec:5.2]や[@fig:5.3]で述べられる、容量とアンダーフィッティングとオーバーフィッティングの間の関係に似ている。](fig/5-6.png){#fig:5.6}

### 一貫性

これまで、我々は固定サイズの訓練セットに対する様々な推定器の特性を議論してきた。通常、我々は訓練データの量が増えるにつれて推定器の振る舞いも考慮する。特に、我々は通常、データ点の数$m$が増加するにつれて、点の推定値は対応するパラメータの真の値に収束することを望む。我々は以下のようにしたい。

$$
\text{plim}_{m \rightarrow \infty} \hat{\theta}_m = \theta
$$

記号$\text{plim}$は確率における収束を示し、任意の$\epsilon > 0$に対して、$m \rightarrow \infty$につれて$P(|\hat{\theta}_m - \theta| > \epsilon) \rightarrow 0$となることを意味する。[@eq:5.55]で述べられる条件は一貫性[consistency]として知られる。これは時折、弱い一貫性と呼ばれ、$\hat{\theta}$の$\theta$へのほとんど確実な[almost sure]収束を指して強い一貫性とする。確率変数の列$\boldsymbol{\mathbf{x}}^{(1)}, \boldsymbol{\mathbf{x}}^{(2)}, \dots$の値$\boldsymbol{x}$への概収束[almost sure convergence]は$p(\lim_{m \rightarrow \infty} \boldsymbol{\mathbf{x}}^{(m)} = \boldsymbol{x}) = 1$であるときに発生する。
一貫性は、推定器によって課されるバイアスがデータexamplesの数が増えるにつれて減退することを保証する。しかし、そのreverseは真ではない。つまり、漸近的な不偏性は一貫性を暗示しない。例えば、$m$個のサンプルのデータセット${x^{(1)}, \dots, x^{(m)}}$で、正規分布$\mathcal{N}(x; \mu, \sigma^2)$の平均パラメータ$\mu$を推定することを考える。我々はデータセットの最初のサンプル$x^{(1)}$を不偏な推定器$\hat{\theta} = x^{(1)}$として用いることができるかもしれない。この場合、$\mathbb{E}(\hat{\theta}_m) = \theta$であるので、推定器は、見えるデータ点の数に関係なく、不偏である。もちろん、これは、推定値が漸近的に不偏であることを暗示する。しかし、これは、$m \rightarrow \infty$となるたびに$\hat{\theta}_m \rightarrow \theta$であるケースではないので、一貫性を持つ推定器ではない。

## 最尤推定

我々は一般的な推定器の定義やこれらの解析された特性を見てきた。しかし、こららの推定器はとこから来たのだろう？ある関数が良好な推定器であるかもしれないことを推測し、そのバイアスと分散を解析するのではなく、我々は様々なモデルに対して良好な推定器となる特定の関数群を導出できるある原則を手に入れたいと考える。
そのような原則の中で最も一般的なものは最尤[maximum likelihood]原則である。
実際のものだが分かっていないデータ生成分布$p_{data}(\boldsymbol{\mathbf{x}})$から独立して描かれる$m$個のexamplesの集合$\mathbb{X} = {\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m)}}$を考える。
$p_{model}(\boldsymbol{\mathbf{x}}; \boldsymbol{\theta})$が$\boldsymbol{\theta}$で指される同じ空間での確率分布のパラメトリックな族であるとする。言い換えれば、$p_{model}(\boldsymbol{x}; \boldsymbol{\theta}$は任意のconfiguration $\boldsymbol{x}$を真の確率$p_{data }(\boldsymbol{x})$を推定する実数にマッピングする。
従って、$\boldsymbol{\theta}$に対する最尤推定量[maximum likelihood estimator]は以下のように定義される。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} p_{model}(\mathbb{X}; \boldsymbol{\theta})
$$

$$
= \argmax_{\boldsymbol{\theta}} \prod_{i=1}^m p_{model}(\boldsymbol{x}^{(i)}; \boldsymbol{\theta})
$$

多数の確率にわたるこの積は様々な理由により都合が悪い可能性がある。例えば、これは数値的なアンダーフローを起こしがちである。より便利だが等価な最適化問題を得るため、尤度の対数を取ることがその$\argmax$を変化させないが、総乗を総和に便利に変換することを示す。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} \sum_{i=1}^m \log p_{model}(\boldsymbol{x}^{(i)}; \boldsymbol{\theta})
$$

$\argmax$は損失関数を再スケーリングするときに変化しないので、訓練データによって定義される経験分布$\hat{p}_{data}$に関する期待値として説明される基準のバージョンを得るために$m$で割ることができる。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} \mathbb{E}_{\boldsymbol{\mathbf{x}} \sim \hat{p}_{data}} \log p_{model}(\boldsymbol{x}; \boldsymbol{\theta})
$$

最尤推定を解釈する方法のひとつは、訓練セットで定義される経験分布$\hat{p}_{data}$とモデルの分布の間の相違度[dissimilarity]を、KLダイバージェンスによって計測されるその2つの間の相違の度合いによって、最小化すると見ることである。KLダイバージェンスは以下によって与えられる。

$$
D_{KL}(\hat{p}_{data} \| p_{model}) = \mathbb{E}_{\boldsymbol{\mathbf{x}} \sim \hat{p}_{data}} [\log \hat{p}_{data}(\boldsymbol{x}) - \log p_{model}(\boldsymbol{x})]
$$

左辺の項は、モデルではなく、データ生成過程だけの関数である。これは、KLダイバージェンスを最小化するためにモデルを訓練するとき、以下を最小化するだけで良いことを意味する。

$$
-\mathbb{E}_{\boldsymbol{\mathbf{x}} \sim \hat{p}_{data}} [\log p_{model}(\boldsymbol{x})]
$$

もちろん、これは[@eq:5.59]における最大化と同じである。

このKLダイバージェンスを最小化することは分布間の交差エントロピーを最小化することとぴったり対応する。多くの著者は、具体的にベルヌーイ分布やsoftmax分布の負の対数尤度を識別するために「交差エントロピー」という用語を用いるが、それは誤称[misnomer]である。負の対数尤度から成るいずれの損失も訓練セットによって定義される経験分布とモデルによって定義される確率分布の間の交差エントロピーである。例えば、平均二乗誤差は経験分布とガウスモデルの間の交差エントロピーである。
故に、我々は最大尤度をモデル分布を経験分布$\hat{p}_{data}$に一致させようとする試みと見ることができる。理想的には、真のデータ生成分布$p_{data}$に一致させたいが、この分布へは直接アクセスできない。
最適な$\boldsymbol{\theta}$は尤度を最大化しているかKLダイバージェンスを最小化しているかのどちらであるかに関わらず同じである一方で、目的関数の値は異なる。ソフトウェアでは、しばしば両方をコスト関数の最小化と表現する。故に、最大尤度は負の対数尤度（NLL）の最小化、または等価的に、交差エントロピーの最小化となる。最小KLダイバージェンスとしての最大尤度の視点は、KLダイバージェンスがゼロという既知の最小値を持つので、この場合に有用である。負の対数尤度は、$\boldsymbol{x}$が実数であるとき、実際に負となる。

### 条件付き対数尤度と平均二乗誤差

最尤推定量は、$\boldsymbol{\mathbf{x}}$が与えられたときに$\boldsymbol{\mathbf{y}}$を予測するための条件付き確率$P(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}}; \boldsymbol{\theta})$を推定するために容易に一般化することができる。これは、ほとんどの教師あり学習に対する基礎を形成するので、実際に最も一般的な状況である。$\boldsymbol{X}$がすべての入力を表し、$\boldsymbol{Y}$がすべての観測される対象を表すならば、条件付き最尤推定量は以下である。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} P(\boldsymbol{Y} | \boldsymbol{X}; \boldsymbol{\theta})
$$

examplesがi.i.dであると仮定される場合、これは以下に分解できる。

$$
\boldsymbol{\theta}_{ML} = \argmax_{\boldsymbol{\theta}} \sum_{i=1}^m \log P(\boldsymbol{y}^{(i)} | \boldsymbol{x}^{(i)}; \boldsymbol{\theta})
$$


#### 例：最大尤度としての線形回帰

[@se:5.1.4]で導入された線形回帰は最大尤度の手順として正当化できるだろう。以前に、我々は線形回帰を入力$\boldsymbol{x}$を取り、出力値$\hat{y}$を生成するように学習するアルゴリズムとして動機づけした。$\boldsymbol{x}$から$\hat{y}$へのマッピングは平均二乗誤差を最小化するよう選ばれる。これは、多かれ少なかれ任意に導入される基準である。我々は最尤推定の視点から線形回帰を再訪する。単一の推定値
$\hat{y}$を生み出す代わりに、今度はそのモデルを条件付き分布$p(y | \boldsymbol{x})$を生み出すとみなす。我々は、無限に大きい訓練セットを用いると、入力値$\boldsymbol{x}$は同じだが、$y$の値が異なるいくつかの訓練examplesを確認できるかもしれないことを想像することができる。学習アルゴリズムの目標は今や、そのすべてが$\boldsymbol{x}$と互換性を持つようなこれらすべての異なる$y$の値に分布$p(y | \boldsymbol{x})$をフィットさせることである。以前に得たのと同じ線形回帰アルゴリズムを導出するため、我々は$p(y | \boldsymbol{x}) = \mathcal{N}(y; \hat{y}(\boldsymbol{x}; \boldsymbol{w}), \sigma^2)$を定義する。関数$\hat{y}(\boldsymbol{x}; \boldsymbol{w})$はガウシアンの平均の予測値を求める。この例では、分散がユーザにいよって選ばれたある定数$\sigma^2$に固定されると仮定する。この$p(y | \boldsymbol{x})$の関数形式の選択によって、最尤推定の手順が以前に作ったのと同じ学習アルゴリズムを生み出すことを理解するだろう。そのexamplesはi.i.d.であることが仮定されているので、条件付き対数尤度（[@eq:5.63]）は以下によって求められる。

$$
\sum_{i=1}^m \log p(y^{(i)} | \boldsymbol{x}^{(i)}; \boldsymbol{\theta})
$$

$$
= -m \log \sigma - \frac{m}{2} \log (2 \pi) - \sum_{i=1}^m \frac{\| \hat{y}^{(i)} - y^{(i)} \|^2}{2 \sigma^2}
$$

ここで、$\hat{y}^{(i)}$は$i$番目の入力$\boldsymbol{x}^{(i)}$での線形回帰の出力であり、$m$は訓練examplesの数である。対数尤度を以下のような平均二乗誤差と比較すると、

$$
\text{MSE}_{train} = \frac{1}{m} \sum_{i = 1}^m \| \hat{y}^{(i)} - y^{(i)} \|^2
$$

$\boldsymbol{w}$に関する対数尤度の最大化が平均二乗誤差の最小化を行うのと同じようなパラメータ$\boldsymbol{w}$の推定値を生み出すことを即座に理解できる。2つの基準は値が異なるが、最適な場所は同じである。これは最尤推定の手順としてMSEの使用を正当化する。これから見ていく通り、最尤推定量はいくつかの望ましい特性を持っている。

### 最大尤度の特性

最尤推定量の主な魅力は、$m$の増加に従う収束率の観点において、examplesの数が$m \rightarrow \infty$となるに従って、漸近的に最良の推定器であることを示すことができるということである。
適切な条件下では、最尤推定量は一貫性の特性を持つ（[@sec:5.4.5]を参照）。これは、訓練examplesの数が無限大に近づくにつれて、パラメータの最尤推定はそのパラメータの真の値に収束することを意味する。これらの条件は以下の通り：

- 真の分布$p_{data}$はモデルの族$p_{model}(\cdot; \boldsymbol{\theta})$の中になければならない。そうでなければ、推定器は$p_{data}$を復元できない。
- 真の分布$p_{data}$はたった1つの$\boldsymbol{\theta}$の値に対応していなければならない。そうでなければ、最大尤度は正確な$p_{data}$を復元し得るが、どの$\boldsymbol{\theta}$の値がデータ生成過程で使われたかを定めることができないだろう。

最尤推定量に加えて他の帰納的原則が存在する。それらの多くは、一貫性のある推定器であるという特性を共有する。しかし、一貫性のある推定機は、統計的効率[statistical efficiency]において、異なる可能性がある。これは、ある一貫性のある推定器が固定のサンプル数$m$に対するより低い汎化誤差を得るかもしれない、あるいは、固定の汎化誤差レベルを得るためにより少ないexamplesを必要とするかもしれないことを意味する。
統計的効率は一般に（線形回帰のような）パラメトリックな場合[parametric case]において研究される。ここでは、我々の目標は、関数の値ではなく、（真のパラメータを識別可能であると仮定して）パラメータの値を推定することである。真のパラメータにどれだけ近いかを測る方法は、推定されたパラメータの値と真のパラメータの値の間の差の二乗を計算する、平均二乗誤差の期待値によるものである。ここでは、その期待値はデータ生成分布からの$m$個の訓練サンプルに対してである。このパラメトリックな平均二乗誤差は$m$が増加するたびに減少する。そして、$m$が大きくなると、Cramér-Raoの下界[Cramér-Rao lower bound] [@Rao1945; @Cramer1946]は、一貫性のある推定器が最尤推定量より小さなMSEを持つことができないことを示す。
これらの理由（一貫性と効率性）により、最大尤度はしばしば機械学習に対して使うのに好ましい推定器であると考えられる。examplesの数がオーバーフィッティングの振る舞いを生み出すのに十分に小さいとき、weight decayのような正則化戦略は訓練データが限られるときにより低い分散を持つ最大尤度のバイアスありバージョンを得るのに使われるかもしれない。

## ベイズ統計学

これまで我々は、頻度論者的統計学[frequentist statistics]や単一の$\boldsymbol{\theta}$の値の推定に基づくアプローチに触れ、そして、その1つの推定値に基づいてすべての予測を行うことを議論してきた。もうひとつのアプローチは予測を立てるときに取り得るすべての$\boldsymbol{\theta}$の値を考慮することである。後者はベイズ統計学[Bayesian statistics]の領域である。
[@sec:5.4.1]で述べた通り、頻度論者の視点では真のパラメータの値$\boldsymbol{\theta}$が決まっているが分からない[fixed but unknown]のであるのに対し、点推定$\hat{\boldsymbol{\theta}}$は（無作為なように見える）データセットの関数であるための確率変数である。
統計学におけるベイズ的な視点はかなり異なる。ベイジアンはstates of knowledgeにおける確かさの度合いを反映するために確率を用いる。データセットは直接観測されるので、無作為ではない。一方で、真のパラメータ$\boldsymbol{\theta}$は不明または不確かであり、故に、確率変数として表現される。
データを観測する前に、事前確率分布[prior probability distribution]$p(\boldsymbol{\theta})$（単に"the prior"と呼ぶこともある）を用いて$\boldsymbol{\theta}$の知識を表現する。一般に、機械学習の専門家[practitioner]は、いずれかのデータを観測する前に$\boldsymbol{\theta}$の値における不確かさの高い度合いを反映するために極めて広い（すなわち、高いエントロピーを持つ）事前分布を選択する。例えば、あるものは、$\boldsymbol{\theta}$が一様分布に従ってある有限の範囲またはボリュームにあるとa priori的に仮定するかもしれないだろう。代わりに、多くの事前確率は（より小さな大きさの係数、または、より定数に近い関数のような）「より単純」な解に対する選好を反映する。
いま、データサンプルの集合${x^{(1)}, \dots, x^{(m)}}$があると考える。ベイズの法則を介して事前確率とデータの尤度$p(x^{(1)}, \dots, x^{(m)} | \boldsymbol{\theta})$を組み合わせることで$\boldsymbol{\theta}$についての我々の信念[belief]に関するデータの影響を復元することができる。

$$
p(\boldsymbol{\theta} | x^{(1)}, \dots, x^{(m)}) = \frac{p(x^{(1)}, \dots, x^{(m)} | \boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(x^{(1)}, \dots, x^{(m)})}
$$

ベイズ推定が一般に用いられる筋書きでは、事前確率は高エントロピーを持つ比較的一様またはガウス分布として始まり、データの観測は通常、事後確率がエントロピーを喪失したり、パラメータのいくつかの非常に確率の高い値に集中することを引き起こす。
最尤推定と比較して、ベイズ推定は2つの重要な違いをもたらす。１つ目は、$\boldsymbol{\theta}$の点推定を用いて予測を立てる最大尤度のアプローチと異なり、ベイズのアプローチとは$\boldsymbol{\theta}$の完全な分布を用いて予測を立てることである。例えば、$m$個のexamplesを観測した後、次のデータサンプル$x^{(m+1)}$での予測される分布は以下によって求められる。

$$
p(x^{(m+1)} | x^{(1)}, \dots, x^{(m)}) = \int p(x^{(m+1)} | \boldsymbol{\theta}) p(\boldsymbol{\theta} | x^{(1)}, \dots, x^{(m)}) d \boldsymbol{\theta}
$$

ここで、正の確率密度を持つ$\boldsymbol{\theta}$のそれぞれの値は、事後確率密度それ自体で重み付けされた寄与で、次のexampleの予測に寄与する。${x^{(1)}, \dots, x^{(m)}}$を観測した後、依然として$\boldsymbol{\theta}$の値について極めて不確かであるならば、この不確かさは、立てられるであろういずれかの予測にも直接的に組み込まれる。
[@sec:5.4]において、分散を計算することによって頻度論者のアプローチが与えられた$\boldsymbol{\theta}$の点推定における不確かさに対してどのように対処するかを述べた。推定器の分散は、推定値が観測されたデータの別のサンプリングで変化するであろう量の評価[assessment]である。推定期における不確かさをどうやって扱うかという問いへのベイズ的な回答は、単にそれの上で積分することである。これは、オーバーフィッティングを上手に防ぐ[protect well against]傾向がある。もちろん、この積分は単なる確率の法則の応用であるので、ベイズのアプローチが正当化されるのを単純にする。一方で、推定器を構築するための頻度論者の機械[machinery]は、単一の点推定によってデータセットに含まれるすべての知識をまとめるためのかなりアドホックな決定に基づいている。
推定に対するベイズ的アプローチと最大尤度のアプローチとの重要な違いの2つ目はベイズ事前確率分布の寄与によるものである。事前確率は、a priori的に好まれるパラメータ空間の領域の方に確率質量密度[probability mass density]をずらすことによって影響を及ぼす。実践では、事前確率はしばしばより単純またはより滑らかであるモデルに対する選好を説明する。ベイズ的アプローチの批評家たちは予測に影響を与える主観的な人間の判断の源として事前確率を識別する。
ベイズの手法は一般に、限定的な訓練データが利用できるときにより良く一般化されるが、訓練examplesの数が大きいときに大きな計算コストに悩まされる。


#### 例：ベイズ的線形回帰

ここで、線形回帰のパラメータを学習するためのベイス推定のアプローチを考える。線形回帰では、スカラ$y \in \mathbb{R}$の値を予測するために入力ベクトル$\boldsymbol{x} \in \mathbb{R}^n$からの線形マッピングを学習する。予測はベクトル$\boldsymbol{w} \in \mathbb{R}^n$によってパラメータ化される。

$$
\hat{y} = \boldsymbol{w}^\top \boldsymbol{x}
$$

$m$個の訓練サンプルの集合$(\boldsymbol{X}^{(train), \boldsymbol{y}^{(train)}})$が与えられるとすると、我々は以下を持つ。

$$
p(\boldsymbol{y}^{(train)} | \boldsymbol{X}^{(train)}, \boldsymbol{w}) = \mathcal{N}(\boldsymbol{y}^{(train)}; \boldsymbol{X}^{(train)} \boldsymbol{w}, \boldsymbol{I}) \propto \exp \left( -\frac{1}{2} (\boldsymbol{y}^{(train)} - \boldsymbol{X}^{(train)} \boldsymbol{w})^\top (\boldsymbol{y}^{(train)} - \boldsymbol{X}^{(train)} \boldsymbol{w}) \right)
$$

ここで、我々は$y$に関するガウシアンの分散が1であるという仮定における標準のMSEの定式化に従う。以下では、表記上の負担を減らすため、$(\boldsymbol{X}^{(train)}, \boldsymbol{y}^{(train)})$を単に$(\boldsymbol{X}, \boldsymbol{y})$と記す。
モデルのパラメータベクトル$\boldsymbol{w}$上の事後確率分布を定めるため、まず事前確率分布を指定する必要がある。事前確率はこれらのパラメータの値についての我々の生来の信念[native belief]を反映するべきである。モデルのパラメータの観点で我々の事前確率の信念を説明することが難しいまたは不自然であることがあるが、実践では、$\boldsymbol{\theta}$についての高い不確かさの度合いを表現するようなかなり広範囲の分布を一般に仮定する。実数値のパラメータに対して、事前確率としてガウシアンを用いることが一般的である。

$$
p(\boldsymbol{w}) = \mathcal{N}(\boldsymbol{w}; \boldsymbol{\mu}_0, \boldsymbol{\Lambda}_0) \propto \exp \left( -\frac{1}{2} (\boldsymbol{w} - \boldsymbol{\mu}_0)^\top \boldsymbol{\Lambda}_0^{-1} (\boldsymbol{w} - \boldsymbol{\mu}_0) \right)
$$

ここで、$\boldsymbol{\mu}_0$と$\boldsymbol{\Lambda}_0$はそれぞれ事前確率分布の平均ベクトルと共分散行列である。[^1]

[^1]: 特定の共分散構造を用いるための理由が存在しなければ、一般的に対角共分散行列$\boldsymbol{\Lambda}_0 = \text{diag}(\boldsymbol{\lambda}_0)$を仮定する。

故に、事前確率が指定されることで、モデルパラメータ上の事後確率分布[posterior distribution]の決定に歩みを進めることができる。

$$
p(\boldsymbol{w} | \boldsymbol{X}, \boldsymbol{y}) \propto p(\boldsymbol{y} | \boldsymbol{X}, \boldsymbol{w}) p(\boldsymbol{w})
$$

$$
\propto \exp \left( -\frac{1}{2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{w})^\top (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{w}) \right) \exp \left( -\frac{1}{2} (\boldsymbol{w} - \boldsymbol{\mu}_0)^\top \boldsymbol{\Lambda}_0^{-1} (\boldsymbol{w} - \boldsymbol{\mu}_0) \right)
$$

$$
\propto \exp \left( -\frac{1}{2} \left( -2 \boldsymbol{y}^\top \boldsymbol{X} \boldsymbol{w} + \boldsymbol{w}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{w} + \boldsymbol{w}^\top \boldsymbol{\Lambda}_0^{-1} \boldsymbol{w} - 2 \boldsymbol{\mu}_0^\top \boldsymbol{\Lambda}_0^{-1} \boldsymbol{w} \right) \right)
$$

我々はいま$\boldsymbol{\Lambda}_m = (\boldsymbol{X}^\top \boldsymbol{X} + \boldsymbol{\Lambda}_0^{-1})^{-1}$と$\boldsymbol{\mu}_m = \boldsymbol{\Lambda}_m (\boldsymbol{X}^\top \boldsymbol{y} + \boldsymbol{\Lambda}_0^{-1} \boldsymbol{\mu}_0)$を定義する。これらの新しい変数を用いて、ガウス分布として書き直した事後確率を求める。

$$
p(\boldsymbol{w} | \boldsymbol{X}, \boldsymbol{y}) \propto \exp \left( -\frac{1}{2} (\boldsymbol{w} - \boldsymbol{\mu}_m)^\top \boldsymbol{\Lambda}_m^{-1} (\boldsymbol{w} - \boldsymbol{\mu}_m) + \frac{1}{2} \boldsymbol{\mu}_m^\top \boldsymbol{\Lambda}_m^{-1} \boldsymbol{\mu}_m \right)
$$

$$
\propto \exp \left( -\frac{1}{2} (\boldsymbol{w} - \boldsymbol{\mu}_m)^\top \boldsymbol{\Lambda}_m^{-1} (\boldsymbol{w} - \boldsymbol{\mu}_m) \right)
$$

パラメータベクトル$\boldsymbol{w}$を含まないすべての項は省略されてしまっている。これらは、分布が$1$に積分するよう正規化されなければならないという事実によって暗示される。[@e:3.23]は多変量ガウス分布を正規化する方法を示す。
この事後確率分布を調査することはベイズ推定の効果に対するいくつかの直観を得ることを可能にする。殆どの状況で、我々は$\boldsymbol{\mu}_0$を$\boldsymbol{0}$に設定する。$\boldsymbol{\Lambda}_0 = \frac{1}{\alpha} \boldsymbol{I}$に設定するならば、$\mu_m$は$\alpha \boldsymbol{w}^\top \boldsymbol{w}$のweight decayペナルティを持つ頻度論者的な線形回帰を行うのと同じ$\boldsymbol{w}$の推定値を求める。ひとつ違うことは、ベイズ推定が$\alpha$がゼロに設定される場合に未定義であるということである。つまり、我々は$\boldsymbol{w}$に関する無限に広い事前確率からベイズ的な学習プロセスを始めることができない。より重要な違いはベイズ推定が、推定値$\mu_m$のみをもたらすのではなく、$\boldsymbol{w}$の異なるすべての値がどれだけの可能性を持つかを示す共分散行列をもたらすことである。

### 最大事後確率（MAP）の推定

最も原則的なアプローチはパラメータ$\boldsymbol{\theta}$上の完全なベイズ事後確率分布を用いて予測を立てることであるが、これは依然としてしばしば単一の点推定を持つのに望ましい。点推定が望ましい一般的な理由のひとつは、もっとも関心のあるモデルに対するベイズ事後確率を伴うほとんどの操作が扱いづらく、点推定が扱いやすい近似をもたらすということである。単に最尤推定に戻るのではなく、我々は依然として、事前確率が点推定の選択に影響を与えられるようにすることでベイズのアプローチの利点のいくつかを享受する事ができる。これを行う合理的な方法のひとつは最大事後確率（MAP）の点推定を選択することである。MAP推定は最大事後確率（または、連続的な$\boldsymbol{\theta}$のより一般的な場合では最大確率密度）の点を選択する。

$$
\boldsymbol{\theta}_{MAP} = \argmax_{\boldsymbol{\theta}} p (\boldsymbol{\theta} | \boldsymbol{x}) = \argmax_{\boldsymbol{\theta}} \log p(\boldsymbol{x} | \boldsymbol{\theta}) + \log p(\boldsymbol{\theta})
$$

右辺では、$\log p(\boldsymbol{x} | \boldsymbol{\theta})$、すなわち、標準の対数尤度の項、および、事前確率分布に対応する$\log p(\boldsymbol{\theta})$を認識する。
例として、重み$\boldsymbol{w}$に関するガウス事前確率を伴う線形回帰モデルを考える。この事前確率は$\mathcal{N}(\boldsymbol{w}; \boldsymbol{0}, \frac{1}{\lambda} \boldsymbol{I}^2)$によって求められるならば、[@eq:5.79]における対数事前確率項はおなじみの$\lambda \boldsymbol{w}^\top \boldsymbol{w}$のweight decayのペナルティと$\boldsymbol{w}$に依存せず、学習プロセスに影響を及ぼさない項を足したものに比例する。故に、その重みに関するガウス事前確率によるMAPのベイズ推定はweight decayに対応する。
完全なベイズ推定と同様に、MAPベイズ推定は事前確率によってもたらされる、訓練データでは求められない情報を活用するという利点を持つ。この追加の情報はMAP点推定において分散を低減させるのに役立つ（ML推定と比較して）。しかし、これはバイアスの増加という対価を支払って行われる。
weight decayによって正則化された最尤学習のような、多くの正則化済み推定の戦略はベイズ推定へのMAP近似を作ると解釈できる。この視点は正則化が$\log p(\boldsymbol{\theta})$に対応する目的関数に追加の項を加えることから成るときに適用される。すべての正則化のペナルティがMAPベイズ推定に対応する訳ではない。例えば、あるregularizerの項は確率分布の対数でないかもしれない。他の正則化項はデータに依存する。これは、もちろん、事前確率分布が行うことを許可されていない。
MAPベイズ推定は複雑で未だに扱いづらい正則化項を設計するための容易な方法を提供する。例えば、より複雑なペナルティ項は、事前確率のように、単一のガウス分布ではなく、ガウシアンの混合を用いることによって導出できる[@Nowlan1992]。

## 教師あり学習アルゴリズム

教師あり学習アルゴリズムは、大雑把に言えば、入力$\boldsymbol{x}$と出力$\boldsymbol{y}$のexamplesの訓練セットを与えられたときに、ある入力をある出力と関連付けることを学ぶ学習アルゴリズムであることを[@sec:5.1.3]から思い出したい。多くの場合、出力$\boldsymbol{y}$は自動的に集めるのが難しいかもしれず、人間の「監督者」によって提供されなければならないが、この用語は依然として訓練セットの対象が自動的に集められたときにも適応される。

### 確率的教師あり学習

本書におけるほとんどの教師あり学習アルゴリズムは確率分布$p(y | \boldsymbol{x})$の推定に基づいている。我々は分布のパラメトリックな族$p(y | \boldsymbol{x}; \boldsymbol{\theta})$に対する最良のパラメータベクトル$\boldsymbol{\theta}$を求めるために最尤推定によって単にこれを行うことができる。
線形回帰がその族に対応することをすでに見てきている。

$$
p(y | \boldsymbol{x}; \boldsymbol{\theta}) = \mathcal{N}(y; \boldsymbol{\theta}^\top, \boldsymbol{x}, \boldsymbol{I})
$$

我々は線形回帰を確率分布の異なる族を定義することで分類シナリオに一般化できる。クラス0とクラス1の2つのクラスがある場合、こららのクラスのひと1確率を指定するだけで良い。これら2つの値は足し合わせて1にならなければならないので、クラス1の確率はクラス0の確率を決定する。
線形回帰に対して用いられる実数値上の正規分布は平均の観点でパラメータ化される。この平均に対して提供する値はいずれも有効である。二値変数上の分布は、その平均が常に0と1の間になければならないので、若干複雑である。この問題を解決する方法のひとつは線形関数の出力を区間$(0, 1)$に押し潰し、その値を確率として解釈するためにロジスティックシグモイド関数を用いることである。

$$
p(y = 1 | \boldsymbol{x}; \boldsymbol{\theta}) = \sigma(\boldsymbol{\theta}^top \boldsymbol{x})
$$

このアプローチはロジスティック回帰[logistic regression]として知られる（我々は回帰ではなく分類にこのモデルを用いているのでこの名前は幾分奇妙だが）。
線形回帰の場合、我々は正規方程式を解くことで最適な重みを求めることができた。ロジスティック回帰は幾分か更に難しい。その最適な重みに対する閉形式の解が存在しない。代わりに、対数尤度を最大化することでこれらを探索しなければならない。勾配降下法を用いて負の対数尤度を最小化することでこれを行うことができる。
この同じ戦略は、正しい種類の入力および出力変数上の条件付き確率分布のパラメトリックな族を書き記すことで、本質的に任意の教師あり学習問題に適応することができる。

### サポートベクトルマシン

教師あり学習への最も影響力のあるアプローチはサポートベクトルマシン[support vector machine]である[@Boser1992; @Cortes1995]。このモデルは一次関数$\boldsymbol{w}^\top \boldsymbol{x} + b$によって動かされるという点でロジスティック回帰と似ている。ロジスティック回帰と異なり、サポートベクトルマシンは確率をもたらさず、クラスの同一性を出力するのみである。SVMは、$\boldsymbol{w}^\top \boldsymbol{x} + b$が正であるときに正のクラスが現れることを予測する。同様に、$\boldsymbol{w}^\top \boldsymbol{x} + b$が負であるときに負のクラスが現れることを予測する。
サポートベクトルマシンに関連付けられる重要なイノベーションのひとつはカーネルトリック[kernel trick]である。カーネルトリックは、多くの機械学習アルゴリズムがexamples間の内積の観点で排他的に記述できることを観察することから成る。例えば、サポートベクトルマシンで使われる一次関数が以下のように書き直せると示すことができる。

$$
\boldsymbol{w}^\top \boldsymbol{x} + b = b + \sum_{i=1}^m \alpha_i \boldsymbol{x}^\top \boldsymbol{x}^{(i)}
$$

ここで、$\boldsymbol{x}^{(i)}$は訓練exampleであり、$\boldsymbol{\alpha}$は係数のベクトルである。学習アルゴリズムをこのように書き直すことは、与えられた特徴関数$\phi(\boldsymbol{x})$の出力およびカーネル[kernel]と呼ばれる関数$k(\boldsymbol{x}, \boldsymbol{x}^{(i)}) = \phi(\boldsymbol{x}) \cdot \phi(\boldsymbol{x}^{(i)})$とのドット積によって$\boldsymbol{x}$を置き換えることを可能にする。この$\cdot$演算子は$\phi(\boldsymbol{x})^\top \phi(\boldsymbol{x}^{(i)})$に類似した内積を表現する。いくつかの特徴空間に対して、我々は文字通りのベクトルの内積を用いないかもしれない。ある無限大次元の空間に対して、我々は他の種類の内積、例えば、総和ではなく積分に基づいた内積を用いる必要がある。こららの種類の内積の完全本書の範疇を超える。
カーネル計算でドット積を置き換えた後、我々は以下の関数を用いて予測を立てることができる。

$$
f(\boldsymbol{x}) = b + \sum_i \alpha_i k(\boldsymbol{x}, \boldsymbol{x}^{(i)})
$$

この関数は$\boldsymbol{x}$に関して非線形であるが、$\phi(\boldsymbol{x})$と$f(\boldsymbol{x})$の関係は線形である。また、$\boldsymbol{\alpha}$と$f(\boldsymbol{x})$の関係も線形である。カーネルベースの関数はすべての入力に$\phi(\boldsymbol{x})$を適用し、変換した新しい空間で線形モデルを学習することでデータを前処理することと厳密に等価である。
カーネルトリックは2つの理由のために強力である。第1に、これは効率的に収束することが保証される凸最適化技術を用いて$\boldsymbol{x}$の関数として非線形であるモデルを学習することを可能にする。$phi$が固定と考えて$\boldsymbol{\alpha}$のみを最適化するので、これは可能である。つまり、最適化アルゴリズムは異なる空間で線形であると決定関数をみなすことができる。第2に、カーネル関数$k$はしばしば、2つの$\phi(\boldsymbol{x})$ベクトルを素朴に構築したり、これらのドット積を明示的に取ったりするより大きく計算的に効率的である実装を認める。
いくつかの場合、$\phi(\boldsymbol{x})$は無限次元である可能性さえもある。これは、結果としてナイーブで明示的なアプローチに対して無限大の計算コストになる。多くの場合、$k(\boldsymbol{x}, \boldsymbol{x}')$は、$\phi(\boldsymbol{x})$が扱いづらいときでさえも、$\boldsymbol{x}$の非線形で扱いやすい関数である。扱いやすいカーネルを持つ無限次元の特徴空間の例として、我々は非負の整数$x$上の特徴マッピング$\phi(x)$を構築する。このマッピングが無限に多くの0に続いて$x$個の1を含むベクトルを返すとする。我々は対応する無限次元のドット積と厳密に等価であるカーネル関数$k(x, x^{(i)}) = \min (x, x^{(i)})$と書くことができる。
一般に使われるほとんどのカーネルはガウシアンカーネル[Gaussian kernel]である。

$$
k(\boldsymbol{u}, \boldsymbol{v}) = \mathcal{N}(\boldsymbol{u} - \boldsymbol{v}; 0, \sigma^2 \boldsymbol{I})
$$

ここで、$\mathcal{N}(\boldsymbol{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})$は標準正規密度である。このカーネルは、その値が$\boldsymbol{u}$から放射状に広がる$\boldsymbol{v}$空間における線に沿って減少するので、放射基底関数[radial basis function]（RBF）カーネルとしても知られる。ガウシアンカーネルは無限次元の空間におけるドット積に対応するが、この空間の導出は整数上のminカーネルの例よりもいささか込み入っている。
我々はガウシアンカーネルをテンプレートマッチング[template matching]の一種を処理することとみなすことができる。訓練ラベル$y$に関連付けられた訓練example $\boldsymbol{x}$はクラス$y$に対するテンプレートとなる。テスト点$\boldsymbol{x}'$がユークリッド距離でいうところの$\boldsymbol{x}$の近くにあるとき、ガウシアンカーネルは大きなレスポンスを持ち、$\boldsymbol{x}'$が$\boldsymbol{x}$テンプレートに非常に似ていることを示す。そして、モデルは関連する訓練ラベル$y$に大きな重みを置く。全体として、その予測は対応する訓練examplesの類似性によって重み付けされた多数のそのような訓練ラベルを組み合わせるだろう。
サポートベクトルマシンはカーネルトリックを用いて改良することができる唯一のアルゴリズムではない。他の多くの線形モデルはこの方法で改良することができる。カーネルトリックを採用するアルゴリズムのカテゴリはカーネルマシン[kernel machines]とかカーネル法として知られる[@Williams1996; @Scholkopf1999]。
カーネルマシンの主要な欠点は、$i$番目のexampleが決定関数に項$\alpha_i k(\boldsymbol{x}, \boldsymbol{x}^{(i)})$で寄与するので、決定関数を計算するコストが訓練examplesの数において線形であることである。サポートベクトルマシンは、ほとんどゼロを含む$\boldsymbol{\alpha}$ベクトルを学習することによってこれを軽減することができる。故に、新しいexampleを分類することは非ゼロの$\alpha_i$を持つ訓練examplesに対してのみカーネル関数を計算する必要がある。これらの訓練examplesはサポートベクトル[support vectors]として知られる。
カーネルマシンもまたデータセットが大きいときに訓練の高い計算コストに悩まされる。我々は[@sec:5.9]でこのアイデアに再訪する。一般的なカーネルを持つカーネルマシンはうまく一般化するのに苦労する。我々は[@sec:5.11]でその理由を説明する。深層学習の近代的な権化[incarnation]はカーネルマシンのこれらの制限を克服するよう設計された。現在の深層学習のルネサンスは、[@Hinton2006]によってニューラルネットワークがMNISTベンチマークにおいてRBFカーネルのSVMを凌駕する可能性があることを実証したときから始まった。

### 他の単純な教師あり学習アルゴリズム

我々はすでにもうひとつの確率的でない教師あり学習アルゴリズムである最近傍回帰に簡単にだが触れてきている。より一般的に言えば、$k$最近傍は分類や回帰に使う事ができる技術の族である。ノンパラメトリックな学習アルゴリズムとして、$k$最近傍は固定のパラメータ数に制約されない。我々は通常、$k$最近傍アルゴリズムを任意のパラメータを持つのではなくむしろ訓練データの単純な関数を実装するとみなすことができる。実際に、本当に訓練ステージや学習プロセスさえも存在しない。代わりに、テスト時に、新しいテスト入力$\boldsymbol{x}$に対して出力$y$を生成したいとき、我々は訓練データ$\boldsymbol{X}$において$\boldsymbol{x}$への$k$最近傍を求める。そして、我々は訓練セットにおける対応する$y$の値の平均を返す。これは$y$の値の平均を定義できる実質的に任意の種類の教師あり学習に対して機能する。分類の場合、$c_y = 1$および他のすべての$i$の値に対して$c_i = 0$を持つone-hotコードベクトル$\boldsymbol{c}$を平均することができる。従って、これらのone-hotコードの平均をクラスの確率分布を与えるものと解釈することができる。ノンパラメトリックな学習アルゴリズムとして、$k$最近傍は非常に高い容量を達成できる。例えば、多クラス分類タスクを持ち、0-1損失でパフォーマンスを計測するとするしよう。この設定では、1最近傍は、訓練examples数が無限大に近づくに従ってベイズ誤差の二倍に収束する。ベイズ誤差を超える誤差は、無作為に等しく遠い近傍の間の均衡を破壊することで単一の近傍を選択することから生じる。無限の訓練データが存在するとき、すべてのテスト点$\boldsymbol{x}$は距離ゼロに無限に多い訓練セットの近傍を持つだろう。アルゴリズムに対して、これらのうちのひとつを無作為に選ぶのではなく、投票するためにこれらすべての近傍を用いることを許可するならば、その手順はベイズ誤差の比率に収束する。$k$最近傍の高容量は大きな訓練セットが与えられたときに高い正確さを得ることを可能にする。しかし、それを行うには高い計算コストがかかり、小さい有限の訓練セットを与えられたときにうまく汎化できないかもしれない。$k$最近傍の弱みのひとつは、ある特徴が別の特徴よりdiscriminativeであることを学習できないことである。例えば、等方的なガウス分布から描かれる$\boldsymbol{x} \in \mathbb{R}^{100}$を持つ回帰タスクがあるが、たったひとつの変数$x_1$だけが出力に関連すると想像しよう。さらに、この特徴が、すべての場合で$y = x_1$となるのように、単に直接的に出力をエンコードするとしよう。最近傍回帰はこの単純なパターンを検知できないだろう。ほとんどの点$\boldsymbol{x}$の最近傍は、ただひとつの特徴$x_1$によってではなく、$x_2$から$x_{100}$の多数の特徴によって決定されるだろう。従って、小さな訓練セットでの出力は実質的に無作為となるだろう。
入力空間を領域に分割し、各領域ごとに別個のパラメータを持つもうひとつの学習アルゴリズムのタイプは決定木[decision tree] [@Breiman1984]およびその多くの変種である。[@fig:5.7]に示される通り、決定木の各ノードは入力空間における領域に関連付けられ、内部ノードはその領域を子ノードごとに1つの部分領域に分割する（一般的には軸平行な切断を用いる）。故に、空間は、葉ノードと入力領域が一対一対応する、オーバーラップしない領域に分割される。各葉ノードは通常、その入力領域におけるすべての点を同じ出力にマッピングする。決定木は通常、本書の範疇を超える特殊化されたアルゴリズムで訓練される。決定木が通常では実践においてこれらをパラメトリックモデルにするような大きさの制限を伴って正則化されるにも関わらず、学習アルゴリズムは任意の大きさの木を学習することができればノンパラメトリックと考えることができる。これらが一般に使われる、軸平行な分割と各ノード内の一定の出力を持つ決定木はロジスティック回帰でも容易であるいくつかの問題を解くのに苦労する。例えば、2クラス問題がある場合、正のクラスは$x_2 > x_1$、つまり、決定境界が軸平行でないところならどこでも発生する。従って、決定木は、軸平行なstepsを持つ真の決定関数を常に行き来するstep関数を実装して、多数のノードで決定境界を近似する必要があるだろう。

![決定木がどのように機能するかを説明する図。（上）ツリーの各ノードは左の子ノード（0）か右の子ノード（1）に入力exampleを送ることを選択する。内部ノードは円で、葉ノードは四角で描かれる。各ノードはツリー内のその位置に対応するバイナリ列の識別子で表示され、親の識別子に1ビットを加えることで得られる（0 = 左または上を選択、1 = 右または下を選択）。（下）ツリーが空間を領域に分割する。2D平面は決定木が$\mathbb{R}^2$をどのように分割するかを示す。ツリーのノードは、examplesを分類するのに使われる分割線に沿って描かれる内部ノードとそれらが受け取るexamplesの領域の中心に描かれる葉ノードをそれぞれ持つこの平面にプロットされる。その結果は葉ノードあたり1つの区分を持つ区分定数関数である。各葉ノードは少なくとも1つの訓練exampleが定義されることを必要とする。なので、決定木が訓練examplesの数より多い極大を持つ関数を学習することを可能にしない。](fig/5-7.png){#fig:5.7}

これまで見てきた通り、最近傍予測器と決定木は多くの制限を持つ。それにも関わらず、これらは計算資源が制限されているときに有用な学習アルゴリズムである。我々は洗練されたアルゴリズムと$k$最近傍または決定木のベースラインの間の類似性と差異についてを考えることでより洗練された学習アルゴリズムに対する直観を構築することもできる。
伝統的な教師あり学習アルゴリズムに関するさらなる資料については[@Murphy2012; @Bishop2006; @Hastie2001]、または、その他の機械学習のテキストを参照のこと。

## 教師なし学習アルゴリズム

教師なしアルゴリズムが監督の信号ではなく「特徴」のみを経験するものであることを[@sec:5.1.3]から再掲する。教師ありアルゴリズムと教師なしアルゴリズムの区別は、監督者によってもたらされる値が特徴か対象かどうかを判別するための客観的なテストが存在しないので、公式にも厳密にも定義されていない。非公式的には、教師なし学習はexamplesを注釈付するのに人間の力を必要としない分布から情報を抽出しようとするほとんどの試みを指す。この用語は通常、密度推定、分布からサンプルを描くための学習、ある分布からデータをdenoiseするための学習、データが近くに置かれる多様体を求めること、または、関連するexamplesのグループにデータをクラスタリングすることと関連付けられる。
古典的な教師なし学習タスクはデータの「最良」の表現を求めることである。「最良」というと様々な解釈ができるが、一般的に言えば、その表現を$\boldsymbol{x}$自体より単純でアクセスしやすいように維持することを目的としたあるペナルティまたは制約に従いつつ、できる限り$\boldsymbol{x}$に関する情報を多く保持する表現を探す。
より単純な表現を定義する方法は複数存在する。最も一般的な3つはより低次元の表現、疎な表現、独立した表現を含む。低次元表現はより小さな表現に$x$に関する情報をできるだけ多く圧縮しようと試みる。疎な表現[@Barlow1989; @Olshausen1996; @Hinton1997]はデータセットを成分がほとんどの入力に対してほぼゼロである表現に埋め込む。スパース表現を用いるには一般に表現の次元を増やす必要がある。すなわち、その表現がほとんどゼロになっても、過剰な情報は破棄されない。これは結果として表現空間の軸に沿ってデータを分布させる傾向にある表現の全体構造となる。独立表現は表現の次元が統計的に独立であるようなデータの分布の根底にある分散の源を解きほぐす[disentangle]ことを試みる。
もちろん、これらの3つの基準は確かに相互排他的ではない。低次元表現はしばしば元々の高次元データより依存性が少ないまたは弱い要素を生み出す。これは表現の大きさを減らす方法のひとつが冗長性を見つけて取り除くことであるためである。より多くの冗長性を識別子し取り除くことは次元削減アルゴリズムに破棄する情報を少なくしつつ、より大きな圧縮を達成することを可能にする。
表現の概念は深層学習の中心的なテーマのひとつであり、それ故に、本書における中心的なテーマのひとつである。この節において、我々は表現学習アルゴリズムのいくつかの単純な例を開発する。それと共に、これらの例のアルゴリズムが上記の3つすべての基準を操作可能にする方法を示す。残りの章のほとんどは様々な方法でこれらの基準を発展させる追加の表現学習アルゴリズムを導入するか、他の基準を導入する。

### 主成分解析

[@sec:2.12]において、我々は主成分解析アルゴリズムがデータを圧縮する方法を提供することを確認した。我々はPCAをデータの表現を学習する教師なし学習アルゴリズムとしてみなすこともできる。この表現は上で述べられる単純な表現に対する基準の2つに基づいている。PCAはもとの入力より低い次元を持つ表現を学習する。これはその要素が互いに線形な相関を持たない表現も学習する。これはその要素が統計的に独立である表現を学習する基準への第一歩である。完全な独立性を達成するため、表現学習アルゴリズムもまた変数間の非線形な関係を取り除かなければならない。
PCAは[@fig:5.6]に示されるように入力$\boldsymbol{x}$を表現$\boldsymbol{z}$に投影するデータの直交線形変換を学習する。[@sec:2.12]では、我々は、（平均二乗誤差の意味で）元のデータを最適に再構築する1次元の表現を学習できるであろうこと、および、この表現が実際にはデータの第一主成分に対応することを確認した。故に、我々はデータ内の情報をできるだけ多く保持する単純で有効な次元削減手法としてPCAを用いることができる（再び、最小二乗の再構築誤差で計測されるので）。以下では、PCA表現がもとのデータ表現$\boldsymbol{X}$をどのようにdecorrelateするかを学ぶだろう。

![PCAは最も大きい分散の方向を新しい空間の軸に揃える線形投影を学習する。（左）元のデータは$\boldsymbol{x}$のサンプルから成る。この空間では、分散は軸並行でない方向に沿って発生しているかもしれないだろう。（右）変換されたデータ$\boldsymbol{z} = \boldsymbol{x}^\top \boldsymbol{W}$は軸$z_1$にほぼ沿って変化する。二番目に大きい分散は$z_2$に沿っている。](fig/5-8.png){#fig:5.8}

$m \times n$の計画行列$\boldsymbol{X}$を考えるとする。我々はデータがゼロの平均$\mathbb{E}[\boldsymbol{x}] = \boldsymbol{0}$を持つことを仮定するだろう。そうでない場合、データは前処理ステップにおいてすべてのexamplesからの平均を差し引くことで容易に中心化できる。
$\boldsymbol{X}$に関連付けられた不偏なサンプルの共分散行列は以下で与えられる。

$$
\text{Var}[\boldsymbol{x}] = \frac{1}{m - 1} \boldsymbol{X}^\top \boldsymbol{X}
$$

PCAは（線形変換を通して）表現$\boldsymbol{z} = \boldsymbol{W}^\top x$を求める。ここで、$Var[\boldsymbol{z}]$は対角である。
[@sec:2.12]では、計画行列$\boldsymbol{X}$の主成分が$\boldsymbol{X}^\top \boldsymbol{X}$の固有ベクトルによって与えられることを確認した。この視点により、以下となる。

$$
\boldsymbol{X}^\top \boldsymbol{X} = \boldsymbol{W} \boldsymbol{\Lambda} \boldsymbol{W}^\top
$$

この節では、主成分の代わりの導出を活用する。主成分は特異値分解（SVD）を介して得ることもできるだろう。具体的には、これらは$\boldsymbol{X}$の右特異ベクトルである。これを確認するため、分解$\boldsymbol{X} = \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^\top$における右特異ベクトルを$\boldsymbol{W}$とする。そして、固有ベクトル基底として$\boldsymbol{W}$によって元の固有ベクトルの方程式を復元する。

$$
\boldsymbol{X}^\top \boldsymbol{X}  (\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^\top)^\top \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^\top = \boldsymbol{W} \boldsymbol{\Sigma}^2 \boldsymbol{W}^\top
$$

SVDはPCAが結果として対角の$\test{Var}[z]$になることを示すのに役立つ。$\boldsymbol{X}$のSVDを用いると、我々は以下として$\boldsymbol{X}$の分散を表現できる。

$$
\text{Var}[\boldsymbol{x}] = \frac{1}{m - 1} \boldsymbol{X}^\top \boldsymbol{X}
$$

$$
= \frac{1}{m - 1} (\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^\top)^\top \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^\top
$$

$$
= \frac{1}{m - 1} \boldsymbol{W} \boldsymbol{\Sigma}^\top \boldsymbol{U}^\top \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{W}^\top
$$

$$
= \frac{1}{m - 1} \boldsymbol{W} \boldsymbol{\Sigma}^2 \boldsymbol{W}^\top
$$

ここで、我々は、特異値分解の$\boldsymbol{U}$行列が直交であると定義されるので、$\boldsymbol{U}^\top \boldsymbol{U} = \boldsymbol{I}$であるという事実を用いる。これは必要に応じて$\boldsymbol{z}$の共分散が対角であることを示す。

$$
\text{Var}[\boldsymbol{z}] = \frac{1}{m - 1} \boldsymbol{Z}^\top \boldsymbol{Z}
$$

$$
= \frac{1}{m - 1} \boldsymbol{W}^\top \boldsymbol{X}^\top \boldsymbol{X} \boldsymbol{W}
$$

$$
= \frac{1}{m - 1} \boldsymbol{W}^\top \boldsymbol{W} \boldsymbol{\Sigma}^2 \boldsymbol{W}^\top \boldsymbol{W}
$$

$$
= \frac{1}{m - 1} \boldsymbol{\Sigma}^2
$$

ここで、今回は、再びSVDの定義から$\boldsymbol{W}^\top \boldsymbol{W} = \boldsymbol{I}$であるという事実を用いる。
上記の解析は、線形変換$\boldsymbol{W}$を介してデータ$\boldsymbol{x}$を$\boldsymbol{z}$に投影するとき、結果の表現は（$\boldsymbol{\Sigma}^2$で与えられる）対角共分散行列を持つことを示している。これは、$\boldsymbol{z}$の個々の要素が相互に相関を持たないことを即座に暗示する。
要素が相互に相関を持たない表現にデータを変換するPCAの能力はPCAの非常に重要なの特性である。そのデータを基礎とするバラツキの不明な要因を紐解こうとする表現の単純な例である。PCAの場合、この紐解きは$\boldsymbol{z}$に関連付けられた新しい表現空間の基底と分散の主軸を並行にするような（$\boldsymbol{W}$で記述される）入力空間の回転を求めるという形を取っている。
相関はデータの要素感の依存性の重要なカテゴリであるが、我々は特徴の依存性のより複雑な形式を紐解く表現を学習することにも関心がある。このため、単純な線形変換でできること以上のものが必要になるだろう。

### k平均法

単純な表現学習アルゴリズムのもうひとつの例は$k$平均法[k-means clustering]である。$k$平均法アルゴリズムは互いに近い$k$個の異なるexamplesのクラスタに訓練セットを分割する。故に、我々はこのアルゴリズムを入力$\boldsymbol{x}$を表現する$k$次元のone-hotコードベクトル$\boldsymbol{h}$をもたらすとみなすことができる。$\boldsymbol{x}$がクラスタ$i$に属する場合、$h_i = 1$であり、表現$\boldsymbol{h}$の他のすべての成分はゼロである。
$k$平均法によってもたらされるone-hotコードは、その成分の大部分がすべての入力に対してゼロであるので、スパース表現の例である。後に、我々はより柔軟なスパース表現を学習する他のアルゴリズムを開発する。ここでは、1つより多い成分が各入力$\boldsymbol{x}$にたいして非ゼロであり得る。one-hotコードは分散表現[distributed representation]の利点の多くを失わせるスパース表現の極端な例である。one-hotコードは依然として統計的な利点をもたらし（これは同じクラスタ内のすべてのexamplesが互いに似ているというアイデアを自ずと伝える）、表現全体が単一の整数によって捕捉され得るという計算的な利点をもたらす。
$k$平均法アルゴリズムは$k$個の異なる重心${\boldsymbol{\mu}^{(1)}, \dots, \boldsymbol{\mu}^{(k)}}$を様々な値に初期化し、収束まで2つの異なるステップの間を代替することによって機能する。一方のステップでは、各訓練exampleがクラスタ$i$に割り当てられる。ここで、$i$は最も近い重心$\boldsymbol{\mu}^{(i)}$のインデックスである。もう一方のステップでは、各重心$\boldsymbol{\mu}^{(i)}$はクラスタ$i$に割り当てられるすべての訓練examples$\boldsymbol{x}^{(j)}$の平均に更新される。

クラスタリングに属する1つの困難さはクラスタリング問題が、データのクラスタリングがどれだけうまく現実世界に対応するかを計測する単一の基準が存在しないという意味で、生来的に不良設定[ill posed]であることである。我々はクラスタ重心からクラスタのメンバへの平均ユークリッド距離のようなクラスタリングの特性を計測できる。これはクラスタ割当から訓練データをどれだけうまく再構築できるかを教えてくれることを可能とする。我々はクラスタ割当が現実世界の特性とどれだけうまく対応するかを知らない。さらに、すべてが現実世界のある特性とうまく対応するような多数の様々なクラスタリングが存在するかもしれない。我々は1つの特徴に関連するクラスタリングを求めることを望むが、我々のタスクと関連しない別の等価で有効なクラスタリングを得るかもしれない。例えば、赤いトラックの画像、赤い車の画像、灰色のトラックの画像、灰色の車の画像から成るデータセットに関して2つのクラスタリングアルゴリズムを実行するとしよう。各クラスタリングアルゴリズムに2つのクラスタを求めるようにするならば、一方のアルゴリズムは車のクラスタとトラックのクラスタを見つけるかもしれないが、もう一方は赤い車両のクラスタと灰色の車両のクラスタを見つけるかもしれない。クラスタ数を定めることができる、3つ目のクラスタリングアルゴリズムを実行するとしよう。これは赤い車、赤いトラック、灰色の車、灰色のトラックの4つのクラスタにexamplesを割り当てるかもしれない。この新しいクラスタリングは少なくとも両方の属性についての情報を補足するが、類似性についての情報を喪失している。赤い車は灰色の車とは異なるクラスタの中にあり、同様に、灰色のトラックとは異なるクラスタの中にある。このクラスタリングアルゴリズムの出力は赤い車が灰色のトラックより灰色の車に似ていることを教えてくれない。これらは両方のものとも異なる、というのが我々が知るすべてである。
これらの問題は、我々がone-hot表現より分散表現の方を好むかもしれないという理由のいくつかを示す。分散表現は車両ごとに2つの属性を持ち得るだろう。つまり、色を表現するものと車かトラックのどちらであるかを表現するものである。最適な分散表現が何であるか（どうすれば学習アルゴリズムは我々が興味を持つ2つの属性が製造元や年数ではなく色や車対トラックであるかどうかを知ることができるか？）は依然として完全に明確ではないが、多くの属性を持つことは、我々がどの単一の属性を木にしているかを推測するためのアルゴリズムの負担を軽減し、ある属性が一致するかどうかをテストする代わりに多くの属性を比較することによってきめ細かい方法で物体間の類似性を計測する能力を与える。

## 確率的勾配降下法

ほぼすべての深層学習は、確率的勾配降下法[stochastic gradient descent]（SGD）という、ある非常に重要なアルゴリズムによって力を得ている。確率的勾配降下法は[@sec:4.3]で導入された勾配降下法アルゴリズムの拡張である。
機械学習において繰り返し発生する問題は、大きな訓練セットが良好な汎化に対して必須であるが、大きな訓練セットが計算的により高価でもあることである。
機械学習アルゴリズムによって用いられるコスト関数はしばしばいくつかのpre-example損失関数の訓練examples上の総和として分割する。例えば、訓練データの負の条件付き対数尤度は以下のように書くことができる。

$$
J(\boldsymbol{\theta}) = \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} L(\boldsymbol{x}, y, \boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^m L(\boldsymbol{x}^{(i)}, y^{(i)}, \boldsymbol{\theta})
$$

ここで、$L$はpre-example損失$L(\boldsymbol{x}, y, \boldsymbol{\theta}) = - \log p(y | \boldsymbol{x}; \boldsymbol{\theta})$である。
これらの付加的な損失関数に対して、勾配降下法は以下の計算を必要とする。

$$
\nabla_\boldsymbol{\theta} J(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^m \nabla_\boldsymbol{\theta} L(\boldsymbol{x}^{(i)}, y^{(i)}, \boldsymbol{\theta})
$$

この操作の計算コストは$O(m)$である。訓練セットの大きさが何十億個のexamplesに膨れ上がると、単一の勾配ステップを取る時間が途方もなく長くなる。
SGDの洞察は勾配が期待値であることである。期待値はサンプルの小さな集合を用いて近似的に推定できるだろう。具体的には、アルゴリズムの各ステップで、訓練セットから一様に描かれるexamplesのミニバッチ[minibatch]$\mathbb{B} = {\boldsymbol{x}^{(1)}, \dots, \boldsymbol{x}^{(m')}}$をサンプルすることができる。ミニバッチの大きさ$m'$は一般に、1から数百の範囲で、比較的小さいexamples数となるように選ばれる。重要なこととして、$m'$は通常、訓練セットの大きさ$m$が増加するたびに固定されるようになる。100個のexamplesのみで計算された更新値を用いて数十億のexamplesと訓練セットをフィットさせても良い。
その勾配の推定値はミニバッチ$\mathbb{B}$からのexamplesを用いて以下のように形成される。

$$
\boldsymbol{g} = \frac{1}{m'} \nabla_\boldsymbol{\theta} \sum_{i = 0}^{m'} L(\boldsymbol{x}^{(i)}, y^{(i)}, \boldsymbol{\theta})
$$

故に、確率的勾配降下法のアルゴリズムは推定された勾配の下り坂に従う。

$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \epsilon \boldsymbol{g}
$$

ここで、$\epsilon$は学習率である。

一般における勾配降下法はしばしば遅いとか不確実であるとみなされてきた。過去には、非凸最適化問題への勾配降下法のアプリケーションは無謀[foolhardy]とか節操なし[unprincipled]とみなされた。こんにち、我々は[@sec:2]で述べられる機械学習モデルが勾配降下法で訓練されるときに非常にうまく機能することを知っている。この最適化アルゴリズムは妥当な時間で極小にさえも到達すると保証されないかもしれないが、しばしば使える程度に十分速く損失関数の非常に小さな値を見つける。
確率的勾配降下法は深層学習の文脈の外で多くの重要な使い方がある。非常に大きなデータセットで大きな線形モデルを訓練することが主な方法である。固定のモデルサイズに対して、SGD更新あたりのコストは訓練セットの大きさ$m$に依存しない。実践では、我々はしばしば訓練セットの大きさが増えるに従ってより大きなモデルを用いるが、強制ではない。収束に至るために必要な更新の数は通常、訓練セットの大きさとともに増加する。しかし、$m$が無限大に近づくに従って、モデルはやがてSGDが訓練セットのすべてのexampleをサンプルし終わる前に取り得る最良のテスト誤差に収束するだろう。さらに$m$を増やしてもモデルの取り得る最良のテスト誤差に至るのに必要な訓練時間は拡大しない。この視点から、SGDによるモデルの訓練の漸近的コストが$m$の関数と同じ$O(1)$であることを論じることができる。
深層学習が現れる以前では、非線形モデルを学習する主な方法は線形モデルと組み合わせてカーネルトリックを用いることであった。多くのカーネル学習アルゴリズムは$m \times m$行列$G_{i,j} k(\boldsymbol{x}^{(i)}, \boldsymbol{x}^{(j)}$を構築する必要がある。この行列を構築することは、数十億のexamplesを持つデータセットに対して明らかに望ましくない$O(m^2)$の計算コストを持つ。2006年に始まる学会[academia]では、数万のexamplesを持つ中くらいの大きさのデータセットで訓練したときに競合のアルゴリズムより上手に新しいexamplesに汎化できたので、深層学習は当初興味深かった。直後、深層学習は、大きなデータセットで非線形モデルを訓練するスケーラブルな方法をもたらしたので、業界[industry]でさらなる関心を集めた。
確率的勾配降下法およびそれに対する多くの改良は[@sec:8]で更に述べられる。

## 機械学習アルゴリズムの作成

ほぼすべての深層学習アルゴリズムは、データセット、コスト関数、最適化手続、モデルの仕様を組み合わせる、というかなり単純なレシピの特定の例として記述できる。
例えば、線形回帰アルゴリズムは$\boldsymbol{X}$と$\boldsymbol{y}$から成るデータセット、以下の損失関数、モデルの仕様$p_{model}(y | \boldsymbol{x}) = \mathcal{N}(y; \boldsymbol{x}^\top \boldsymbol{w} + b, 1)$、および、多くの場合、正規方程式を用いてコストの勾配がゼロとなるところに対して解くことによって定義される最適化アルゴリズムを組み合わせる。

$$
J(\boldsymbol{w}, b) = -\mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \log p_{model}(y | \boldsymbol{x})
$$

その他からほとんど独立してこれらの構成要素のいずれかを置き換えることができると気づくことによって、我々は幅の広いアルゴリズムを得ることができる。
コスト関数は一般に学習プロセスに統計的な推定を行わせる少なくとも1つの項を含む。最も一般的なコスト関数は負の対数尤度である。そのため、コスト関数を最小化することは最尤推定を引き起こす。
コスト関数は、正則化項のような、追加の項も含むかもしれない。例えば、我々は以下を得るために線形回帰のコスト関数へweight decayを追加することができる。

$$
J(\boldsymbol{w}, b) = \lambda \| \boldsymbol{w} \|_2^2 - \mathbb{E}_{\boldsymbol{\mathbf{x}}, \mathbf{y} \sim \hat{p}_{data}} \log p_{model}(y | \boldsymbol{x})
$$

これは依然として閉形式の最適化を可能にする。
モデルを非線形とするならば、ほとんどのコスト関数は閉形式で最適化できない。これは、勾配降下法のような、反復的な数値最適化手順を選択する必要がある。
モデル、コスト、最適化アルゴリズムを組み合わせることで学習アルゴリズムを構築するためのレシピは教師あり学習と教師なし学習の両方をサポートする。線形回帰の例は教師あり学習をサポートする方法を示す。教師なし学習は$\boldsymbol{X}$のみを含むデータセットを定義し、適切な教師なしコストおよびモデルをもたらすことでサポートさせることができる。例えば、我々の損失関数が以下であると指定することで第一PCAベクトルを得ることができる。

$$
J(\boldsymbol{w}) = \mathbb{E}_{\boldsymbol{\mathbf{x}} \sim \hat{p}_{data}} \| \boldsymbol{x} - r(\boldsymbol{x}; \boldsymbol{w}) \|_2^2
$$

一方、我々のモデルは1のノルムを持つ$\boldsymbol{w}$と再構築関数$r(\boldsymbol{x}) = \boldsymbol{w}^\top \boldsymbol{x} \boldsymbol{w}$を持つと定義される。

いくつかの場合、コスト関数は、計算上の理由により、実際には計算できない関数であるかもしれない。これらの場合、我々は依然として、その勾配を近似する方法を持つ限り、反復的な数値最適化を用いてこれを近似的に最小化できる。
ほとんどの機械学習アルゴリズムは、一見して明らかでないかもしれないにも関わらず、このレシピを活用する。機械学習アルゴリズムが特別ユニークである、または、手作業で設計されたように見えるならば、通常、特殊な場合のoptimizerを用いていると理解することができる。決定木や$k$平均方のような、いくつかのモデルは、これらのコスト関数が勾配ベースのオプティマイザによる最小化に対して適切でなくなる平坦な領域を持つので、特殊な場合のオプティマイザを必要とする。ほとんどの機械学習アルゴリズムがこのレシピを用いて記述できると認識することは、それぞれが別個の正当化を持つアルゴリズムの長いリストとしてではなく、似た理由により機能する関連するタスクを行うための手法の分類法の一部として様々なアルゴリズムを理解するのに役立つ。

## 深層学習の動機付けの課題

この章で述べられる単純な機械学習アルゴリズムは多種多様な重要な問題に関してうまく機能する。しかし、これらは、音声認識や物体認識のようなAIにおける中心的な問題を解くことに成功していない。
深層学習の開発はそのようなAIタスクに関してうまく一般化するための伝統的なアルゴリズムの失敗によってある程度動機付けられた。
この節は高次元データで動作するときに新しいexamplesへの一般化の課題がどれだけ指数関数的にさらに難しくなるか、および、伝統的な機械学習において一般化を達成するために使われるメカニズムが高次元空間において複雑な関数を学習するのにどれだけ不十分かについてである。そのような空間はしばしば高い計算コストも課す。深層学習はこれらやその他の障害を克服するために設計された。

### 次元の呪い

多くの機械学習問題はデータにおける次元数が高いときに非常に難しくなる。この現象は次元の呪い[curse of dimensionality]として知られる。特に懸念されることは、変数の数が増加するにつれて、変数の集合の取り得る別個のconfigurationsの数が指数関数的に増加することである。
次元の呪いは計算機科学において、特に機械学習において、多くの場所で現れる。
次元の呪いによってもたらされる1つの課題は統計的な課題である。[@fig:5.9]に図示される通り、統計的な課題は、$\boldsymbol{x}$の取り得るconfigurationsの数が訓練examplesの数より多いことを理由に発生する。この問題を理解するため、図にあるように、入力空間がグリッド状に構成されると考えるとする。我々は主にデータによって占められる少数のグリッドセルで低次元空間を記述できる。新しいデータポイントに一般化するとき、我々は通常、その新しい入力と同じセル内にある訓練examplesを詳しく調べることによって単純にすべきことを伝えることができる。例えば、ある点$\boldsymbol{x}$で確率密度を推定するならば、訓練examplesの総数で割った、$\boldsymbol{x}$と同じ単位ボリュームセルにおける訓練examples数を返すことができる。exampleを分類することを欲するならば、同じセルにおける最も共通する訓練examplesのクラスを返すことができる。回帰を行っているならば、そのセルにおけるexamples全体で観測される目的値を平均できる。しかし、exampleがないセルはどうだろうか？高次元空間では、configurations数が巨大で、我々のexmaples数より大きいので、一般的なグリッドセルはそれに関連付けられた訓練exampleを持たない。一体どうしてこれらの新しいconfigurationsについて有意義な何かを言えたのか？多くの伝統的な機械学習アルゴリズムは新しい点の出力が最も近い訓練点の出力と近似的に同じであるはずであることを単純に仮定する。

### 局所的な恒常性と平滑さの正則化

上手に一般化するため、機械学習アルゴリズムはこれらがどんな種類の関数を学習すべきかについての事前確率の信念によって導かれる必要がある。我々はモデルのパラメータ上の確率分布の形式における明示的な信念として組み込まれたこれらの事前確率を見てきた。より噛み砕いて言えば、パラメータと関数の関係の結果として、関数それ自体に直接影響を与え、パラメータには間接的にのみ影響を与えると事前確率の信念を考察できるだろう。加えて、これらのバイアスが様々な関数において信念の度合いを表現する確率関数の観点で説明されない（または、説明することすらできない）かもしれないとしても、他のクラスより関数のあるクラスを選択する方にバイアスをかけるアルゴリズムを選択することで暗黙的に説明されると事前確率の信念を非公式的に考察する。
これらの暗黙的な「事前確率」の中で最も広く使われるものは***

