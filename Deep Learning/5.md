# 機械学習の基礎

深層学習は機械学習の特定の種類である。深層学習を良く理解するためには、機械学習の基本原則をしっかりと理解しなければならない。この章は以降の本書全体を通して適用される最重要の一般的な原則についての簡潔な授業を提供する。新参の読者や視野を広くしたい方は、[@Murphy2012]や[@Bishop2006]のような、基礎部分をより包括的にカバーする機械学習のテキストを検討することをオススメする。あなたがすでに機械学習の基礎に親しんでいるならば、[@sec:5.11]まで飛ばしてもらって構わない。この節は深層学習アルゴリズムの発展に強く影響を与えてきた伝統的な機械学習技術に対するいくつかの視点をカバーする。

学習アルゴリズムとは何かを定義するところから始めて、線形回帰アルゴリズムを使って一例を示そう。そこで次に、訓練データを一致させる課題が新しいデータに一般化するパターンを見つける課題とどれだけ異なるかを記述することにする。ほとんどの機械学習アルゴリズムはハイパーパラメータ[hyperparameters]と呼ばれる設定を持つ。これは、学習アルゴリズム自体の外で決定されなければならない。つまり、我々は追加データを用いてこれらを設定する方法を議論する。機械学習は実質的に、複雑な関数を統計的に推定するためにコンピュータを使用することを重んじ、これらの関数の周りの信頼区間を証明することを軽んじる、応用的な統計学の一形式である。従って、我々は、頻度論的推定量[frequentist estimators]とベイズ推定[Bayesian inference]という、2つの統計学への中心的アプローチを示す。ほとんどの機械学習アルゴリズムは、教師あり学習と教師なし学習のカテゴリに分けられる。そこで、我々はこれらのカテゴリを説明し、各カテゴリに由来するいくつかの単純な学習アルゴリズムの例を与える。ほとんどの深層学習アルゴリズムは確率的勾配降下法とよばれる最適化アルゴリズムに基づいている。我々は、機械学習アルゴリズムを作るために、最適化アルゴリズム、コスト関数、モデル、データセットといった、様々なアルゴリズムの構成要素を組み合わせる方法を述べる。最後に、[@sec:5.11]では、一般化するために伝統的な機械学習の能力を制限していた要因のいくつかを述べる。これらの課題は、これらの障害を克服する深層学習アルゴリズムの開発を動機付けてきた。

## 学習アルゴリズム

機械学習アルゴリズムとはデータから学習することができるアルゴリズムである。しかし、学習とは何を意味するのだろうか？ @Mitchell1997 は、「ある種別のタスク$T$およびパフォーマンス尺度$P$に関する経験$E$によって、$T$におけるタスクでの、$P$によって測定されるパフォーマンスが改善する場合、コンピュータプログラムは$E$から学習すると言う。」という、簡潔な定義を与えている。経験$E$、タスク$T$、パフォーマンス尺度$P$は多種多様なものが想像できるが、我々は本書においてこれらのエンティティのそれぞれに使われるかもしれないものを形式的に定義しようとしたりはしない。代わりに、次節にて、直観的な説明と機械学習アルゴリズムを構築するのに使える様々な種類のタスク、パフォーマンス尺度、経験の例を提供する。

### タスク$T$

機械学習は人間が記述および設計した固定のプログラムで解くには難しすぎるタスクに対処することが可能となる。科学的および哲学的視点から見ると、その理解を深めることは知性の基となる原則の理解を深めることを必然的に伴うので、機械学習は興味深い。
この「タスク」という語の比較的に形式的な定義において、学習の工程それ自体はタスクではない。学習はタスクを処理するための能力を獲得する方法である。例えば、ロボットを歩けるようにしたい場合、歩くことはタスクである。我々は、歩き方を学習するようにロボットをプログラミングしたり、手動で歩き方を指定するプログラムを直接書いてみようとしたりすることができる。
機械学習のタスクは通常、機械学習システムがどのようなexampleを処理すべきかに関して記述される。exampleは機械学習システムに処理させたい物体または事象から定量的に計測された特徴[features]の集合である。我々は一般にexampleを各成分$x_i$が別の特徴であるようなベクトル$\boldsymbol{x} \in \mathbb{R}^n$と表現する。例えば、画像の特徴は通常、その画像の中のピクセルの値である。
多くの種類のタスクは機械学習によって解くことができる。最も一般的な機械学習タスクのいくつかには以下が含まれる。

- 分類[classification]：この種のタスクでは、コンピュータプログラムはある入力が$k$個のカテゴリのいずれに属すかを特定することを求められる。このタスクを解くために、学習アルゴリズムは通常、関数$f : \mathbb{R}^n \rightarrow {1, \dots, k}$を生み出すことを求められる。$y = f(\boldsymbol{x})$であるとき、モデルはベクトル$\boldsymbol{x}$で記述される入力を数値コード$y$で識別されるカテゴリに割り当てる。他にも、例えば、$f$が種別の確率分布を出力するような、様々な分類法が存在する。分類タスクの一例として、物体認識がある。これは、（通常ではピクセルの明るさの値の集合として記述される）画像を入力とし、画像中の物体を識別する数値コードを出力とする。例えば、Willow GarageのPR2というロボットは異なる種類の飲み物を認識し、命令によって人々に届けることができるウェイターととして振る舞うことができる[@Goodfellow2010]。近年の物体認識は深層学習によるものが最も洗練されている[@Krizhevsky2012; @Ioffe2015]。物体認識はコンピュータが顔を認識することを可能にする技術[@Taigman2014]と基本は同じである。これは、写真アルバムの中の人たちを自動的にタグ付けしたり、コンピュータがユーザとより自然に対話したりするために使うことができる。
- 欠けている入力を伴う分類[classification with missing inputs]：コンピュータ・プログラムがその入力ベクトルにすべての測定値を常に与えることを保証しない場合、分類はより挑戦的になる。分類タスクを解くために、学習アルゴリズムはベクトルの入力からカテゴリの出力へマッピングする単一の関数を定義しなければならない。入力のいくつかが欠けているとき、学習アルゴリズムは、単一の分類関数を与えるのではなく、関数の集合を学習しなければならない。各関数はその欠けている入力の異なる部分集合を持つ$\boldsymbol{x}$を分類することに対応する。この種の状況は医療診断において、多くの種の医療検査は高価であり侵襲的[invasive]であるので、頻繁に発生する。そのような関数の大きな集合を効率的に定義する方法のひとつはすべての関連する変数上の確率分布を学習し、欠けている変数を周辺化すること[marginalizing out]で分類タスクを解くことである。$n$個の入力変数では、欠けている入力の取り得る集合ごとに必要となる$2^n$個の異なる分類関数すべてを得られるが、コンピュータプログラムは同時確率分布を記述する単一の関数のみを学習する必要がある。この方法でそのようなタスクに適用される深層確率モデルの例は[@Goodfellow2013b]を参照のこと。この節で述べられるその他のタスクの多くは欠けている入力とともに機能するように一般化することもできる。つまり、欠けている入力を伴う分類は機械学習ができることの単なる一例である。
- 認識[regression]：この種類のタスクでは、コンピュータプログラムはある入力が与えられた場合に数値を予測することを求められる。このタスクを解くために、学習アルゴリズムは関数$f : \mathbb{R}^n \rightarrow \mathbb{R}$を出力することを求められる。この種類のタスクは、出力の形式が異なることを除いて、分類と似ている。認識タスクの例は、被保険者になされるであろう保険金支払額の期待値[expected claim amount]の予測（保険料[insurance premiums]を設定するために使われる）、または、有価証券[securities]の将来的な価格の予測である。
- 文字起こし[transcription]：この種類のタスクでは、機械学習システムはある種のデータの比較的に非構造的な表現を調べて、離散的な文章形式に情報を書き起こすことを求められる。例えば、光学的な文字認識では、コンピュータプログラムは文章の画像を含む写真を見せられ、文字列の形式（例えば、ASCIIかUnicode形式）でこの文章を返すことを求められる。Googleストリートビューはこの方法で郵便番号を処理するために深層学習を用いている[@Goodfellow2014d]。もうひとつの例は音声認識[speech recognition]である。そこでは、コンピュータプログラムは音声波形を与えられ、録音の中で話された言葉を記述する文字列か単語IDを吐き出す。深層学習は、Microsoft、IBM、Googleを含む主要な企業で用いられる近代的な音声認識システムの極めて重要な要素である[@Hinton2012b]。
- 機械翻訳[machine translation]：機械翻訳タスクでは、入力はある言語における記号の列から予め構成され、コンピュータプログラムはこれを別の言語における記号の列に変換しなければならない。これは一般的に、英語からフランス語への翻訳のように、自然言語へ応用される。深層学習は最近になってこの種のタスクに重要な影響をもたらし始めている[@Sutskever2014; @Bahdanau2015]。
- 構造化された出力[structured output]：構造化された出力のタスクは出力が異なる要素間の重要な関係を持つベクトル（または、複数の値から成る他のデータ構造）である任意のタスクである。これは広範囲のカテゴリであり、他の多くのタスクと同様に、上記で述べられる書き起こしや翻訳のタスクを包含する[subsume]。一例として、構文解析[parsing]がある。これは、動詞、名詞、副詞などとして木の節をタグ付けすることでその文法構造を記述する木に自然言語の文をマッピングすることである。構文解析タスクに適用される深層学習の例は[@Collobert2011]を参照のこと。もうひとつの例は画像のピクセル単位の領域分割である。ここでは、コンピュータプログラムは画像中のすべてのピクセルに特定のカテゴリを割り当てる。例えば、深層学習は空撮写真で道路の位置に注釈を付けるのに使うことができる[@Mnih2010]。出力形式はこれらの注釈スタイルのタスクと同じくらいに入力の構造を鏡写しにする必要はない。例えば、画像の表題付け[image captioning]では、コンピュータプログラムは画像を調べ、画像を説明する自然言語の文を出力する[@Kiros2014a; @Kiros2014b; @Mao2015; @Vinyals2015b; @Donahue2014; @Karpathy2015; @Fang2015; @Xu2015]。これらのタスクは、プログラムがすべてで強固な相互関連を持ついくつかの値を出力しなければならないので、構造化出力タスク[structured output tasks]と呼ばれる。例えば、image captioningプログラムで生成される単語は正しい文を形作らなければならない。
- 異常検知[anomaly detection]：この種のタスクでは、コンピュータプログラムは事象または物体の集合を選別し、普通ではない[unusual]または非定型[atypical]であるとしてこれらの一部をフラグを立てる。異常検知タスクの例はクレジットカード詐欺[fraud]の検知である。買い物の習慣をモデル化することで、クレジットカード会社はカードの不正使用を検知できる。泥棒があなたのクレジットカードやクレジットカード情報を盗む場合、泥棒の買い物はしばしばあなた自身とは異なる購買タイプ上の確率分布を生じさせることがあるだろう。クレジットカード会社は、そのカードが特徴的ではない買い物に使われたらすぐに取引を一時停止することで詐欺を防ぐことができる。異常検知の手法の調査については[@Chandola2009]を参照のこと。
- 合成と標本化[synthesis and sampling]：この種のタスクでは、機械学習アルゴリズムは訓練データにあるものと似ている新しいexamplesを生成することを求められる。機械学習を介する合成および標本化は、大量のコンテンツを手作業で作るのが高価だったり、退屈だったり、時間がかかりすぎたりするであろうときのメディアアプリケーションに対して役立つ可能性がある。例えば、ビデオゲームは大きな物体や地形に対するテクスチャを、アーティストに各ピクセルを手作業でラベル付けしてもらうのではなく、自動的に生成できる[@Luo2013]。いくつかの場合、我々は標本化や合成のプロシージャが入力を与えられたときに特定の種類の出力を生成してほしいと考える。例えば、音声生成タスクでは、文字の形の文[written sentence]を与え、プログラムにその文の声のバージョン[spoken version]を含む音声波形を吐くよう求める。これは構造化出力タスクの一種であるが、各入力に対するたった1つの正解の出力が存在しないような定量化が追加されている。そして、我々は、出力がより自然で現実味を帯びているように見せるために、出力において大量のバラツキを明示的に望む。
- 欠落値の補完[imputation of missing values]：この種のタスクでは、機械学習アルゴリズムは新しいexample$\boldsymbol{x} \in \mathbb{R}^n$を与えられるが、$\boldsymbol{x}$のいくつかの成分$x_i$が欠落している。アルゴリズムは欠けている成分の値の推定をもたらす必要がある。
- ノイズ除去[denoising]：この種のタスクでは、機械学習アルゴリズムは不明の破損プロセスによってキレイなexample$\boldsymbol{x} \in \mathbb{R}^n$から破損したexample$\boldsymbol{x}^\tilde \in \mathbb{R}^n$を入力として与えられる。学習者はキレイなexample$\boldsymbol{x}$をその破損したバージョン$\boldsymbol{x}^\tilde$から予測する、または、より一般的には条件付き確率分布$p(\boldsymbol{x} | \boldsymbol{x}^\tilde)$を予測する必要がある。
- 密度推定[density estimation]または確率質量関数の推定[probability mass function estimation]：密度推定問題では、機械学習アルゴリズムは関数$p_{model} : \mathbb{R}^n \rightarrow \mathbb{R}$を学習することを求められる。ここで、$p_{model}(\boldsymbol{x})$はexamplesが描かれる空間上の確率密度関数（$\boldsymbol{\mathbf{x}}$が連続的である場合）または確率質量関数（$\boldsymbol{\mathbf{x}}$が離散的である場合）として解釈できる。そのようなタスクをうまくこなすために（パフォーマンス尺度$P$を述べるときにそれが意味するものを厳密に示そうと思う）、アルゴリズムはそこに示されるデータの構造を学習する必要がある。examplesが密集している所や発生する可能性が低い所を知っていなければならない。前述されたほとんどのタスクは学習アルゴリズムが少なくとも確率分布の構造を暗黙のうちにキャプチャする必要がある。密度推定はその分布を明示的にキャプチャすることを可能にする。原則として、我々は同様に他のタスクを解くためにその分布上の計算を行うことができる。例えば、確率分布$p(\boldsymbol{x})$を得るために確率推定を行ったとすれば、欠落値補完タスクを解くためにその分布を用いることができる。値$x_i$が欠けていて、$\mathbb{x}_{-i}$で表記されるその他すべての値が与えられている場合、それ上の分布が$p(x_i | \boldsymbol{x}_{-i})$によって与えられることを知っている。実践では、多くの場合で$p(\boldsymbol{x})$で必要な処理が計算上扱いづらいので、密度推定はこれらすべての関連するタスクを解くことを常に可能としない。

もちろん、多くのその他のタスクやタスクの種類が取り得る。我々がここに並べたタスクの種類は、タスクの厳密な分類体系を定義するためではなく、機械学習ができることの例を提供するためのみを意図している。

### パフォーマンス尺度$P$

機械学習アルゴリズムの能力を評価するため、我々はそのパフォーマンスの定量的な尺度を設計しなければならない。通常、このパフォーマンス尺度$P$はシステムによってもたらされるタスク$T$に特有なものである。
分類、欠けている入力を伴う分類、文字起こしのようなタスクに対しては、しばしばモデルの正確さ[accuracy]を計測する。正確さはモデルが正しい出力を生み出しているexamplesの単なる割合である。モデルが正しくない出力を生み出しているexamplesの割合である誤り率[error rate]を計測することで同等の情報も得られる。我々はしばしば誤り率を0-1 lossの期待値として参照する。特定のexampleでの0-1 lossは正しく分類されれば$0$となり、そうでなければ$1$となる。密度推定のようなタスクでは、正確さ、誤り率、その他いずれの種類の0-1 lossを測定することも意味をなさない。代わりに、exampleごとに連続値のスコアをモデルに与える別のパフォーマンスの測定基準を用いなければならない。最も一般的なアプローチは、モデルがいくつかの
examplesに割り当てる平均多数確率を報告することである。
通常、それが現実世界に展開させたときにどれだけうまく機能するだろうかを決定するので、我々は機械学習アルゴリズムがそれまでに見たことのないデータをどれだけうまく処理するかについて興味がある。故に、我々は機械学習システムを訓練するために使われるデータとは別のテスト用データセット[test set of data]を用いてこれらのパフォーマンス尺度を評価する。
パフォーマンス尺度の選択は率直で客観的であるように見えるかもしれないが、システムの望ましい振る舞いにうまく対応するパフォーマンス尺度を選ぶことはしばしば難しい。
いくつかの場合、これは計測すべきものを決めることが難しいためである。例えば、文字起こしタスクを処理するとき、文字列全体を書き起こすという点でシステムの正確さを計測すべきだろうか、または、いくつかの文字列の要素が正解しているときに部分点を与えるようなよりきめ細かいパフォーマンス尺度を用いるべきだろうか？認識タスクを処理するとき、頻繁に中くらいのミスをする場合、または、まれに非常に大きなミスをする場合であれば、システムをより減点すべきだろうか？これらの種類の設計上の選択はアプリケーションに依存する。
その他の場合、我々は理想的には計測したい量を知っているが、それを測定することは実用的ではない。例えば、これは密度推定の文脈において頻繁に発生する。最適な確率モデルの多くは暗黙的にのみ確率分布を表現する。多くのそのようなモデルでは空間の特定の点に割り当てられる実際の確率の値を計算することは解決困難である。これらの場合、設計目的に依然として対応する代替基準を設計するか、望ましい基準に対する良好な近似を設計するか、をしなければならない。

### 経験$E$

機械学習アルゴリズムは、学習プロセス中に持ち得る経験の種類によって、教師なし[unsupervised]と教師あり[supervised]に大別することができる。
本書における学習アルゴリズムのほとんどはデータセット[dataset]全体を経験することができると理解できる。データセットは、[@sec:5.1.1]で定義されるように、多数のexamplesの集合である。時折、我々はexamplesをデータポイント[data points]と呼ぶ。
統計学者や機械学習の研究者によって研究される最古のデータセットひとつはIris datasetである[@Fisher1936]。これは150本のアヤメ属の花の様々な部分の計測値の集合である。それぞれの個々の花は1つのexampleに対応する。各exampleにある特徴は花の各部分の計測値である。つまり、萼片[sepal]の長さ、萼片の幅、花弁[petal]の長さ、花弁の幅である。そのデータセットはそれぞれの花がどの種に属しているかも記録している。そのデータセットには3つの異なる種が示されている。
教師なし学習アルゴリズム[unsupervised learning algorithms]は多数の特徴を含むデータセットを経験し、このデータセットの構造の有用な特性を学習する。深層学習の文脈において、我々は通常、密度推定にあるように明示的に、または、合成やノイズ除去のようなタスクのように暗黙的に、データセットを生成する確率分布全体を学習したい。いくつかのその他の教師なし学習アルゴリズムは、似たexamplesのクラスタにデータセットを分けることから成るクラスタリングのような、他の役割をこなす。
教師あり学習アルゴリズム[supervised learning algorithms]は特徴を含むデータセットを経験するが、各exampleはラベル[label]または対象[target]に関連付けられてもいる。例えば、Iris datasetはそれぞれのアヤメの花の種類で注釈が付けられている。教師あり学習アルゴリズムはIris datasetを学んだり、これらの計測値に基づいてアヤメの花を3つの異なる種類に分類することを学習したりすることができる。
大雑把に言えば、教師なし学習は無作為なベクトル$\boldsymbol{\mathbf{x}}$のいくつかのexamplesを観察し、確率分布$p(\boldsymbol{\mathbf{x}})$、または、その分布のいくつかの興味深い特性を暗黙的または明示的に学習しようと試みることを伴う。一方で、教師あり学習は無作為なベクトル$\boldsymbol{\mathbf{x}}$や関連する値またはベクトル$\boldsymbol{\mathbf{y}}$のいくつかのexamplesを観察し、$\boldsymbol{\mathbf{x}}$から$\boldsymbol{\mathbf{y}}$を、通常では$p(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$を推定することによって、予測することを学習することを伴う。教師あり学習という用語は機械学習システムにすべきことを示す講師や教師によってもたらされる対象$\boldsymbol{\mathbf{y}}$の視点に由来する。教師なし学習では、講師や教師がおらず、アルゴリズムはこの導きなしにデータを理解するために学習しなければならない。
教師なし学習と教師あり学習は公式に定義された用語ではない。これらの線引きはしばしば曖昧である。多くの機械学習技術は両方のタスクを処理するために使うことができる。例えば、確率の連鎖律は、ベクトル$\boldsymbol{\mathbf{x}} \in \mathbb{R}^n$に対して、同時分布が以下のように分解できることを示している。

$$
p(\boldsymbol{\mathbf{x}}) = \prod_{i = 1}^n p(\mathbf{x}_i | \mathbf{x}_1, \dots, \mathbf{x}_{i-1})
$$

この分解は、モデル化$p(\boldsymbol{\mathbf{x}})$の表面上は教師なしである問題を$n$個の教師あり学習の問題に分けることで解くことができることを意味する。あるいは、学習$p(y | \boldsymbol{\mathbf{x}})$の教師あり学習の問題を同時分布$p(\boldsymbol{\mathbf{x}}, y)$を学習するための伝統的な教師なし学習技術を用いて以下を推論することで解くことができる。

$$
p(y | \boldsymbol{\mathbf{x}}) = \frac{p(\boldsymbol{\mathbf{x}}, y)}{\sum_{y'} p(\boldsymbol{\mathbf{x}}, y')}
$$

教師なし学習と教師あり学習は完全に形式的で明確な概念ではないにも関わらず、これらは機械学習アルゴリズムで行うことのいくつかをおおまかに分類するのに役立っている。慣例として、人々は認識、分類、構造化出力の問題を教師あり学習と呼ぶ。他のタスクの裏付けとしての密度推定は通常、教師なし学習とみなされる。
他の種類の学習パラダイムが取り得る。例えば、半教師あり学習では、いくつかのexamplesはsupervision targetを含むが、それ以外は含まない。multi-instance学習では、examplesの集合全体はある種別のexampleを含むかどうかでラベル付けされるが、集合の個々のメンバーはラベル付けされない。深層モデルを伴うmulti-instance学習の最近の例は[@Kotzias2015]を参照のこと。
いくつかの機械学習アルゴリズムは単に固定のデータセットを経験するだけではない。例えば、強化学習[reinforcement learning]アルゴリズムは環境と相互作用する。つまり、学習アルゴリズムとその経験の間にフィードバックループが存在する。そのようなアルゴリズムは本書の範疇を超える。強化学習に関する情報は@Sutton1998か@Bertsekas1996を、強化学習への深層学習のアプローチは@Mnih2013を参照ください。
ほとんどの機械学習アルゴリズムは単純にデータセットを経験する。データセットはいろんな方法で記述できる。すべての場合で、データセットはexamplesの集合である。これは、特徴の集合でもある。
データセットを記述する一般的な方法のひとつは***
