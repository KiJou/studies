# 深層学習のための正則化

機械学習における中心的問題はどのようにして訓練データだけでなく新しい入力に関してもうまく働くであろうアルゴリズムを作るかである。機械学習で使われる多くの戦略は、おそらく訓練誤差の増加という犠牲を払って、テスト誤差を減らすために明示的に設計されている。これらの戦略はまとめて正則化として知られている。深層学習の実践者は非常に多くの形式の正則化を利用できる。実際、より効果的な正則化戦略の開発はこの分野における主要な研究成果のひとつとなっている。
[@sec:5]では汎化、アンダーフィッティング、オーバーフィッティング、バイアス、分散、正則化の基本概念を紹介した。あなたが未だこれらの概念に親しんでいないならば、本章を読み進める前にその章を参照してほしい。
本章では、深層モデルや深層モデルを形成するための構成部品として用いられるかもしれないモデルに焦点を当てて、より詳細に正則化を述べる。
本章のいくつかの節は機械学習における標準的な概念を扱う。あなたが既にこれらの概念に親しんでいるならば、関連する節を読み飛ばしてもらって一向に構わない。しかし、本章のほとんどはニューラルネットワークの特定のケースへのこれらの基本概念の拡張と関係している。
[@sec:5.2.2]では、我々は「訓練誤差ではなく汎化誤差を減らすよう意図された学習アルゴリズムに施すいずれかの修正」として正則化を定義した。正則化戦略は多数存在する。あるものは、パラメータの値に制限を加えるような、追加の制約を機械学習モデルにもたらす。あるものはパラメータの値のsoft constraintに対応すると考えることができる追加の項を目的関数に加える。慎重に選ばれれば、これらの追加の制約や罰則はテストセットにおいて改善されたパフォーマンスを引き起こし得る。ある時には、これらの制約や罰則は特定の種類の事前知識をエンコードするために設計される。またある時には、これらの制約や罰則は汎化を促進させるためにより単純なモデルのクラスに対する一般的な選好を表現するために設計される。時折、罰則や制約はunderdetermined problemをdeterminedにするのに必要である。アンサンブル法として知られる他の形式の正則化は訓練データを説明する複数の仮説を組み合わせる。
深層学習の文脈では、ほとんどの正則化戦略は推定器を正則化することに基づいている。推定器の正則化はバイアスの増加を分散の低減とトレードすることで行われる。効果的なregularizerは、分散を大幅に低減しつつバイアスを過度に増やさない、有利なトレード[profitable trade]を行うものである。[@sec:5]で汎化とオーバーフィッティングを考察したとき、我々は3つの状況に焦点を当てた。これは、訓練されるモデル族が（１）真のデータ生成過程を除外した --- アンダーフィットすることとバイアスを含めることに対応する --- か、（２）真のデータ生成過程に一致したか、（３）生成過程だけでなく他の多くの取り得る生成過程を含めた --- バイアスより分散の方が推定値の誤差に占める割合が大きいオーバーフィッティング状態である --- かのいずれかである。正則化の目標はモデルを３番目の状態から２番目の状態にすることである。
実践では、過度に複雑なモデル族は対象の関数や真のデータ生成過程、または、どちらかの近しい近似であっても必ず含むというわけではない。我々は真のデータ生成過程にアクセスすることはほぼできず、そのために、推定されるモデル族がその生成過程を含むかどうかを確信できることは一切ない。しかしながら、深層学習アルゴリズムのほとんどのアプリケーションは真のデータ生成過程がほぼ確実にモデル族の外側にある領域にある。深層学習アルゴリズムは一般に、真の生成過程が本質的に宇宙全体のシミュレーションを伴う、画像、音声シーケンス、文章のような極めて複雑な領域に適用される。ある程度までは、我々は常に四角いペグ（データ生成過程）を丸い穴（我々のモデル族）に合わせようと試みている。
これが意味することは、モデルの複雑さを制御することが正しいパラメータ数を持つ正しい大きさのモデルを見つけるという単純な問題ではない、ということである。代わりに、我々は（汎化誤差を最小化するという意味において）最適にフィットするモデルが適切に正則化された大きなモデルであることをおそらく見つけ --- そして実際に、実践的な深層学習のシナリオにおいて、ほとんどいつも見つけ --- るだろう。
我々はそのような大きく深い正則化されたモデルを生成する方法に対するいくつかの戦略を再調査する。

## パラメータのノルムの罰則

正則化は深層学習の出現に先立って数十年の間使われてきた。線形回帰やロジスティック回帰のような線形モデルは単純で素直で効果的な正則化戦略を可能にする。多くの正則化アプローチは、目的関数$J$にパラメータノルムの罰則$\Omega(\boldsymbol{\theta})$を加えることによって、ニューラルネットワーク、線形回帰、ロジスティック回帰のようなモデルの能力を制限することに基づいている。我々は正則化された目的関数を$\tilde{J}$で表記する。

$$
\tilde{J}(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) = J(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) + \alpha \Omega(\boldsymbol{\theta})
$$

ここで、$\alpha \in [0, \infty)$は、標準の目的関数$J$に関連して、ノルムの罰則項$\Omega$の相対的寄与を重み付けするハイパーパラメータである。$\alpha$を$0$に設定することは正則化しないということになる。より大きな$\alpha$の値はより大きな正則化に対応する。
我々の訓練アルゴリズムが正規化された目的関数$\tilde{J}$を最小化するとき、訓練データに関する元の目的$J$とパラメータ$\boldsymbol{\theta}$の大きさのある測度（または、パラメータのある部分集合）の両方を減らすだろう。パラメータノルム$\Omega$に対する選択が異なれば、結果として優先される解は異なり得る。本節では、我々はモデルのパラメータに関する罰則として使われるときの様々なノルムの効果を考察する。
異なるノルムの正則化の振る舞いを掘り下げる前に、我々は、ニューラルネットワークに対して、一般に各層でアフィン変換の重みだけに罰則を課し、バイアスを非正則化のままにするパラメータノルムの罰則$\Omega$を用いることを選択することに留意する。バイアスは一般に正確にフィットさせるのに重みほど多くのデータを必要としない。各重みは2つの変数がどのように相互作用するかを明示する。重みをうまくフィットさせることは多種多様な状況で両方の変数を観察することを必要とする。各バイアスは単一の変数のみを制御する。これはバイアスを非正則化のままとすることによってそこまでの分散を引き起こさないことを意味する。また、バイアスパラメータを正則化することは大幅なアンダーフィッティングをもたらし得る。従って、我々はノルムの罰則に影響を受けるべきすべての重みを示すためにベクトル$\boldsymbol{w}$を用い、その一方で、ベクトル$\boldsymbol{\theta}$は、$\boldsymbol{w}$と非正則化パラメータを含めた、すべてのパラメータを表記する。
ニューラルネットワークの文脈では、時折、ネットワークの層ごとに異なる$\alpha$係数を持つ別個の罰則を用いることが望ましい。複数のハイパーパラメータの正しい値を探すのは効果となり得るので、探索空間の大きさを減らすためだけにすべての層で同じweight decayを用いることは依然として合理的である。

### $L^2$パラメータ正則化

我々は既に、[@sec:5.2.2]において、最も単純で最も一般的な種類のパラメータノルムの罰則を見てきた。すなわち、weight decayとして一般的に知られる$L^2$パラメータノルム罰則である。この正則化戦略は正則化項$\Omega(\boldsymbol{\theta}) = \fac{1}{2} \| \boldsymbol{w} \|_2^2$を目的関数に加えることで重みを原点[^1]に近づける。他の学術コミュニティでは、$L^2$正則化はリッジ回帰[ridge regression]とかTikhonovの正則化としても知られる。

[^1]: より一般的に言えば、我々は空間のいずれかの特定の点の近くになるようパラメータを正規化し、驚くことに、依然として正則化の効果を得ることできるだろうが、より良い結果は、正しい値が正または負であるべきかを知らないときに理にかなう既定値であるゼロを伴う、真の点により近い値に対して得られるだろう。ゼロに向かってモデルパラメータを正則化することはかなり一般的であるので、我々は説明の中でこの特殊なケースに焦点を当てるだろう。

我々は正則化された目的関数の勾配を研究することでweight decayの正則化の振る舞いへの洞察を得られる。その提示を単純化するため、我々はバイアスパラメータがない、すなわち、$\boldsymbol{\theta}$が単に$\boldsymbol{w}$であると仮定する。そのようなモデルは以下の総目的関数を持つ。

$$
\tilde{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y}) = \frac{\alpha}{2} \boldsymbol{w}^\top \boldsymbol{w} + J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})
$$

これは以下のような対応するパラメータの勾配を持つ。

$$
\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y}) = \alpha \boldsymbol{w} + \nabla_{\boldsymbol{w}} J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})
$$

重みを更新するために単一の勾配ステップをとるには、以下のようにこの更新を行う。

$$
\boldsymbol{w} \leftarrow \boldsymbol{w} - \epsilon(\alpha \boldsymbol{w} + \nabla_{\boldsymbol{w}} J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y}))
$$

別の方法で書くと、この更新は以下となる。

$$
\boldsymbol{w} \leftarrow (1 - \epsilon \alpha) \boldsymbol{w} - \epsilon \nabla_{\boldsymbol{w}} J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})
$$

我々は、通常の勾配更新を行う前に、weight decay項の追加が各ステップで定数のファクタによって重みベクトルを倍々に縮小するように、学習規則を修正したと見ることができる。これは単一のステップに起こることを述べている。では、訓練の全過程では何が起こっているのだろう？
我々は最小の非正規化訓練コスト$\boldsymbol{w}^* = \argmin_{\boldsymbol{w}} J(\boldsymbol{w})$を得る重みの値の近傍において目的関数への二次近似を作ることによって解析をかなり単純化するだろう。平均二乗誤差で線形回帰モデルをフィットさせる場合のように、目的関数が真の二次関数であるならば、その近似は完璧である。近似$\hat{J}$は以下で与えられる。

$$
\hat{J}(\boldsymbol{\theta}) = J(\boldsymbol{w}^*) + \frac{1}{2} (\boldsymbol{w} - \boldsymbol{w}^*)^\top \boldsymbol{H}(\boldsymbol{w} - \boldsymbol{w}^*)
$$

ここで、$\boldsymbol{H}$は$\boldsymbol{w}^*$で計算される$\boldsymbol{w}$に関する$J$のヘッセ行列である。$\boldsymbol{w}^*$が勾配を消失させる最小であると定義されるので、この二次近似には一次項が存在しない。同様に、$\boldsymbol{w}^*$が$J$の最小の場所であるので、$\boldsymbol{H}$が半正定値であると結論付けることができる。
$\hat{J}$の最小はその勾配

$$
\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w}) = \boldsymbol{H}(\boldsymbol{w} - \boldsymbol{w}^*)
$$

が$\boldsymbol{0}$に等しいところで発生する。
weight decayの効果を研究するため、我々は[@eq:7.7]をweight decayの勾配を加えることによって修正する。すると、$\hat{J}$の正則化バージョンの最小に対して解くことができる。我々は最小の場所を表現するために変数$\tilde{\boldsymbol{w}}$を用いる。

$$
\alpha \tilde{\boldsymbol{w}} + \boldsymbol{H}(\tilde{\boldsymbol{w}} - \boldsymbol{w}^*) = 0
$$

$$
(\boldsymbol{H} + \alpha \boldsymbol{I}) \tilde{\boldsymbol{w}} = \boldsymbol{H} \boldsymbol{w}^*
$$

$$
\tilde{\boldsymbol{w}} = (\boldsymbol{H} + \alpha \boldsymbol{I})^{-1} \boldsymbol{H} \boldsymbol{w}^*
$$

$\alpha$が$0$に近づくにつれて、正則化された解$\tilde{\boldsymbol{w}}$は$\boldsymbol{w}^*$に近づく。しかし、$\alpha$が増えるたびに何が起こるか？$\boldsymbol{H}$は実対称であるので、我々はこれを、$\boldsymbol{H} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top$のように、対角行列$\boldsymbol{\Lambda}$と固有ベクトルの正規直交基底$\boldsymbol{Q}$に分解する。[@eq:7.10]へ分解を適用すると、以下を得る。

$$
\tilde{\boldsymbol{w}} = (\boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top + \alpha \boldsymbol{I})^{-1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top \boldsymbol{w}^*
$$

$$
= \left[ \boldsymbol{Q} (\boldsymbol{\Lambda} + \alpha \boldsymbol{I}) \boldsymbol{Q}^\top \right]^{-1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top \boldsymbol{w}^*
$$

$$
= \boldsymbol{Q} (\boldsymbol{\Lambda} + \alpha \boldsymbol{I})^{-1} \boldsymbol{\Lambda} \boldsymbol{Q}^\top \boldsymbol{w}^*
$$

我々は、weight decayの効果が$\boldsymbol{H}$の固有ベクトルによって定義される軸に沿って$\boldsymbol{w}^*$を再スケールすることである、ということを理解している。具体的には、$\boldsymbol{H}$の$i$番目の固有ベクトルに並行である$\boldsymbol{w}^*$の成分は$\frac{\lambda_i}{\lambda_i + \alpha}$のファクタによって再スケールされる。（[@fig:2.3]で最初に説明されるこの種類のスケーリングがどのように機能するかを確認することをオススメする）。
$\boldsymbol{H}$の固有値が比較的大きいところ、例えば、$\lambda_i \gg \alpha$であるところの方向に沿うと、正則化の影響は比較的小さい。未だ$\lambda_i \ll \alpha$を持つ成分はほぼゼロの大きさを持つよう縮小されるだろう。この効果は[@fig:7.1]に図示される。

![最適な$\boldsymbol{w}$の値に関する$L^2$（または、weight decay）正則化の効果の図。実践の楕円は正規化されていない目的の等値の等高線を表現する。破線の円は$L^2$ regularizerの等値の等高線を表現する。点$\tilde{\boldsymbol{w}}$では、これらの競合する目的は均衡に達する。1次元目では、$J$のヘシアンの固有値は小さい。目的関数は$\boldsymbol{w}^*$から水平に離れるときにそこまで増加しない。目的関数はこの方向に沿って強い選好を表現しないので、regularizerはこの軸での強い影響を持つ。regularizerは$w_1$をゼロに近づける。2次元目では、目的関数は$\boldsymbol{w}^*$から離れる動きに非常に敏感である。対応する固有値は大きく、高い曲率を示す。結果として、weight decayは比較的少なく$w_2$の位置に影響を与える。](fig/7-1.png){#fig:7.1}

パラメータが目的関数を減らすのに大幅に貢献する方向のみが比較的そのままに保持される。目的関数を減らすのに貢献しない方向において、ヘシアンの小さな固有値は、この方向での動きが勾配を大幅に増やさないであろうことを教えてくれる。そのような重要でない方向に対応する重みベクトルの成分は訓練全体で正則化を用いることを通して減衰される[decayed away]。
これまで、我々は抽象的で一般的な二次のコスト関数の最適化に関する影響の観点でweight decayを考察してきた。これらの効果は特に機械学習とどのように関連するのだろう？我々は線形回帰を研究することで見つけ出すことができる。このモデルは、真のコスト関数が二次であり、それ故に、これまでに使ってきたのと同種の解析を受ける余地がある。再びその解析を適用すると、我々は同じ結果の特殊なケースを得ることができるだろうが、その解は訓練データの観点で表現されている。線形回帰に対して、コスト関数は二乗誤差の総和である。

$$
(\boldsymbol{X} \boldsymbol{w} - \boldsymbol{y})^\top (\boldsymbol{X} \boldsymbol{w} - \boldsymbol{y})
$$

$L^2$正則化を追加するとき、目的関数は以下のように変化する。

$$
(\boldsymbol{X} \boldsymbol{w} - \boldsymbol{y})^\top (\boldsymbol{X} \boldsymbol{w} - \boldsymbol{y}) + \frac{1}{2} \alpha \boldsymbol{w}^\top \boldsymbol{w}
$$

これは

$$
\boldsymbol{w} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1} \boldsymbol{X}^\top \boldsymbol{y}
$$

から

$$
\boldsymbol{w} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1} \boldsymbol{X}^\top \boldsymbol{y}
$$

へ解に対する正規方程式を変化させる。[@eq:7.18]における行列$\boldsymbol{X} ^\top \boldsymbol{X}$は共分散行列$\frac{1}{m} \boldsymbol{X} ^\top \boldsymbol{X}$に比例する。$L^2$正則化を用いることはこの行列を[@eq:7.17]における$(\boldsymbol{X}^\top \boldsymbol{X} + \alpha \boldsymbol{I})^{-1}$で置き換える。新しい行列は元の行列と同じであるが、対角に$\alpha$の加算を伴う。この行列の対角成分は各入力の特徴の分散に対応する。我々は、$L^2$正則化が学習アルゴリズムをより高い分散を持つと入力$\boldsymbol{X}$を「知覚」するようにすることを理解できる。これは、この追加される分散と比較して低い出力の対象を伴う共分散を持つ特徴に関する重みを縮小させる。

### $L^1$正則化

$L^2$ weight decayはweight decayの最も一般的な形式であるが、モデルパラメータのサイズに罰則を課す方法が他に存在する。もうひとつの選択肢は$L^1$正則化を用いることである。
公式的に言えば、モデルパラメータ$\boldsymbol{w}$に関する$L^1$正則化は以下のように定義される。

$$
\Omega(\boldsymbol{\theta}) = \| \boldsymbol{w} \|_1 = \sum_i |w_i|
$$

すなわち、個々のパラメータの絶対値の総和として定義される[^2]。ここで、我々は$L^2$正則化の解析で研究したようなバイアスパラメータを持たない単純な線形回帰モデルに関する$L^1$正則化の効果を考察するだろう。特に、我々は正則化の$L^1$と$L^2$の形式の間の差異を描くことに関心がある。$L^2$ weight decayと同様に、$L^1$ weight decayは正のハイパーパラメータ$\alpha$を用いて罰則$\Omega$をスケールすることによって正則化の強さを制御する。故に、正則化された目的関数$\tilde{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})$は以下によって与えられる。

$$
\tilde{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y}) = \alpha \| \boldsymbol{w} \|_1 + J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})
$$

これは、以下のような対応する勾配（実際には、劣勾配）を持つ。

$$
\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y}) = \alpha \text{sign}(\boldsymbol{w}) + \nabla_{\boldsymbol{w}} J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})
$$

ここで、$\text{sign}(\boldsymbol{w})$は単に要素ごとに適用される$\boldsymbol{w}$の符号である。

[^2]: $L^2$正則化と同様に、我々はゼロでない値に向かってではなく、代わりに、あるパラメータの値$\boldsymbol{w}^{(o)}$に向かってパラメータを正則化出来るだろう。この場合、$L^1$正則化は項$\Omega(\boldsymbol{\theta}) = \| \boldsymbol{w} - \boldsymbol{w}^{(o)} \|_1 = \sum_i | w_i - w_i^{(o)} |$を導入するだろう。

[@eq:7.20]を詳しく調べる事によって、$L^1$正則化の効果が$L^2$正則化の効果とかなり異なることを即座に理解できる。具体的には、勾配への正則化の寄与がもはや各$w_i$で線形にスケールしないことを理解できる。その代わりとして、$\text{sign}(w_i)$に等しい符号を持つ定数のファクタとなる。勾配のこの形式の１つの結論は、$L^2$正則化に対して行ったような$J(\boldsymbol{X}, \boldsymbol{y}; \boldsymbol{w})$の二次近似へのきれいな代数的な解を必ずしも理解する必要はないであろう、ということである。
我々の単純な線形モデルはテイラー級数を介して表現できる二次のコスト関数を持つ。言い換えれば、これがより洗練されたモデルのコスト関数を近似する、端を切り捨てたテイラー級数であると想像できるだろう。この設定における勾配は以下によって与えられる。

$$
\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w}) = \boldsymbol{H}(\boldsymbol{w} - \boldsymbol{w}^*)
$$

ここで、再び、$\boldsymbol{H}$は$\boldsymbol{w}^*$で計算される$\boldsymbol{w}$に関する$J$のヘッセ行列である。
$L^1$罰則は完全に一般的なヘシアンの場合にキレイな代数式を認めないので、ヘシアンが対角である、つまり、それぞれ$H_{i,i} > 0$となる$\boldsymbol{H} = \text{diag}([H_{1,1}, \dots, H_{n,n}$であるといったかなり単純化する仮定も作る。この仮定は線形回帰問題が入力の特徴の間のすべての相関を取り除くために前処理されている場合に成り立つ。これは、PCAを用いて達成されるかもしれない。
$L^1$正規化される目的関数の二次近似はパラメータ上の総和に分解される。

$$
\hat{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y}) = J(\boldsymbol{w}^*; \boldsymbol{X}, \boldsymbol{y}) + \sum_i \left[ \frac{1}{2} H_{i,i} (\boldsymbol{w}_i - \boldsymbol{w}_i^*)^2 + \alpha |w_i| \right]
$$

この近似的なコスト関数の最小化の問題は、以下の形式を伴って、（次元$i$ごとに）解析解を持つ。

$$
w_i = \text{sign}(w_i^*) \max \left{ |w_i^*| - \frarc{\alpha}{H_{i,i}}, 0 \right}
$$

すべての$i$に対して$w_i^* > 0$となる状況を考える。起こり得る結果として2つの場合が存在する。

1. $w_i^* \le \frac{\alpha}{H_{i,i}}$となる場合。ここでは、正規化された目的の下での$w_i$の最適値は単に$w_i = 0$である。これは、正規化された目的$\tilde{J}(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})$への$J(\boldsymbol{w}; \boldsymbol{X}, \boldsymbol{y})$の寄与が、方向$i$において、$w_i$の値をゼロに押し出す$L^1$正則化によって圧倒されるので、発生する。
2. $w_i^* > \frac{\alpha}{H_{i,i}}$となる場合。この場合、正則化は$w_i$の最適値をゼロに動かさないが、代わりに、$\frac{\alpha}{H_{i,i}}$に等しい距離によってその方向にずらす。

$w_i^* < 0$のときに似た処理が発生するが、$L^1$罰則は$w_i$を$\frac{\alpha}{H_{i, i}}$によってより小さな負にする、または、$0$にする。
$L^2$正則化と比較して、$L^1$正則化は結果としてより疎[sparse]である解となる。この文脈におけるスパース性はいくつかのパラメータがゼロの最適値を持つという事実を指している。$L^1$正則化のスパース性は$L^2$正則化で発生する振る舞いとは質的に異なる。[@eq:7.13]は$L^2$正則化に対して解$\tilde{w}$を与えた。$L^1$正則化の解析に導入した対角正定値のヘシアン$\boldsymbol{H}$の仮定を用いるその式を再訪するならば、我々は$\tilde{w}_i = \frac{H_{i,i}}{H_{i,i} + \alpha} w_i^*$を求める。$w_i^*$が非ゼロであったならば、$\tilde{w}_i$は非ゼロのままである。これは、$L^2$正則化がパラメータを疎にしないが、$L^1$正則化が十分に大きな$\alpha$ではそうさせるかもしれない、ということを実証する。$L^1$正則化に課されるスパース性の特性は特徴選択[feature selection]のメカニズムとして広範囲に用いられてきた。特徴選択はどの利用可能な特徴の部分集合が使われるべきかを選択することによって機械学習問題を単純化する。特に、良く知られたLASSO[@Tibshirani1995]（least absolute shrinkage and selection operator）モデルは$L^1$罰則を線形モデルと最小二乗のコスト関数で統合する。$L^1$罰則は重みの部分集合をゼロとなるようにし、対応する特徴が安全に破棄されても良いことを示唆する。
[@sec:5.6.1]では、我々は多くの正則化戦略がMAPベイズ推定として解釈できること、具体的には、$L^2$正則化が重みに関するガウシアン事前確率を持つMAPベイズ推定と等価であることを確認した。$L^1$正則化では、コスト関数を正則化するのに使われる罰則$\alpha \Omega(\boldsymbol{w}) = \alpha \sum_i |w_i|$は、事前確率が$\boldsymbol{w} \in \mathbb{R}^n$上の等方的ラプラス分布[@eq:3.26]であるとき、MAPベイズ推定によって最大化される対数事前確率項と等価である。

$$
\log p(\boldsymbol{w}) = \sum_i \log \text{Laplace}(w_i; 0, \frac{1}{\alpha}) = -\alpha \| \boldsymbol{w} \|_1 + n \log \alpha - n \log 2
$$

$\boldsymbol{w}$に関する最大化を介した学習の視点から、我々は$\log \alpha - \log 2$の項を、$\boldsymbol{w}$に依存しないという理由から、無視できる。

## 制約付き最適化としてのノルム罰則

パラメータノルム罰則によって正則化されるコスト関数を考える。

$$
\tilde{J}(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) = J(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) + \alpha \Omega(\boldsymbol{\theta})
$$

元の目的関数と罰則の集合から成る、一般化したラグランジュ関数を構築することで制約に従って関数を最小化できる、ということを[@sec:4.4]から再掲する。各罰則はKarush-Kuhn-Tuckerｆ（KKT）乗数と呼ばれる係数と制約を満たすかどうかを表現する関数の間の積である。$\Omega(\boldsymbol{\theta})$をある定数$k$より小さくなるよう制約を課すことを欲した場合、以下のような一般化したラグランジュ関数を構築できただろう。

$$
\mathcal{L}(\boldsymbol{\theta}, \alpha; \boldsymbol{X}, \boldsymbol{y}) = J(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) + \alpha(\Omega(\boldsymbol{\theta}) - k)
$$

その制約付き問題への解は以下によって与えられる。

$$
\boldsymbol{\theta}^* = \argmin_{\boldsymbol{\theta}} \max_{\alpha, \alpha \ge 0} \mathcal{L}(\boldsymbol{\theta}, \alpha)
$$

[@sec:4.4]で述べた通り、この問題を解くには$\boldsymbol{\theta}$と$\alpha$の両方を弄る必要がある。[@sec:4.5]は$L^2$制約を伴う線形回帰のうまくいく例を提供する。多くの様々な手順が取り得る --- いくつかは勾配降下法を用いるかもしれないし、またいくつかは勾配がゼロのところでの解析解を用いるかもしれない --- が、すべての手順において、$\alpha$は$\Omega(\boldsymbol{\theta}) > k$であるならどこでも増加しなければならないし、$\Omega(\boldsymbol{\theta}) < k$であるならどこでも減少しなければならない。すべての取り得る$\alpha$は$\Omega(\boldsymbol{\theta})$が縮小するよう促す。最適な値$\alpha^*$は$\Omega(\boldsymbol{\theta})$が縮小するよう促すが、$\Omega(\boldsymbol{\theta})$を$k$より小さくするほど強力ではない。
制約の効果へのいくつかの洞察を得るため、我々は$\alpha^*$を固定して、単なる$\boldsymbol{\theta}$の関数としてこの問題を見ることができる。

$$
\boldsymbol{\theta}^* = \argmin_{\boldsymbol{\theta}} \mathcal{L}(\boldsymbol{\theta}, \alpha^*) = \argmin_{\boldsymbol{\theta}} J(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) + \alpha^* \Omega(\boldsymbol{\theta})
$$

これは$\tilde{J}$の最適化の正則化された訓練問題とまったく同じである。故に、我々はパラメータノルム罰則を重みに関する制約を課すこととして考えることができる。$\Omega$が$L^2$ノルムであれば、重みは$L^2$の玉の中にあるよう制限される。$\Omega$が$L^1$ノルムであれば、重みは制限された$L^1$ノルムの領域の中にあるよう制限される。通常では、$\alpha^*$の値は$k$の値を直接教えてくれないので、我々は係数$\alpha^*$でweight decayを用いる事によって課される制約領域の大きさを知らない。原則として、$k$に対して解くことはできるが、$k$と$\alpha^*$の間の関係は$J$の形式に依存する。我々は制約領域の厳密な大きさを知らないが、一方で、制約領域を拡大または縮小するために$\alpha$を増加または現象させることによって大まかに制御することができる。より大きい$\alpha$はより小さな制約領域となるだろう。より小さな$\alpha$はより大きな制約領域となるだろう。
時折、我々は罰則ではなく明示的な制約を用いることを望むかもしれない。[@sec:4.4]で述べた通り、我々は$J(\boldsymbol{\theta})$上で下り坂に一歩踏み出し、$\Omega(\boldsymbol{\theta}) < k$を満たす最も近い点に$\boldsymbol{\theta}$を投影し直すために確率的勾配降下法のようなアルゴリズムを修正できる。これは、どんな$k$の値が適切であるかのアイデアを持ち、この$k$に対応する$\alpha$の値を探すのに時間をかけたくない場合、有用となり得る。
罰則で制約を強いるのではなく明示的な制約と再投影を用いるもうひとつの理由は、罰則が非凸最適化の手順を小さな$\boldsymbol{\theta}$に対応する極小値にはまり込ませてしまうことである。ニューラルネットワークを訓練するとき、これは通常ではいくつかの「死んだユニット」とともに訓練するニューラルネットワークとして現れる。これらは、そこに出入りする重みがすべて非常に小さいので、ネットワークによって学習される関数の振る舞いにほとんど寄与しないユニットである。重みのノルムに関する罰則で訓練するとき、これらのconfigurationsは、たとえ重みをより大きくすることによって$J$を大幅に減らすことができるとしても、局所的に最適となり得る。再投影によって実装される明示的制約は、重みが原点に近づくことを促さないので、これらのケースでさらにうまく機能し得る。再投影によって実装される明示的制約は重みが大きくなり、制約領域を離れようとするときにのみ影響を持つ。
最後に、再投影による明示的制約は、最適化手順に関するある安定性を課すので、有用となり得る。高い学習率を用いるとき、大きな重みが大きな勾配を誘発する正のフィードバックループに遭遇する可能性がある。故に、これは重みの大きな更新を誘発する。これらの更新が重みの大きさを一貫して増加させるならば、$\boldsymbol{\theta}$は数値的なオーバーフローが起きるまで原点から急速には離れる。再投影による明示的制約はこのフィードバックループが際限なしに重みの絶対値を増加し続けることを防ぐ。@Hinton2012cは、いくらかの安定性を維持しつつパラメータ空間の急速な探索を可能にするために高い学習率と組み合わせられた制約を用いることを推奨する。
特に、@Hinton2012cは@Srebro2005によって導入された、重み行列全体のFrobeniusノルムを制限するのではなく、ニューラルネットの層の重み行列の各劣のノルムを制限する、という戦略を推奨する。別個に各列のノルムを制限することはいずれかの1つの隠れユニットが非常に大きな重みを持つことを防ぐ。この制約をラグランジュ関数における罰則に変換した場合、$L^2$ weight decayと似ているが、各隠れユニットの重みに対して個別のKKT乗数を持つだろう。これらのKKT乗数のそれぞれは各隠れユニットを制約に従わせるために別個で動的に更新されるだろう。実践では列ノルムの制限は再投影による明示的制約として常に実装される。

## 正則化と制約不足[under-constrained]な問題

いくつかの場合、正則化は機械学習問題が適切に定義されるのに必須である。線形回帰やPCAを含む多くの機械学習における線形モデルは行列$\boldsymbol{X}^\top \boldsymbol{X}$の逆に依存する。これは$\boldsymbol{X}^\top \boldsymbol{X}$が特異であるときにできない。この行列は、データ生成分布がある方向で本当に分散を持たないときならいつでも、または、入力の特徴（$\boldsymbol{X}の列$）より少ないexamples（$\boldsymbol{X}$の行）が存在するという理由からある方向において分散が観察されないときに特異となり得る。この場合、多くの形式の正則化は代わりに$\boldsymbol{X}^\top \boldsymbol{X} + \alpha \boldsymbol{I}$の逆に対応する。この正則化された行列は可逆であることが保証される。
これらの線形問題は関連する行列が可逆であるときに閉形式の解を持つ。閉形式の解を持たない問題では劣決定されることもあり得る。ひとつの例はクラスが線形に分離可能である問題に適用されるロジスティック回帰である。重みベクトル$\boldsymbol{w}$が完璧な分類を達成できるならば、$2\boldsymbol{w}$もまた完璧な分類とより高い尤度を達成するだろう。確率的勾配降下法のような反復的な最適化手順は$\boldsymbol{w}$の大きさを絶えず増加させ、理論的には、決して止まらないだろう。実践では、勾配降下法の数値的な実装は最終的に数値的オーバーフローを引き起こすのに十分に大きな重みに達するだろう。この時点では、その挙動はプログラマが実数でない値を扱うと決めていたかどうかに依存するだろう。
ほとんどの形式の正則化は劣決定問題に適用される反復的手法の収束を保証できる。例えば、weight decayは勾配降下法に、尤度の傾きがweight decayの係数と等しいときに重みの大きさを増加させるのを止めさせるだろう。
劣決定問題を解くために正則化を用いるというアイデアは機械学習を超えて広がる。同じアイデアはいくつかの基本的な線形代数の問題で有用である。
[@sec:2.9]で見た通り、我々はMoore-Penroseの擬似逆行列を用いて劣決定の一次方程式を解くことができる。行列$\boldsymbol{X}$の擬似逆行列$\boldsymbol{X}^+$のある定義は以下であることを思い出したい。

$$
\boldsymbol{X}^+ = \lim_{\alpha \searrow 0} (\boldsymbol{X}^\top \boldsymbol{X} + \alpha \boldsymbol{I})^{-1} \boldsymbol{X}^\top
$$

我々はいま[@eq:7.29]をweight decayを持つ線形回帰を行うこととして認識できる。具体的には、[@eq:7.29]は正則化係数をゼロに縮小するときの[@eq:7.17]の極限である。故に、我々はその擬似逆行列を正則化を用いて劣決定問題を安定化することと解釈できる。

## データセットの増強[dataset augmentation]

機械学習モデルをより良く汎化させる最良の方法はより多くのデータで訓練することである。もちろん、実践では、持てるデータ量には限界がある。この問題を迂回する1つの方法は偽のデータを作り、それを訓練セットに加えることである。いくつかの機械学習タスクでは、新しい偽データを作ることは理に適って単純明快である。
このアプローチは分類に対するのが最も容易である。分類器は複雑で高次元の入力$\boldsymbol{x}$を取り、単一のカテゴリ識別子$y$にまとめる必要がある。これは分類器が特免する主なタスクが多種多様な変換に不変であることを意味する。我々は単に訓練セットにおける入力を$\boldsymbol{x}$に変換することによって新しい$(\boldsymbol{x}, y)$のペアを容易に生成できる。
このアプローチは他の多くのタスクに容易に適用可能とはならない。例えば、すでに密度推定問題を解いていなければ、密度推定タスクに対する新しい偽データを生成することは難しい。
dataset augmentationは特定の分類問題、すなわち、物体認識に対して特に効果的な技術となっている。画像は高次元であり、その多くが容易にシミュレートできる膨大な種類のバラツキの要因を含む。各方向に数ピクセルだけ訓練画像を平行移動するような演算はしばしば、モデルがすでに[@sec:9]で述べられる畳み込みやプーリングの技術を用いることによって部分的な平行移動に不変であるよう設計されている場合でさえ、汎化を大きく改善し得る。画像の回転や画像のスケーリングのような他の多くの演算もまたかなり効果的であると証明されている。
正しいクラスを変化させるであろう変換を適用しないよう注意しなければならない。例えば、光学的な文字認識タスクは"b"と"d"の差や"6"と"9"の差を認識する必要があり、そのために、左右の反転や180°の回転はこれらのタスクに対してデータセットを増強する適切な方法ではない。
分類器をそれに不変にしたいが、処理するのが容易でない変換も存在する。例えば、面外の回転は入力ピクセルの単純な幾何演算として実装できない。
dataset augmentationは同様に音声認識タスクに対して効果的である[@Jaitly2013]。
入力におけるノイズをニューラルネットワークに注入すること[@Sietsma1991]もまたdata augmentationのいち形式として理解できる。多くの分類に対して、そして、いくつかの認識タスクに対してさえも、タスクは依然として、小さなランダムノイズが入力に付加されていても、解くことができるはずである。とは言っても、ニューラルネットワークはノイズに対して非常にロバストでないことが証明されている[@Tang2010]。ニューラルネットワークのロバスト性を改善する1つの方法は単純にこれらの入力に適用されるランダムノイズとともに訓練することである。入力ノイズ注入は、denoising autoencoder [@Vincent2008]のような、いくつかの教師なし学習アルゴリズムの一部である。ノイズ注入は、ノイズが隠れ層に適用されるときにも機能する。これは、複数の抽象化レベルでdataset augmentationを行うこととして理解できる。@Poole2014は、ノイズの大きさが注意深く調整されることを条件に、このアプローチを高効率にすることができることを最近になって示した。[@sec:7.12]で述べられるであろう強力な正則化戦略であるdropoutはノイズを乗算することによって新しい入力を構築する工程として理解できる。
機械学習のベンチマーク結果を比較するとき、dataset augmentationの効果を計算に入れることは重要である。しばしば、手製のdataset augmentationスキームは機械学習技術の汎化誤差を劇的に減らし得る。ある機械学習アルゴリズムのパフォーマンスを別のものと比較するため、制御された実験を行う必要がある。機械学習アルゴリズムAと機械学習アルゴリズムBを比較するとき、両方のアルゴリズムが同じ手製のdataset augmentationスキームを用いて評価されることを確実にする。アルゴリズムAはdataset augmentationなしではうまく行われず、アルゴリズムBは入力の膨大な合成的変換と組み合わせられるときにうまく行うとする。そのような場合、機械学習アルゴリズムBの使用ではなく、合成的変換が改善されたパフォーマンスを引き起こした可能性が高い。時折、実験が適切に制御されているかどうか決めることは主観的な判断を必要とする。例えば、入力にノイズを注入する機械学習アルゴリズムはdataset augmentationのいち形式を処理している。通常、（入力にガウシアンノイズを加えるような）一般的に適用できる演算は機械学習アルゴリズムの一部と考えられ、一方で、（画像の無作為なクロッピングのような）ある応用領域に特有である演算は別個の前処理ステップであると考えられる。

## ノイズ堅牢性

[@sec:7.4]はdataset augmentation戦略として入力に適用されるノイズの使用を動機付けている。いくつかのモデルにとって、モデルの入力での微小分散を持つノイズの付加は重みのノルムに関する罰則を課すことと等価である[@Bishop1995a; @Bishop1995b]。一般的なケースにおいて、ノイズの注入が、特にノイズが隠れユニットに付加されるとき、パラメータを単に縮小するよりもかなり強力となり得ることを覚えておくことは重要である。隠れユニットに適用されるノイズはそれ自体を個別に考察するに値するなんとも重要なトピックである。[@sec:7.12]で述べられるdropoutアルゴリズムはそのアプローチの主な発展形である。
ノイズがモデルの正則化のために使われているもうひとつの方法は重みにそれを付加することによるものである。この技術はリカレントニューラルネットワークの文脈で主に使われている[@Jim1996; @Graves2011]。これは重みでのベイズ推定の確率的な実装として解釈できる。学習のベイズ的な扱いは、モデルの重みが不確かであり、この不確かさを反映する確率分布を介して表現可能であると見なすだろう。重みにノイズを付加することはこの不確かさを反映する実践的で確率的な方法である。
重みに適用されるノイズは、学習される関数の安定性を促すような、より伝統的な形式の正則化と（いくつかの仮定の下で）等価であると解釈することもできる。モデルの推定値$\hat{y}(\boldsymbol{x})$と真の値$y$の間の最小二乗のコスト関数を用いて特徴の集合$\boldsymbol{x}$をスカラにマッピングする関数$\hat{y}(\boldsymbol{x})$を訓練したいとする回帰の設定を考える。

$$
J = \mathbb{E}_{p(x, y)} \left[ (\hat{y}(\boldsymbol{x}) - y)^2 \right]
$$

その訓練セットは$m$個のラベル付きexamples${(\boldsymbol{x}^{(1)}, y^{(1)}), \dots, (\boldsymbol{x}^{(m)}, y^{(m)})}$から成る。
我々は、各入力表現とともに、ネットワークの重みの無作為な摂動$\epsilon_{\boldsymbol{W}} \sim \mathcal{N}(\boldsymbol{\epsilon}; \boldsymbol{0}, \eta \boldsymbol{I})$もまた含むと仮定する。標準的な$l$層のMLPがあると想像しよう。我々は摂動したモデルを$\hat{y}_{\boldsymbol{\epsilon W}}(\boldsymbol{x})$と表記する。ノイズの注入に関わらず、我々は依然としてネットワークの出力の二乗誤差を最小化することに関心がある。故に、目的関数は以下となる。

$$
\tilde{J}_{\boldsymbol{W}} = \mathbb{E}_{p(\boldsymbol{x}, y, \boldsymbol{\epsilon W})} \left[(\hat{y}_{\boldsymbol{\epsilon W}}(\boldsymbol{x}) - y)^2 \right]
$$

$$
= \mathbb{E}_{p(\boldsymbol{x}, y, \boldsymbol{\epsilon W})} \left[\hat{y}_{\boldsymbol{\epsilon W}}^2(\boldsymbol{x}) - 2 y \hat{y}_{\boldsymbol{\epsilon W}}^2(\boldsymbol{x}) + y^2 \right]
$$

小さな$\eta$では、（共分散$\eta \boldsymbol{I}$を持つ）付加された重みノイズを伴う$J$の最小化は追加の正則化項$\eta \mathbb{E}_{p(\boldsymbol{x}, y)} [ \| \nabla_{\boldsymbol{W}} \hat{y}(\boldsymbol{x}) \|^2 ]$を持つ$J$の最小化と等価である。この形式の正則化はパラメータを重みの小さな摂動が出力に関する比較的小さな影響を持つところのパラメータ空間の領域に進むよう促す。別の言い方をすれば、これはモデルが重みにおける小さなバラツキに比較的鈍感であるところの領域にモデルを押し進め、単なる極小ではなく、平坦な領域に囲まれる極小となる点を見つける[@Hochreiter1995]。（たとえば、$\hat{y}(\boldsymbol{x}) = \boldsymbol{w}^\top \boldsymbol{x} + b$となる）線形回帰の単純化したケースでは、この正則化項は$\eta \mathbb{E}_{p(\boldsymbol{x})} [ \| \boldsymbol{x} \|^2 ]$にcollapseする。これは、パラメータの関数ではなく、それ故に、モデルパラメータに関する$\tilde{J}_{\boldsymbol{W}}$の勾配に貢献しない。

### 出力対象でのノイズの注入

ほとんどのデータセットは$y$個のラベルにおいて数個の間違いがある。$y$が間違っているときに$\log p(y | \boldsymbol{x})$を最大化することは有害となり得る。これを防ぐ1つの方法はラベルに関するノイズを明示的にモデル化することである。例えば、我々は、ある小さな定数$\epsilon$に対して、訓練セットのラベル$y$が確率$1 - \epsilon$で正しいと仮定でき、そうでなければ、他の取り得るラベルのいずれかが正しいかもれないだろう。この仮定は、ノイズのサンプルを明示的に描くことによってではなく、コスト関数に解析的に組み入れることが容易である。例えば、label smoothingは、ハードな0と1の分類の対象を個々に$\frac{\epsilon}{k - 1}$と$1 - \epsilon$の対象に置き換えることによって、$k$個の出力値を持つsoftmaxに基づいてモデルを正則化する。故に、標準の交差エントロピーの損失はこれらのソフトな対象で使われるかもしれない。softmax分類器とハードな対象を持つ最大尤度学習は実際には一切収束しないかもしれない --- softmaxはぴったり0かぴったり1の確率をまったく予測できず、そのために、ますます大きな重みを学習し続け、より極端な予測を永遠に行うだろう。このシナリオがweight decayのような他の正則化戦略を用いることを妨げる可能性がある。label smoothingは正確な分類を促すことなしにハードな確率の追跡を防止するという利点を持つ。この戦略は1980年代から用いられていて、モダンなニューラルネットワークにおいてひときわ目立つ特徴となり続けている。

## 半教師あり学習

半教師あり学習のパラダイムにおいて、$P(\boldsymbol{\mathbf{x}})$由来のラベルなしexamplesと$P(\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}})$由来のラベルありexamplesの両方は$P(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$を推定するか、$\boldsymbol{\mathbf{x}}$から$\boldsymbol{\mathbf{y}}$を予測するのに使われる。
深層学習の文脈では、半教師あり学習は通常では表現$\boldsymbol{h} = f(\boldsymbol{x})$の学習を指す。その目標は表現を学習することであり、すなわち、同じクラスからのexamplesは似た表現を持つ。教師なし学習は表現空間でexamplesをグループ化する方法のための有用な手がかりを提供し得る。入力空間においてタイトに集まるexamplesは似た表現にマッピングされるべきである。新しい空間における線形の分類器は多くの場合でより良い汎化を達成するかもしれない[@Belkin2002; @Chapelle2003]。このアプローチの長年に渡る変種は（投影されたデータに関する）分類器を適用する前の前処理ステップとしての主成分分析のアプリケーションである。
モデルにおける別個の教師なしおよび教師あり要素を持つ代わりに、$P(\boldsymbol{\mathbf{x}})$または$P(\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}})$のどちらかの生成モデルが$P(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$の識別モデルとパラメータを共有するモデルを構築できる。故に、教師あり基準$-\log P(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$を（$-\log P(\mathbf{x})$または$-\log P(\boldsymbol{\mathbf{x}}, \boldsymbol{\mathbf{y}})$のような）教師なしまたは生成の基準とトレードオフできる。故に、生成基準は教師あり学習問題への解についての事前学習の信念の特定の形式を表現する[@Lasserre2006]。つまりは、$P(\boldsymbol{\mathbf{x}})$の構造は、共有されたパラメータ化によって捕捉される方法で$P(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$の構造と接続する。どれだけの生成基準が総基準に含まれているかを制御することによって、純粋に生成または純粋に識別の基準とよりも良いトレードオフを見つけることができる[@Lasserre2006; @Larochelle2008]。
@Salakhutdinov2008は回帰で用いられるカーネルマシンのカーネル関数を学習ための手法を述べる。ここでは、$P(\boldsymbol{\mathbf{x}})$をモデル化するためのラベルなしexamplesの使い方が$P(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}})$をかなり大幅に改善する。
半教師あり学習についてのさらなる情報は@Chapelle2006を参照のこと。

## マルチタスク学習

マルチタスク学習[@Caruana1993]はいくつかのタスクから生じる（パラメータに課されるソフトな制約として見ることができる）examplesをプーリングすることによって汎化を改善する方法である。追加の訓練examplesがうまく汎化する値に向かってモデルのパラメータにさらなる圧力をかけるのと同じ方法において、モデルの一部がタスク間で共有されるとき、（共有が正当化されると仮定して）そのモデルの一部は良い値に向かってさらに制限され、しばしばより良い汎化を生み出す。
[@fig:7.2]はマルチタスク学習の非常に一般的な形式を図示する。ここでは、（$\boldsymbol{\mathbf{x}}$を与えられたときの$\boldsymbol{\mathbf{y}}^{(i)}$を予測する）様々な教師ありタスクが、ある中間レベル表現$\boldsymbol{h}^{(shared)}$と同様に、同じ入力$\boldsymbol{\mathbf{x}}$を共有して、ファクタの共通プールを捕捉する。そのモデルは一般に２種類の部分と関連するパラメータに分けられる。

1. （良い汎化を達成するためにこれらのタスクのexamplesから恩恵を受けるだけの）タスク特有のパラメータ。これらは[@fig:7.2]におけるニューラルネットワークの上の方の層である。
2. （タスクすべてのプールされたデータから恩恵を受ける）すべてのタスク間で共有される汎用パラメータ。これらは[@fig:7.2]におけるニューラルネットワークの下の方の層である。

![マルチタスク学習は深層学習フレームワークにおけるいくつかの方法でcastでき、この図はタスクが共通の入力を共有するが、異なる対象の確率変数を伴うところの一般的な状況を図示する。（教師ありかつフィードフォワードである、または、下向きの矢印を持つ生成的の構成要素を含むかどうかの）深層ネットワークの低層はそのようなタスク間で共有され得る一方、（$\boldsymbol{h}^{(1)}$と$\boldsymbol{h}^{(2)}$に出入りする重みと個々に関連する）タスク特有のパラメータは共有された表現$\boldsymbol{h}^{(shared)}$を生成するこれらの上部で学習され得る。その下地となる仮定は入力$\boldsymbol{\mathbf{x}}$におけるバラツキを説明するファクタの共通プールが存在する一方で、各タスクがこれらのファクタの部分集合と関連しているということである。この例では、トップレベルの隠れユニット$\boldsymbol{h}^{(1)}$と$\boldsymbol{h}^{(2)}$が（$\boldsymbol{\mathbf{y}}^{(1)}$と$\boldsymbol{\mathbf{y}}^{(2)}$をそれぞれ予測する）各タスクに特殊化される一方で、ある中間レベルの表現$\boldsymbol{h}^{(shared)}$がすべてのタスク間で共有されることを加えて仮定する。教師なし学習の文脈において、これはトップレベルのファクタのいくつかが出力タスク（$\boldsymbol{h}^{(3)}$）のどれとも関連しないことを確実する。つまり、入力のバラツキのいくつかを説明するファクタは存在するが、$\boldsymbol{\mathbf{y}}^{(1)}$とも$\boldsymbol{\mathbf{y}}^{(2)}$とも関係ない。](fig/7-2.png){#fig:7.2}

改善された汎化および汎化誤差境界[@Baxter1995]は共有されたパラメータのために達成され得る。このために、（単一タスクのモデルのシナリオと比較したとき、共有されたパラメータに対するexamples数の増加に比例して）統計的な強度が大幅に改善され得る。もちろん、これは異なるタスクの間の統計的な関係についてのいくつかの仮定が有効である場合に限り発生し、タスクのいくつかの解だで共有される何かがあることを意味する。
深層学習の視点から、下地となる事前確率の信念は以下である。*異なるタスクに関連するデータにおいて観察されるバラツキを説明するファクタの内、いくつかは2つ以上のタスク間で共有される。*

## 早期終了[Early Stopping]

タスクをオーバーフィットするのに十分な表現上の容量を持つ大きなモデルを訓練するとき、我々はしばしば、訓練誤差が時間とともに徐々に減少するが、検証セット誤差が再び上昇し始めることに気付く。確実に起こるこの挙動の例は[@fig:7.3]を参照のこと。

![時間とともに負の対数尤度の損失がどのように変化するかを示す学習曲線（データセット、または、epochsに対する訓練の反復数として示される）。この例では、我々はMNISTに関するmaxoutネットワークを訓練する。目的の訓練が時間とともに一貫して減少するが、検証セットの平均損失が最終的に再び増加し始め、非対称なU型曲線を形成することを見てほしい。](fig/7-3.png){#fig:7.3}

これは我々が最低の検証セット誤差を持つ時点でのパラメータ設定に戻すことによってより良い検証セット誤差（そして、それ故に、できればより良いテストセット誤差）を持つモデルを得ることができることを意味する。検証セットに関する誤差が改善するたび、我々は最後のパラメータではなくこれらのパラメータを返す。そのアルゴリズムはパラメータが事前に指定したある反復回数で最良を記録した検証誤差よりも改善できなかったときに終了する。この手順は[@lst:7.1]でより公式的に明示される。

```alporithm
\State{評価間のステップ数を$n$とする。}
\State{諦める前に悪化する検証セット誤差を観察する回数"patience"を$p$とする。}
\State{初期パラメータを$\boldsymbol{\theta}_o$とする。}
\State{$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}_o$}
\State{$i \leftarrow 0$}
\State{$j \leftarrow 0$}
\State{$v \leftarrow \infty$}
\State{$\boldsymbol{\theta}^* \leftarrow \boldsymbol{\theta}$}
\State{$i^* \leftarrow i$}
\While{$j < p$}
    \State{$n$ステップだけ訓練アルゴリズムを実行することで$\boldsymbol{\theta}$を更新する。}
    \State{$i \leftarrow i + n$}
    \State{$v' \leftarrow \text{ValidationSetError}(\boldsymbol{\theta})$}
    \If{$v' < v$}
        \State{$j \leftarrow 0$}
        \State{$\boldsymbol{\theta}^* \leftarrow \boldsymbol{\theta}$}
        \State{$i^* \leftarrow i$}
        \State{$v \leftarrow v'$}
    \Else
        \State{$j \leftarrow j + 1$}
    \EndIf
\EndWhile
\State{最良のパラメータは$\boldsymbol{\theta}^*$であり、最良の訓練ステップ数は$i^*$である。}
```
: 訓練するための最適な時間を決定する早期終了のメタアルゴリズム。このメタアルゴリズムは多種多様な訓練アルゴリズムおよび検証セットの誤差を定量化する方法でうまく機能する一般的な戦略である。 {#lst:7.1}

この戦略は早期終了[early stopping]として知られる。これはおそらく深層学習において最も一般的に使われる形式の正則化である。その人気は有効性と単純さの両方に起因する。
早期終了を考える1つの方法は非常に効率的なハイパーパラメータ選択アルゴリズムとしてである。この見方では、訓練ステップ数は単なる別のハイパーパラメータである。我々は[@fig:7.3]においてこのハイパーパラメータがU型の検証セットパフォーマンス曲線を持つことを確認できる。モデル容量を制御するほとんどのハイパーパラメータは、[@fig:5.3]図示される通り、そのようなU型の検証セットパフォーマンス曲線を持つ。早期終了の場合では、我々は訓練セットをフィットさせるのにどれだけのステップ数を取り得るかを決定することによってモデルの実効容量を制御している。ほとんどのハイパーパラメータは高価な推測および確認プロセスを用いて選ばれなければならない。ここでは、訓練の開始時にハイパーパラメータをセットし、その効果を見るために数ステップだけ訓練を実行する。「訓練時間」のハイパーパラメータは、定義により、訓練の1回の実行がハイパーパラメータの多くの値を試行するという点で、一意である。早期終了を介してこのハイパーパラメータを自動的に選ぶための唯一の重要なコストは訓練の間に定期的に検証セットの評価を実行することである。理想的には、これは、主要の訓練プロセスから分離したマシン、分離したCPU、または、分離したGPUで訓練プロセスと並列に行われる。そのような資源が使えない場合、これらの定期的な評価のコストは訓練セットと比べて小さい検証セットを用いることによって、または、より低頻度に検証セット誤差を評価したり、最適な訓練時間のより低分解能の推定値を得たりすることによって減少するかもしれない。
早期終了への追加コストは最適なパラメータのコピーを維持する必要性である。このコストは、より遅くより大きい形式のメモリにこれらのパラメータを格納することを許容するので、一般に無視できる（例えば、訓練はGPUメモリで行われるが、最適なパラメータの格納はホストメモリかディスクドライブで行われる）。最良のパラメータはほとんど書き込まれず、訓練中に一切読み出されないので、これらの時折発生する低速な書き込みは総訓練時間に関してそれほど影響を持たない。
早期終了は、基礎を成す訓練手順、目的関数、許容されるパラメータの値の集合にほとんど手を加える必要がないという点で、目立たない形式の正則化である。これは学習ダイナミクスを傷付けずに早期終了を用いることが容易であることを意味する。これは、病理的に小さな重みを持つ解に対応する誤った極小にネットワークが囚われたり、過剰なweight decayを用いたりしないよう気を付けなければならないweight decayとは対照的である。
早期終了は単体か他の正則化戦略との併用かのいずれかで用いられるかもしれない。より良い汎化を促すよう目的関数を修正する正則化戦略を用いるときでさえ、最良の汎化が訓練目的の極小で起こることは稀である。
早期終了は検証セットを必要とする。これは、いくつかの訓練データがモデルに供給されないことを意味する。この追加データを最も良く活用するため、早期終了を伴う初期の訓練が完了した後に追加の訓練を行うことができる。その次に、追加の訓練ステップ、訓練データすべてが含まれる。基本戦略は2つあり、ひとつはこの二番目の訓練手順で使える。
一方の戦略[@lst:7.2]は再びモデルを初期化し、すべてのデータで再訓練することである。この2番目の訓練パスにおいて、我々は決定された早期終了の手順が一番目のパスにおいて最適であったのと同じステップ数で訓練する。この手順に関連したいくつかの微妙な点が存在する。例えば、データセットを通して同じパラメータ更新数または同じパス数で再訓練するかどうかを知る良い方法は存在しない。訓練の第二ラウンドでは、データセットを通した各パスは、訓練セットがより大きくなるので、より多くのパラメータ更新を必要とするだろう。

```algorithm
\State{$\boldsymbol{X}^{(train)}$と$\boldsymbol{y}^{(train)}$を訓練セットとする。}
\State{$\boldsymbol{X}^{(train)}$と$\boldsymbol{y}^{(train)}$をそれぞれ$(\boldsymbol{X}^{(subtrain)}, \boldsymbol{X}^{(valid)})$と$(\boldsymbol{y}^{(subtrain)}, \boldsymbol{y}^{(valid)})$に分割する。}
\State{訓練データに対して$\boldsymbol{X}^{(subtrain)}$と$\boldsymbol{y}^{(subtrain)}$を用いて、検証データに対して$\boldsymbol{X}^{(valid)}$と$\boldsymbol{y}^{(valid)}$を用いて無作為な$\boldsymbol{\theta}$から初めて早期終了[@lst:7.1]を実行する。これは最適なステップ数$i^*$を返す。}
\State{再び$\boldsymbol{\theta}$を乱数にセットする。}
\State{$i^*$ステップだけ$\boldsymbol{X}^{(train)}$と$\boldsymbol{y}^{(train)}$で訓練する。}
```
: どれだけ長く訓練し、すべてのデータで再訓練するかを決定するために早期終了を用いるためのメタアルゴリズム。 {#lst:7.2}

すべてのデータを使うためのもう一方の戦略は訓練の第一ラウンドから得られるパラメータを維持し、訓練し続けるが、今度はすべてのデータを用いることである。このステージでは、我々はもはやステップ数の観点で終了するタイミングに対するガイドを持たない。代わりに、我々は検証セットで平均損失関数をモニタリングし、訓練セットの目的の値を下回って早期終了手順を停止させるまで訓練を継続することができる。この戦略はゼロからモデルを再訓練する高いコストを回避するが、同じようにはうまくいかない。例えば、検証セットに関する目的は対象の値に達しさえしないかもしれず、そのために、この戦略は終了する保証さえされない。この手順は[@lst:7.3]においてより公式的に示される。

```algorithm
\State{$\boldsymbol{X}^{(train)}$と$\boldsymbol{y}^{(train)}$を訓練セットとする。}
\State{$\boldsymbol{X}^{(train)}$と$\boldsymbol{y}^{(train)}$をそれぞれ$(\boldsymbol{X}^{(subtrain)}, \boldsymbol{X}^{(valid)})$と$(\boldsymbol{y}^{(subtrain)}, \boldsymbol{y}^{(valid)})$に分割する。}
\State{訓練データに対して$\boldsymbol{X}^{(subtrain)}$と$\boldsymbol{y}^{(subtrain)}$を用いて、検証データに対して$\boldsymbol{X}^{(valid)}$と$\boldsymbol{y}^{(valid)}$を用いて無作為な$\boldsymbol{\theta}$から初めて早期終了[@lst:7.1]を実行する。これは$\boldsymbol{\theta}$を更新する。}
\State{$\epsilon \leftarrow J(\boldsymbol{\theta}, \boldsymbol{X}^{(subtrain)}, \boldsymbol{y}^{(subtrain)}$}
\While{$J(\boldsymbol{\theta}, \boldsymbol{X}^{(valid)}, \boldsymbol{y}^{(valid)}) > \epsilon$}
    \State{$n$ステップだけ$\boldsymbol{X}^{(train)}$と$\boldsymbol{y}^{(train)}$で訓練する。}
\EndWhile
```
: 目的値がどれくらいでオーバーフィットし始め、その値に達するまで訓練し続けるかを決定するために早期終了を用いるメタアルゴリズム。 {#lst:7.3}

早期終了は、訓練手順の計算コストを減らすので、有用でもある。訓練の反復回数を制限することによるコストにおける明らかな減少に加えて、コスト関数に罰則項を追加したり、そのような追加の項の勾配をけいさんしたりする必要なしに正則化をもたらすという恩恵も持つ。

**早期終了はどのように正則化として振る舞うか：**　これまでに我々は早期終了が正則化戦略であると述べてきたが、検証セット誤差がU型曲線を持つ学習曲線を示すことによってのみこの主張を支持してきた。早期終了がモデルを正規化する実際のメカニズムはどんなものだろう？@Bishop1995aと@Sjoberg1995は、早期終了が、[@fig:7.4]に図示される通り、初期パラメータ値$\boldsymbol{\theta}_o$の近傍において比較的小さなボリュームのパラメータ空間に最適化手順を制限する効果を持つと論じた。より具体的に言えば、学習率$\epsilon$で（$\tau$回の訓練の反復に対応する）$\tau$個の最適化ステップを取ると想像してほしい。我々はその積$\epsilon \tau$を実効容量の測度として見なすことができる。その勾配が有界であると仮定すると、反復数と学習率の両方を制限することは$\boldsymbol{\theta}_o$から到達可能なパラメータ空間のボリュームに限定する。この意味において、$\epsilon \tau$はweight decayで使われる係数の逆数であったかのように振る舞う。

![早期終了の効果の図。（左）実線の等高線は負の対数尤度の等高線を示す。破線は原点から始まるSGDによって取られる軌道を示す。コストを最小化する点$\boldsymbol{w}^*$で終了するのではなく、早期終了はより早期の点$\tilde{\boldsymbol{w}}$で終了する軌道となる。（右）比較対象としての$L^2$正則化の効果の図。破線の円は$L^2$罰則の等高線を示す。これは、総コストの最小値を非正則化コストの最小値より原点に近くにあるようにする。](fig/7-4.png){#fig:7.4}

もちろん、我々は早期終了が$L^2$正則化と---二次の誤差関数と単純な勾配降下法を伴う単純な線形モデルの場合において---どれだけ等価であるかを示すことができる。
古典的な$L^2$正則化と比較するため、我々はパラメータが線形な重みのみ（$\boldsymbol{\theta} = \boldsymbol{w}$）となる単純な設定をテストする。我々は重み$\boldsymbol{w}^*$の経験的に最適な値の近傍における二次近似を持つコスト関数$J$をモデル化できる。

$$
\hat{J}(\boldsymbol{\theta}) = J(\boldsymbol{w}^*) + \frac{1}{2}(\boldsymbol{w} - \boldsymbol{w}^*)^\top \boldsymbol{H}(\boldsymbol{w} - \boldsymbol{w}^*)
$$

ここで、$\boldsymbol{H}$は$\boldsymbol{w}^*$で計算したときの$\boldsymbol{w}$に関する$J$のヘッセ行列である。$\boldsymbol{w}^*$が$J(\boldsymbol{w})$の最小であると仮定するならば、我々は$\boldsymbol{H}$が半正定値であることを知っている。局所的なテイラー級数近似に基づき、その勾配は以下によって与えられる。

$$
\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w}) = \boldsymbol{H}(\boldsymbol{w} - \boldsymbol{w}^*)
$$

我々は訓練の間にパラメータベクトルが従う軌道を研究している。単純さのため、初期パラメータベクトルを原点にセットする[^3]、すなわち、$\boldsymbol{w}^{(0)} = \boldsymbol{0}$とする。$\hat{J}$に関する勾配降下法を解析することによって$J$に関する勾配降下法の近似的な挙動を研究するとしよう。

[^3]: aaa

$$
\boldsymbol{w}^{(\tau)} = \boldsymbol{w}^{(\tau - 1)} - \epsilon \nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w}^{(\tau - 1)})
$$

$$
= \boldsymbol{w}^{(\tau - 1)} - \epsilon \boldsymbol{H}(\boldsymbol{w}^{(\tau - 1)} - \boldsymbol{w}^*)
$$

$$
\boldsymbol{w}^{(\tau)} - \boldsymbol{w}^* = (\boldsymbol{I} - \epsilon \boldsymbol{H})(\boldsymbol{w}^{(\tau - 1)} - \boldsymbol{w}^*)
$$

そして、$\boldsymbol{H}$の固有値分解$\boldsymbol{H} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top$を活用して、$\boldsymbol{H}$の固有ベクトルの空間においてこの式を書き換えるとする。ここで、$\boldsymbol{\Lambda}$は対角行列であり、$\boldsymbol{Q}$は固有ベクトルの正規直交基底である。

$$
\boldsymbol{w}^{(\tau)} - \boldsymbol{w}^* = (\boldsymbol{I} - \epsilon \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^\top)(\boldsymbol{w}^{(\tau - 1)} - \boldsymbol{w}^*)
$$

$$
\boldsymbol{Q}^\top (\boldsymbol{w}^{(\tau)} - \boldsymbol{w}^*) = (\boldsymbol{I} - \epsilon \boldsymbol{\Lambda}) \boldsymbol{Q}^\top (\boldsymbol{w}^{(\tau - 1)} - \boldsymbol{w}^*)
$$

$\boldsymbol{w}^{(0)} = 0$であり、$\epsilon$が$|1 - \epsilon \lambda_i| < 1$であることを保証するのに十分小さくなるよう選ばれると仮定すると、$\tau$回のパラメータ更新の後の訓練の間のパラメータ軌道は以下のようになる。

$$
\boldsymbol{Q}^\top \boldsymbol{w}^{(\tau)} = [\boldsymbol{I} - (\boldsymbol{I} - \epsilon \boldsymbol{\Lambda})^\tau] \boldsymbol{Q}^\top \boldsymbol{w}^*
$$

そして、$L^2$正則化に対する[@eq:7.13]における$\boldsymbol{Q}^\top \tilde{\boldsymbol{w}}$のための式は以下のように再配置できる。

$$
\boldsymbol{Q}^\top \tilde{\boldsymbol{w}} = (\boldsymbol{\Lambda} + \alpha \boldsymbol{I})^{-1} \boldsymbol{\Lambda} \boldsymbol{Q}^\top \boldsymbol{w}^*
$$

$$
\boldsymbol{Q}^\top \tilde{\boldsymbol{w}} [\boldsymbol{I} - (\boldsymbol{\Lambda} + \alpha \boldsymbol{I})^{-1} \alpha] \boldsymbol{Q}^\top \boldsymbol{w}^*
$$

[@eq:7.40]と[@eq:7.42]を比較すると、ハイパーパラメータ$\epsilon$、$\alpha$、$\tau$が以下のように選ばれるならば、$L^2$正則化と早期終了は（少なくとも目的関数の二次近似の下では）等価のように見える、ということがわかる。

$$
(\boldsymbol{I} - \epsilon \boldsymbol{\Lambda})^\tau = (\boldsymbol{\Lambda} + \alpha \boldsymbol{I})^{-1} \alpha
$$

さらに推し進めて、対数を取って$\log(1 + x)$に対して級数展開を用いることによって、すべての$\lambda_i$が小さい（つまり、$\epsilon \lambda_i \ll 1$かつ$\lambda_i / \alpha \ll 1$である）ならば、以下となると結論付けることができる。

$$
\tau \approx \frac{1}{\epsilon \alpha}
$$

$$
\alpha \approx \frac{1}{\tau \epsilon}
$$

すなわち、これらの仮定の下で、訓練の反復回数$\tau$は$L^2$正則化のパラメータに反比例する役割を果たし、$\tau \epsilon$の逆はweight decay係数の役割を果たす。
（目的関数の）有意な曲率の方向に対応するパラメータの値は小さな曲率の方向ほど小さく正規化される。もちろん、早期終了の文脈では、これは実際に、有意な曲率の方向に対応するパラメータがより少ない曲率の方向に対応するパラメータと比較して早く学習する傾向があることを意味する。
本節における導出は長さ$\tau$の軌道が$L^2$正則化された目的の最小に対応する点で終わることを示した。もちろん、早期終了は軌道の長さの単なる制約以上のものである。代わりに、早期終了は一般に空間における特に良い点で軌道を停止するために検証セット誤差をモニタリングすることを伴う。したがって、早期終了は、それが正しい正則化量を自動的に決定する一方で、weight decayが様々なハイパーパラメータの値で多くの訓練の実験を必要とする、という点においてweight decayに勝る利点を持つ。

## parameter tyingとパラメータ共有

これまでのところ、本章では、パラメータへの制約や罰則を追加することを考察していたとき、固定の領域または点に関して常にそれを行っていた。例えば、$L^2$正則化（または、weight decay）はゼロの固定値から逸らすためのモデルパラメータを罰則化する。しかしながら、あるときには、我々はモデルパラメータの適切な値についての事前知識を説明するための他の方法を必要とするかもしれない。またあるときには、我々はパラメータが取るべき値についてを正確に知らないが、領域の知識やモデルのアーキテクチャにより、モデルパラメータの間にいくつかの依存性が存在すべきであることを知っているかもしれない。
しばしば説明したい依存性の一般的なタイプはパラメータが互いに隣接しているはずであるということである。以下のシナリオを考える。（同じクラスの集合を伴う）同じ分類タスクを行うが、幾分異なる入力分布を持つ2つのモデルがある。公式的には、パラメータ$\boldsymbol{w}^{(A)}$を持つモデル$A$とパラメータ$\boldsymbol{w}^{(B)}$を持つモデル$B$がある。2つのモデルは入力を2つの異なるが関連した出力$\hat{y}^{(A)} = f(\boldsymbol{w}^{(A)}, \boldsymbol{x})$と$\hat{y}^{(B)} = g(\boldsymbol{w}^{(B)}, \boldsymbol{x})$にマッピングする。
そのタスクは、モデルパラメータが互いに隣接しているはず、つまり、$\forall_i, w_i^{(A)}$が$w_i^{(B)}$と隣接しているはずであると信じるのに十分似ている（おそらく似た入力と出力の分布を持つ）と想像するとしよう。我々は正則化を通してこの情報を活用出来る。具体的には、$\Omega(\boldsymbol{w}^{(A)}, \boldsymbol{w}^{(B)}) = \| \boldsymbol{w}^{(A)} - \boldsymbol{B}^{(B)} \|_2^2$の形式のパラメータノルム罰則を用いることができる。ここでは、$L^2$罰則を用いたが、その他の選択もまた可能である。
この種のアプローチは@Lasserre2006で提案された。彼らは、（観測した入力データの分布を捕捉するため）教師なしパラダイムで訓練したもう一方のモデルのパラメータに近づけるように、教師ありパラダイムにおける分類器として訓練した一方のモデルのパラメータを正則化した。そのアーキテクチャは分類器モデルにおけるパラメータの多くが教師なしモデルにおける対応するパラメータと対に成り得るように構築された。
パラメータノルム罰則は互いに隣接するようパラメータを正則化する1つの方法である一方で、より人気のある方法は制約を用いること、すなわち、パラメータの集合が等しくなるよう強制することである。この正則化手法はしばしば、パラメータのユニークな集合を共有すると様々なモデルやモデルの構成要素を解釈するので、パラメータ共有[parameter sharing]と呼ばれる。（ノルム罰則を介して）近くなるようパラメータを正則化することに対するパラメータ共有の有意な利点はパラメータの部分集合（ユニークな集合）のみがメモリに格納される必要があるということである。畳み込みニューラルネットワークのような、あるモデルでは、これはモデルのメモリフットプリントにおいて著しい減少を引き起こし得る。

### 畳み込みニューラルネットワーク

パラメータ共有を使う断然最も人気があり広範な方法はコンピュータビジョンに適用される畳み込みニューラルネットワーク（CNN）において発生する。
自然の画像は平行移動に不変である多くの統計的特性を持つ。例えば、猫の写真は1ピクセル右に平行移動させても猫の写真のままである。CNNは複数の画像の位置の間でパラメータを共有することによってこの特性を計算に入れる。同じ特徴（同じ重みを持つ隠れユニット）は入力における異なる位置に渡って計算される。これは画像中に現れる猫が列$i$にいるか列$i+1$にいるかどうかに関わらず同じ猫検出器で猫を見つけることができることを意味する。
パラメータ共有はCNNをユニークなモデルパラメータの数を劇的に小さくし、訓練データにおける対応する増加を必要とせずにネットワークサイズを著しく増加させることを可能にする。これは領域の知識をネットワークアーキテクチャに効果的に組み込む方法の最良の例のひとつのままである。
CNNは[@sec:9]にてより詳しく考察する。

## スパース表現

weight decayはモデルパラメータへ直接的に罰則を配置することで作用する。もうひとつの戦略は、活性化を疎にするよう促すよう、ニューラルネットワークにおけるユニットの活性化に罰則を配置することである。これはモデルパラメータへ複雑な罰則を間接的に課す。

我々はすでに（[@sec:7.1.2]において）$L^1$罰則がスパース表現をどのように含むかを考察してきた---パラメータの多くがゼロ（または、ゼロ近く）になることを意味する。その一方で、表現的スパース性は表現の要素の多くがゼロ（または、ゼロ近く）である表現を述べる。この区別の単純化した見方は線形回帰の文脈において以下のように図示され得る。

$$
\underset{\boldsymbol{y} \in \mathbb{R}^m}{\left[ \begin{matrix}
18 \\
5 \\
15 \\
-9 \\
-3
\end{matrix} \right]} = \underset{\boldsymbol{A} \in \mathbb{R}^{m \times n}}{\left[ \begin{matrix}
4 & 0 & 0 & -2 & 0 & 0 \\
0 & 0 & -1 & 0 & 3 & 0 \\
0 & 5 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & -1 & 0 & -4 \\
1 & 0 & 0 & 0 & -5 & 0
\end{matrix} \right]} \underset{\boldsymbol{x} \in \mathbb{R}^n}{\left[ \begin{matrix}
2 \\
3 \\
-2 \\
-5 \\
1 \\
4
\end{matrix} \right]}
$$

$$
\underset{\boldsymbol{y} \in \mathbb{R}^m}{\left[ \begin{matrix}
-14 \\
1 \\
19 \\
2 \\
23
\end{matrix} \right]} = \underset{\boldsymbol{B} \in \mathbb{R}^{m \times n}}{\left[ \begin{matrix}
3 & -1 & 2 & -5 & 4 & 1 \\
4 & 2 & -3 & -1 & 1 & 3 \\
-1 & 5 & 4 & 2 & -3 & -2 \\
3 & 1 & 2 & -3 & 0 & -3 \\
-5 & 4 & -2 & 2 & -5 & -1
\end{matrix} \right]} \underset{\boldsymbol{h} \in \mathbb{R}^n}{\left[ \begin{matrix}
0 \\
2 \\
0 \\
0 \\
-3 \\
0
\end{matrix} \right]}
$$

1つ目の式には、疎にパラメータ化された線形回帰モデルの例がある。2つ目には、データ$\boldsymbol{x}$のスパース表現$\boldsymbol{h}$を持つ線形回帰がある。つまり、$\boldsymbol{h}$は、$\boldsymbol{x}$に示される情報を表現するが、それが疎なベクトルで行われるという、ある意味で、$\boldsymbol{x}$の関数である。
表現上の正則化はパラメータ正則化で使ってきたものと同種のメカニズムによって達成される。
表現のノルム罰則正則化は表現に関するノルム罰則を損失関数$J$に加えることで行われる。この罰則は$\Omega(\boldsymbol{h})$と表記される。前と同様に、我々は$\tilde{J}$にって正則化された損失関数を以下と表記する。

$$
\tilde{J}(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) = J(\boldsymbol{\theta}; \boldsymbol{X}, \boldsymbol{y}) + \alpha \Omega(\boldsymbol{h})
$$

ここで、$\alpha \in [0, \infty)$は、値が大きいほどより大きく正則化するよう、ノルム罰則項の相対寄与を重み付けする。
パラメータの$L^1$罰則がパラメータのスパース性を引き起こすのと同様に、表現の要素に関する$L^1$罰則は表現上のスパース性$\Omega(\boldsymbol{h}) = \|\boldsymbol{h}\|_1 = \sum_i |h_i|$を引き起こす。もちろん、$L^1$罰則は結果としてスパース表現となり得る罰則のだだひとつの選択肢である。その他は表現に関するStudent $t$ priorから導出される罰則[@Olshausen1996; @Bergstra2011]とKLダイバージェンス罰則[@Larochelle2008]を含む。これは、単位区間にあるよう制限された要素を持つ表現で特に有用である。@Lee2008と@Goodfellow2009の両者は成分ごとに$.01$を持つベクトルのように、ある対象の値に近くなるよう、いくつかのexamplesに渡る平均活性化$\frac{1}{m} \sum_i \boldsymbol{h}^{(i)}$を正則化することに基づく戦略の例をもたらす。
他のアプローチは活性化の値に関するハード制約によって表現上のスパース性を得る。例えば、直交マッチング追跡[orthogonal matching pursuit][@Pati1993]は以下の制約付き最適化問題を解く表現$\boldsymbol{h}$で入力$\boldsymbol{x}$をエンコードする。

$$
\argmin_{\boldsymbol{h}, \| \boldsymbol{h} \|_0 < k} \| \boldsymbol{x} - \boldsymbol{W} \boldsymbol{h} \|^2
$$

ここで、$\|\boldsymbol{h}\|_0$は$\boldsymbol{h}$の非ゼロ成分の数である。この問題は、$\boldsymbol{W}$が直交であるよう制限されているとき、効率的に解くことができる。この手法はしばしば、非ゼロの特徴数が許可されることを示すための指定される$k$の値とともに、OMP-$k$と呼ばれる。@Coates2011はOMP-1が深層アーキテクチャに対する非常に効果的な特徴抽出器であり得ることを実証した。
本質的には、隠れ層を持つどのモデルも疎にすることができる。本書を通して、我々は様々な場面で使われるスパース性正則化の例にたくさん出会う。

## baggingと他のアンサンブル手法

bagging（bootstrap aggregatingの略）はいくつかのもｄるを組み合わせることで汎化誤差を減らすための技術である[@Breiman1994]。そのアイデアはいくつかの異なるモデルを別個に訓練し、すべてのモデルにテストexamplesに対する出力へ投票してもらうことである。これはmodel averagingと呼ばれる機械学習における一般的な戦略の例である。この戦略で採用している技術はアンサンブル手法[ensemble methods]として知られる。
model averagingが機能する理由は、異なるモデルは通常ではテストセットに関してすべて同じ誤差を生成しないだろう、ということである。
例として、$k$個の回帰モデルの集合を考える。各モデルは、分散$\mathbb{E}[\epsilon_i^2] = v$と共分散$\mathbb{E}[\epsilon_i \epsilon_j] = c$を持つゼロ平均多変量正規分布から描かれる誤差とともに、各exampleに関して誤差$\epsilon_i$を作るとする。そして、すべてのアンサンブルモデルの平均予測値によって作られる誤差は$\frac{1}{k} \sum_i \epsilon_i$である。アンサンブル予測器の二乗誤差の期待値は以下である。

$$
\mathbb{E} \left[ \left( \frac{1}{k} \sum_i \epsilon_i \right)^2 \right] = \frac{1}{k^2} \mathbb{E} \left[ \sum_i \left( \eps_i^2 + \sum_{j \ne i} \epsilon_i \epsilon_j \right) \right]
$$

$$
= \frac{1}{k} v + \frac{k - 1}{k} c
$$

誤差が完全に相関していて$c = v$である場合、平均二乗誤差が$v$に減少し、そのために、model averagingはまったく役に立たない。誤差が完全に相関しておらず$c = 0$である場合、アンサンブルの二乗誤差の期待値は$\frac{1}{k} v$のみである。これはアンサンブルの二乗誤差の期待値がアンサンブルの大きさに反比例することを意味する。言い換えれば、平均的に、アンサンブルはそのメンバのいずれかと少なくとも同等に処理するだろうし、そのメンバが独立した誤差を作るならば、アンサンブルはそのメンバより大幅によく処理するだろう。
異なるアンサンブルモデルは異なる方法でモデルのアンサンブルを構築する。例えば、アンサンブルの各メンバは異なるアルゴリズムまたは目的関数を用いて完全に異なる種類のモデルを訓練することで形成され得るだろう。baggingは同種のモデル、訓練アルゴリズム、目的関数を数回再利用することを許可する手法である。
具体的には、baggingは$k$個の異なるデータセットを構築することを伴う。各データセットはオリジナルのデータセットと同じ数のexamplesを持つが、各データセットはオリジナルのデータセットからの復元抽出によって構築される。これは、高確率で、各データセットはオリジナルのデータセットからexamplesのいくつかを失い、いくつかの重複するexamplesを含むことを意味する[^4]。モデル$i$はデータセット$i$で訓練される。各データセットに含まれるexamplesの差異は結果として訓練されたモデルの差異となる。一例は[@fig:7.5]を参照のこと。

[^4]: オリジナルのデータセットと再サンプルされたデータセットの両方が$m$個のexamplesを含むとき、新しいデータセットから失われるexamplesの厳密な割合は$(1 - \frac{1}{m})^m$である。これは特定のexampleが新しいデータセットを作るのに使われるすべての$m$個のdrawsから$m$個の取り得る元のexamplesの中から選ばれない可能性である。$m$が無限大に近づくにつれて、この量は$\frac{1}{e}$に収束する。これは$\frac{1}{3}$より若干大きい。

![baggingの動き方の図解。8と6と9を含む上に描かれたデータセットで8の字識別器を訓練するとする。2つの異なる再サンプルされたデータセットを作るとする。bagging訓練手順は復元抽出を行うことでこれらのデータセットのそれぞれを構築することである。1つ目のデータセットは9を除外し、9を繰り返す。このデータセットでは、識別器は数字の上部の輪っかが8に対応することを学習する。2つ目のデータセットでは、9を繰り返し、6を除外する。この場合、識別器は数字の下部の輪っかが8に対応することを学習する。これらの個々の分類規則のそれぞれは脆弱だが、これらの出力を平均するならば、識別器は堅牢であり、8の両方の輪っかが示されるときのみ最大の信頼度を達成する。](fig/7-5.png){#fig:7.5}

ニューラルネットワークは、すべてのモデルが同じデータセットで訓練される場合でさえ、model averagingからの恩恵をしばしば得られる十分に多種多様な解の点に到達する。無作為な初期化、ミニバッチの無作為な選択、ハイパーパラメータ、または、ニューラルネットワークの非決定的な実装の結果における差異はしばしばアンサンブルの異なるメンバに部分てkに独立な誤差を作らせるのに十分である。
model averagingは極めて強力で信頼性の高い汎化誤差を減らす手法である。増加する計算量およびメモリ量を対価としてmodel averagingから実質的に恩恵を得る可能性があるので、これの使用は科学論文のためのアルゴリズムをベンチマークするときに通常オススメされない。この理由から、ベンチマーク比較は通常では単一のモデルを用いて作られる。
機械学習コンテストでは大抵、数十のモデルに対してmodel averagingを用いる手法が勝利している。昨今の著名な例はNetflix Grand Prize[@Koren2009]である。
アンサンブルを構築するすべての技術がアンサンブルを個々のモデルより正則化させるために設計される訳ではない。例えば、boostingと呼ばれる技術[@Freund1996b; @Freund1996a]は個々のモデルより高い容量を持つアンサンブルを構築する。boostingはニューラルネットワークをアンサンブルへ徐々に加えることによってニューラルネットワークのアンサンブル[@Schwenk1998]を作るために適用されている。boostingは、隠れ層をネットワークへ徐々に加え、個々のネットワークをアンサンブルとして解釈して適用されてもいる[@Bengio2006a]。

## dropout

dropout [@Srivastava2014]はモデルの広範な族を正則化する計算上安価だが強力な手法を提供する。大まかな当たり付けとして[to a first approximation]、dropoutは非常に多くの大きなニューラルネットワークのアンサンブルに対してbaggingを実践的にする手法として考えることができる。baggingは複数のモデルを訓練することと各テストexampleで複数のモデルを評価することを伴う。これは各モデルが大きなニューラルネットワークであるとき、そのようなモデルを訓練および評価することが実行時間とメモリの観点から高コストであるので、実践的でないように見える。5から10のニューラルネットワークのアンサンブルを用いることが一般的---@Szegedy2014aはILSVRCを勝ち取るのに6を用いた---であり、これ以上のものは急速に扱いづらくなる。dropoutは指数関数的に多くのニューラルネットワークのbagされたアンサンブルの訓練および評価の高価でない近似を提供する。
具体的には、dropoutは、[@fig:7.6]に図示される通り、下地となるベースネットワークから非出力ユニットを取り除くことによって形成され得るすべての部分ネットワークから成るアンサンブルを訓練する。一連のアフィン変換と非線形性に基づく、最もモダンなニューラルネットワークでは、出力値にゼロをかけることでネットワークからユニットを効率的に取り除くことができる。この手順は、ユニットの状態とある参照値の間の差を取る放射基底関数ネットワークのようなモデルに対して若干の修正を要する。ここでは、単純さのためにゼロの掛け算の観点でdropoutアルゴリズムを提示するが、ちょっとした修正を行ってネットワークからユニットを取り除く他の操作で動かすことができる。

![dropoutは基礎を成すベースのネットワークから非出力ユニットを取り除くことによって構築され得るすべての部分ネットワークから成るアンサンブルを訓練する。ここでは、2つの可視ユニットと2つの隠れユニットを持つベースネットワークで始める。これら4つのユニットの取り得る部分集合は16つある。我々はオリジナルのネットワークからユニットの異なる部分集合をdropoutすることによって形成されるかもしれない16つすべての部分ネットワークを示す。この小さな例では、結果のネットワークの大部分は入力ユニットを持たないか入力と出力をつなぐパスを持たない。この問題は、入力から出力へのすべての取り得るパスをdropする確率がより小さくなるところのより広い層を持つネットワークに対して重要ではなくなる。](fig/7-6.png){#fig:7.6}

bagging付きで学習するには、$k$個の異なるモデルを定義し、訓練セットからの復元抽出によって$k$個の異なるデータセットを構築し、データセット$i$でモデル$i$を訓練する、ということを思い出したい。dropoutはこのプロセスを近似することを目指しているが、指数関数的に大きな数のニューラルネットワークを伴う。具体的に言えば、dropout付きで訓練するには、確率的勾配降下法のような小さなステップを作るミニバッチ式学習アルゴリズムを用いる。exampleをミニバッチに積み込むたびに、ネットワーク中のすべての入力と隠れ層に適用するための異なるバイナリマスクを無作為にサンプルする。各ユニットに対するマスクは他すべてから独立してサンプルされる。（ユニット1つを含ませるような）ひとつのマスク値をサンプリングする確率は訓練を始める前に固定したハイパーパラメータである。これはモデルパラメータの現在の値の関数でも入力exampleでもない。一般に、入力ユニットは確率$0.8$で含まれ、隠れ層は確率$0.5$で含まれる。そして、forward propagation、逆伝播、学習の更新を通常通りに実行する。[@fig:7.7]はdropout付きでforward propagationを実行する方法を図示する。

![dropoutを用いたフィードフォワードネットワークを通るforward propagationの例。（上）この例では、2つの入力ユニット、2つの隠れユニットを持つ1つの隠れ層、1つの出力ユニットを持つフィードフォワードネットワークを用いる。（下）dropout付きforward propagationを行うため、ネットワークにおける入力および隠れユニットごとに1つの成分を持つベクトル$\boldsymbol{\mu}$を無作為にサンプルする。$\boldsymbol{\mu}$の成分は二値であり、互いに独立してサンプルされる。各成分が$1$である確率はハイパーパラメータであり、通常では隠れ層に対して$0.5$であり、入力に対して$0.8$である。ネットワークの各ユニットは対応するマスクによって乗算され、forward propagationは通常通りにネットワークの残りを通って継続する。これは[@fig:7.6から部分ネットワークの1つを無作為に選択し、それを通してforward propagationを実行することと等価である。]](fig/7-7.png){#fig:7.7}

より公式的には、マスクベクトル$\boldsymbol{\mu}$がどのユニットが含まれるべきかを明示し、$J(\boldsymbol{\theta, \boldsymbol{\mu}}$がパラメータ$\boldsymbol{\theta}$とマスク$\boldsymbol{\mu}$によって定義されるモデルのコストを定義するとする。故に、dropoutの訓練は$\mathbb{E}_{\boldsymbol{\mu}} J(\boldsymbol{\theta}, \boldsymbol{\mu})$を最小化することから成る。期待値は指数関数的に多くの項を含むが、$\boldsymbol{\mu}$の値をサンプリングすることによってその勾配の推定値を得ることができる。
dropout訓練はbagging訓練とそこまで同じではない。baggingの場合、モデルはすべて独立である。dropoutuの場合、モデルは、各モデルが親のニューラルネットワークからパラメータの異なる部分集合を継承しながら、パラメータを共有する。このパラメータ共有は扱いやすいメモリ量を持つモデルの指数関数的な数を表すことができるようする。baggingの場合、各モデルはそのrepresentive訓練セットで収束するよう訓練される。dropoutの場合、一般にほとんどのモデルはまったく明示的に訓練されない---通常、モデルは宇宙の寿命の間に取り得るすべての部分ネットワークをサンプリングすることが実行不可能であろうくらいに十分に大きい。代わりに、ほんの僅かの取り得る部分ネットワークが単一ステップでそれぞれ訓練され、パラメータ共有は残りの部分ネットワークにパラメータの良い設定に到達させる。これらが唯一の差異である。これらの以外では、dropoutはbaggingアルゴリズムに従う。例えば、各部分ネットワークが遭遇する訓練セットはもちろん、復元抽出された元の訓練セットの部分集合である。
予測を行うため、bagされたアンサンブルはすべてのそのメンバから投票を集めなければならない。我々はこの文脈においてこのプロセスを推論[inference]と呼ぶ。これまで、baggingやdropoutの記述はモデルが明示的に確率論的であることを必要としてこなかった。いま、我々はモデルの役割が確率分布を出力することと仮定する。baggingの場合、各モデル$i$は確率分布$p^{(i)}(y | \boldsymbol{x})$を生成する。アンサンブルの予測値はこれらすべての分布の算術平均によって与えられる。

$$
\frac{1}{k} \sum_{i = 1}^k p^{(i)}(y | \boldsymbol{x})
$$

dropoutの場合、マスクベクトル$\boldsymbol{\mu}$によって定義される各部分モデルは確率分布$p(y | \boldsymbol{x}, \boldsymbol{\mu})$を定義する。すべてのマスクに対する算術平均は以下によって与えられる。

$$
\sum_{\boldsymbol{\mu}} p(\boldsymbol{\mu})p(y | \boldsymbol{x}, \boldsymbol{\mu})
$$

ここで、$p(\boldsymbol{\mu})$は訓練時に$\boldsymbol{\mu}$をサンプルするのに使われた確率分布である。
この総和は指数関数的な数の項を含むので、モデルの構造がある形式の単純化を許可しているとき以外では評価するためには扱いにくい。これまで、深層ニューラルネットが扱いやすい単純化を許可することは知られていない。代わりに、多くのマスクから出力を一緒に平均することによって、サンプリングで推論を近似することができる。10～20のマスクでさえしばしば良いパフォーマンスを得るのに十分である。
しかし、更に良いアプローチは、たった1回のforward propagationのコストで、アンサンブル全体の予測値への良い近似を得ることを可能にする。そうするため、我々はアンサンブルのメンバの予測された分布の算術平均ではなく幾何平均を用いるよう変更する。@WardeFarley2014は幾何平均がこの文脈において算術演算に匹敵するほどに処理することの論拠および経験的証拠を提示する。
複数の確率分布の幾何平均は確率分布であることを保証しない。結果が確率分布であることを保証するため、いずれかの事象に確率$0$を割り当てる部分モデルが存在しないという要件を課し、結果の分布を再正規化する。幾何平均で直接定義される非正規化確率分布は以下によって定義される。

$$
\tilde{p}_{ensemble}(y | \boldsymbol{x}) = \sqrt[2^d]{\prod_{\boldsymbol{\mu}} p(y | \boldsymbol{x}, \boldsymbol{\mu})}
$$

ここで、$d$はdropされるかもしれないユニット数である。ここでは、プレゼンテーションを単純化するために$\boldsymbol{\mu}$に対する一様分布を用いるが、一様でない分布でも可能である。予測を行うため、アンサンブルを再正規化しなければならない。

$$
p_{ensemble}(y | \boldsymbol{x}) = \frac{\tilde{p}_{ensemble}(y | \boldsymbol{x})}{\sum_{y'} \tilde{p}_{ensemble}(y' | \boldsymbol{x})}
$$

dropoutに関わる重要な洞察[@Hinton2012c]は、我々のモデル、すなわち、すべてのユニットを持つが、ユニット$i$を含む確率を乗算したユニット$i$を出る重みを持つモデルにおける$p(y | \boldsymbol{x})$を計算することによって$p_{ensemble}$を近似できる、ということである。この修正の動機はそのユニットからの出力の正しい期待値を捕捉することである。我々はこの手法をweight scaling inference ruleと呼ぶ。深層非線形ネットワークにおけるこの近似的推論規則の正確性に対する理論的な論拠は未だに何もないが、経験的には非常にうまくいく。
我々は大抵$\frac{1}{2}$のinclusion確率を用いるので、weight scaling ruleは通常、訓練終了時に$2$で重みを割り、その後、通常通りにモデルを用いることに等しい。同じ結果を達成するもうひとつの方法は訓練の間にユニットの状態に$2$を掛けることである。どちらの方法でも、その目標は、平均で訓練時の半数のユニットが失われるとしても、テスト時でのユニットへの総入力の期待値がおおまかに訓練時でのそのユニットへの総入力の期待値と等しいことを確実にすることである。
非線形な隠れユニットを持たないモデルの多くのクラスでは、weight scaling inference ruleは厳密である。単純な例として、ベクトル$\boldsymbol{\mathbf{v}}$によって表現される$n$個の入力変数を持つsoftmax回帰分類器を考える。

$$
P(\mathbf{y} = y | \boldsymbol{\mathbf{v}}) = \text{softmax} \left( \boldsymbol{W}^\top \boldsymbol{\mathbf{v}} + \boldsymbol{b} \right)_y
$$

二値ベクトル$\boldsymbol{d}$による入力の成分ごとの乗算によって部分モデルのファミリーにインデックスを付けることができる。

$$
P(\mathbf{y} = y | \boldsymbol{\mathbf{v}}; \boldsymbol{d}) = \text{softmax} \left( \boldsymbol{W}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + \boldsymbol{b} \right)_y
$$

アンサンブル予測器はすべてのアンサンブルメンバの予測値に対する幾何平均を正規化することによって定義される。

$$
P_{ensemble}(\mathbf{y} = y | \boldsymbol{\mathbf{v}}) = \frac{\tilde{P}_{ensemble}(\mathbf{y} = y | \boldsymbol{\mathbf{v}})}{\sum_{y'} \tilde{P}_{ensemble}(\mathbf{y} = y' | \boldsymbol{\mathbf{v}})}
$$

ここで、

$$
\tilde{P}_{ensemble}(\mathbf{y} = y | \boldsymbol{\mathbf{v}}) = \sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} P(\mathbf{y} = y | \boldsymbol{\mathbf{v}}; \boldsymbol{d})}
$$

weight scaling ruleが厳密であると理解するため、$\tilde{P}_{ensemble}$を単純化できる。

$$
\tilde{P}_{ensemble}(\mathbf{y} = y | \boldsymbol{\mathbf{v}}) = \sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} P(\mathbf{y} = y | \boldsymbol{\mathbf{v}}; \boldsymbol{d})}
$$

$$
= \sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} \text{softmax}(\boldsymbol{W}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + \boldsymbol{b})_y}
$$

$$
= \sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} \frac{\exp(\boldsymbol{W}_{y,:}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + b_y)}{\sum_{y'} \exp(\boldsymbol{W}_{y',:}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + b_{y'})}}
$$

$$
\frac{\sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} \exp(\boldsymbol{W}_{y,:}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + b_y)}}{\sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} \sum_{y'} \exp(\boldsymbol{W}_{y',:}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + b_{y'})}}
$$

$\tilde{P}$は正規化されるので、$y$に関する定数であるファクタの乗算を安全に無視できる。

$$
\tilde{P}_{ensemble}(\mathbf{y} = y | \boldsymbol{\mathbf{v}}) \propto \sqrt[2^n]{\prod_{\boldsymbol{d} \in \{0, 1\}^n} \exp(\boldsymbol{W}_{y,:}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + b_y)}
$$

$$
= \exp \left( \frac{1}{2^n} \sum_{\boldsymbol{d} \in \{0, 1\}^n} \boldsymbol{W}_{y,:}^\top (\boldsymbol{d} \odot \boldsymbol{\mathbf{v}}) + b_y \right)
$$


$$
= \exp \left( \frac{1}{2} \boldsymbol{W}_{y,:}^\top \boldsymbol{\mathbf{v}} + b_y)
$$

これを[@eq:7.58]置き換え直すと、重み$\frac{1}{2} \boldsymbol{W}$を持つsoftmax分類器を得る。
weight scaling ruleは***
