# 深層学習のための正則化

機械学習における中心的問題はどのようにして訓練データだけでなく新しい入力に関してもうまく働くであろうアルゴリズムを作るかである。機械学習で使われる多くの戦略は、おそらく訓練誤差の増加という犠牲を払って、テスト誤差を減らすために明示的に設計されている。これらの戦略はまとめて正則化として知られている。深層学習の実践者は非常に多くの形式の正則化を利用できる。実際、より効果的な正則化戦略の開発はこの分野における主要な研究成果のひとつとなっている。
[@sec:5]では汎化、アンダーフィッティング、オーバーフィッティング、バイアス、分散、正則化の基本概念を紹介した。あなたが未だこれらの概念に親しんでいないならば、本章を読み進める前にその章を参照してほしい。
本章では、深層モデルや深層モデルを形成するための構成部品として用いられるかもしれないモデルに焦点を当てて、より詳細に正則化を述べる。
本章のいくつかの節は機械学習における標準的な概念を扱う。あなたが既にこれらの概念に親しんでいるならば、関連する節を読み飛ばしてもらって一向に構わない。しかし、本章のほとんどはニューラルネットワークの特定のケースへのこれらの基本概念の拡張と関係している。
[@sec:5.2.2]では、我々は「訓練誤差ではなく汎化誤差を減らすよう意図された学習アルゴリズムに施すいずれかの修正」として正則化を定義した。正則化戦略は多数存在する。あるものは、パラメータの値に制限を加えるような、追加の制約を機械学習モデルにもたらす。あるものはパラメータの値のsoft constraintに対応すると考えることができる追加の項を目的関数に加える。慎重に選ばれれば、これらの追加の制約や罰則はテストセットにおいて改善されたパフォーマンスを引き起こし得る。ある時には、これらの制約や罰則は特定の種類の事前知識をエンコードするために設計される。またある時には、これらの制約や罰則は汎化を促進させるためにより単純なモデルのクラスに対する一般的な選好を表現するために設計される。時折、罰則や制約はunderdetermined problemをdeterminedにするのに必要である。アンサンブル法として知られる他の形式の正則化は訓練データを説明する複数の仮説を組み合わせる。
深層学習の文脈では、ほとんどの正則化戦略は推定器を正則化することに基づいている。推定器の正則化はバイアスの増加を分散の低減とトレードすることで行われる。効果的なregularizerは、分散を大幅に低減しつつバイアスを過度に増やさない、有利なトレード[profitable trade]を行うものである。[@sec:5]で汎化とオーバーフィッティングを考察したとき、我々は3つの状況に焦点を当てた。これは、訓練されるモデル族が（１）真のデータ生成過程を除外した --- アンダーフィットすることとバイアスを含めることに対応する --- か、（２）真のデータ生成過程に一致したか、（３）生成過程だけでなく他の多くの取り得る生成過程を含めた --- バイアスより分散の方が推定値の誤差に占める割合が大きいオーバーフィッティング状態である --- かのいずれかである。正則化の目標はモデルを３番目の状態から２番目の状態にすることである。
実践では、過度に複雑なモデル族は対象の関数や真のデータ生成過程、または、どちらかの近しい近似であっても必ず含むというわけではない。我々は真のデータ生成過程にアクセスすることはほぼできず、そのために、推定されるモデル族がその生成過程を含むかどうかを確信できることは一切ない。しかしながら、深層学習アルゴリズムのほとんどのアプリケーションは真のデータ生成過程がほぼ確実にモデル族の外側にある領域にある。深層学習アルゴリズムは一般に、真の生成過程が本質的に宇宙全体のシミュレーションを伴う、画像、音声シーケンス、文章のような極めて複雑な領域に適用される。ある程度までは、我々は常に四角いペグ（データ生成過程）を丸い穴（我々のモデル族）に合わせようと試みている。
これが意味することは、モデルの複雑さを制御することが正しいパラメータ数を持つ正しい大きさのモデルを見つけるという単純な問題ではない、ということである。代わりに、我々は（汎化誤差を最小化するという意味において）最適にフィットするモデルが適切に正則化された大きなモデルであることをおそらく見つけ --- そして実際に、実践的な深層学習のシナリオにおいて、ほとんどいつも見つけ --- るだろう。
我々はそのような大きく深い正則化されたモデルを生成する方法に対するいくつかの戦略を再調査する。

## パラメータのノルムの罰則

正則化は***
